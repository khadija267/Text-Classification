{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOjKtpIi/Eb4BsstwDZu9RW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khadija267/Text-Classification/blob/main/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Project Set-up"
      ],
      "metadata": {
        "id": "WfoN1HSatXQE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "!pip install rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6a59lFTtcmq",
        "outputId": "8ed3403d-db18-4214-90fe-b4ba30fc1dd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from rouge import Rouge\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "zo4F2fbgtZ6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXvw07NFtfD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Data Collection"
      ],
      "metadata": {
        "id": "vOXvfZuCtnbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('cnn_dailymail', '3.0.0', split='train')\n",
        "\n",
        "# Extract input texts and target texts from the dataset\n",
        "input_texts = dataset['article']\n",
        "target_texts = dataset['highlights']\n"
      ],
      "metadata": {
        "id": "HBD4MPIJttWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_size=len(input_texts)"
      ],
      "metadata": {
        "id": "H214Jk-P0D9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Data Preprocessing:"
      ],
      "metadata": {
        "id": "nWIpYn_kvfCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, input_texts, target_texts, tokenizer):\n",
        "        self.input_texts = input_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.input_texts[idx]\n",
        "        target_text = self.target_texts[idx]\n",
        "        input_ids = self.tokenizer.encode(input_text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')[0]\n",
        "        target_ids = self.tokenizer.encode(target_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')[0]\n",
        "        return input_ids, target_ids\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "dataset = CustomDataset(input_texts, target_texts, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7XmjIX0vjU2",
        "outputId": "57d9e08d-5f4f-4662-930d-3f4f53b20e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:217: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Model Construction:"
      ],
      "metadata": {
        "id": "SmlNorrywMzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 't5-base'\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "batch_size=10\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "#num_epochs = 3\n",
        "\n",
        "model.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSNltOXBwP64",
        "outputId": "f724112f-f039-408a-bac4-6b6c99c77e00"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z7vqUqrnROUQ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_values = []\n",
        "num_batches=int(data_size/batch_size)\n",
        "\n",
        "epoch_train_loss = 0.0\n",
        "for batch_ind,batch in enumerate(train_loader):\n",
        "    batch_input_ids, batch_target_ids = batch\n",
        "    batch_input_ids = batch_input_ids.to(device)\n",
        "    batch_target_ids = batch_target_ids.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(input_ids=batch_input_ids, labels=batch_target_ids)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_train_loss += loss.item() * len(batch_input_ids)\n",
        "    print(f'Batch [{batch_ind+1}/{num_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_train_loss /= len(dataset)\n",
        "\n",
        "    train_loss_values.append(epoch_train_loss)\n",
        "\n"
      ],
      "metadata": {
        "id": "VveYqzIr7TV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4665cfa-1ee2-4d7e-8e0c-cbe3d74ca4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Batch [23713/28711], Loss: 1.1276\n",
            "Batch [23714/28711], Loss: 0.9791\n",
            "Batch [23715/28711], Loss: 1.0743\n",
            "Batch [23716/28711], Loss: 1.0551\n",
            "Batch [23717/28711], Loss: 0.8534\n",
            "Batch [23718/28711], Loss: 1.0844\n",
            "Batch [23719/28711], Loss: 0.7950\n",
            "Batch [23720/28711], Loss: 1.0179\n",
            "Batch [23721/28711], Loss: 0.8071\n",
            "Batch [23722/28711], Loss: 0.9714\n",
            "Batch [23723/28711], Loss: 1.0194\n",
            "Batch [23724/28711], Loss: 0.8067\n",
            "Batch [23725/28711], Loss: 0.9362\n",
            "Batch [23726/28711], Loss: 1.0126\n",
            "Batch [23727/28711], Loss: 0.8999\n",
            "Batch [23728/28711], Loss: 1.0263\n",
            "Batch [23729/28711], Loss: 0.7368\n",
            "Batch [23730/28711], Loss: 0.8508\n",
            "Batch [23731/28711], Loss: 1.0178\n",
            "Batch [23732/28711], Loss: 1.2126\n",
            "Batch [23733/28711], Loss: 0.6192\n",
            "Batch [23734/28711], Loss: 0.9907\n",
            "Batch [23735/28711], Loss: 0.9844\n",
            "Batch [23736/28711], Loss: 1.0018\n",
            "Batch [23737/28711], Loss: 0.8692\n",
            "Batch [23738/28711], Loss: 1.1629\n",
            "Batch [23739/28711], Loss: 1.0750\n",
            "Batch [23740/28711], Loss: 1.1869\n",
            "Batch [23741/28711], Loss: 0.7815\n",
            "Batch [23742/28711], Loss: 1.1044\n",
            "Batch [23743/28711], Loss: 1.1274\n",
            "Batch [23744/28711], Loss: 0.8821\n",
            "Batch [23745/28711], Loss: 0.5877\n",
            "Batch [23746/28711], Loss: 0.6688\n",
            "Batch [23747/28711], Loss: 0.8531\n",
            "Batch [23748/28711], Loss: 0.9033\n",
            "Batch [23749/28711], Loss: 0.7934\n",
            "Batch [23750/28711], Loss: 0.6970\n",
            "Batch [23751/28711], Loss: 0.6649\n",
            "Batch [23752/28711], Loss: 0.8197\n",
            "Batch [23753/28711], Loss: 1.0275\n",
            "Batch [23754/28711], Loss: 0.8247\n",
            "Batch [23755/28711], Loss: 0.8622\n",
            "Batch [23756/28711], Loss: 1.0247\n",
            "Batch [23757/28711], Loss: 0.7033\n",
            "Batch [23758/28711], Loss: 0.9274\n",
            "Batch [23759/28711], Loss: 1.0916\n",
            "Batch [23760/28711], Loss: 0.8972\n",
            "Batch [23761/28711], Loss: 1.0729\n",
            "Batch [23762/28711], Loss: 1.1641\n",
            "Batch [23763/28711], Loss: 0.8319\n",
            "Batch [23764/28711], Loss: 0.6448\n",
            "Batch [23765/28711], Loss: 0.8496\n",
            "Batch [23766/28711], Loss: 0.8469\n",
            "Batch [23767/28711], Loss: 0.8753\n",
            "Batch [23768/28711], Loss: 1.0505\n",
            "Batch [23769/28711], Loss: 1.0286\n",
            "Batch [23770/28711], Loss: 1.0237\n",
            "Batch [23771/28711], Loss: 0.8462\n",
            "Batch [23772/28711], Loss: 1.1929\n",
            "Batch [23773/28711], Loss: 0.9837\n",
            "Batch [23774/28711], Loss: 0.8187\n",
            "Batch [23775/28711], Loss: 1.1610\n",
            "Batch [23776/28711], Loss: 0.9110\n",
            "Batch [23777/28711], Loss: 1.1125\n",
            "Batch [23778/28711], Loss: 1.2592\n",
            "Batch [23779/28711], Loss: 1.3507\n",
            "Batch [23780/28711], Loss: 0.9156\n",
            "Batch [23781/28711], Loss: 1.1295\n",
            "Batch [23782/28711], Loss: 1.1832\n",
            "Batch [23783/28711], Loss: 0.7885\n",
            "Batch [23784/28711], Loss: 0.7270\n",
            "Batch [23785/28711], Loss: 1.0173\n",
            "Batch [23786/28711], Loss: 0.9535\n",
            "Batch [23787/28711], Loss: 0.5720\n",
            "Batch [23788/28711], Loss: 0.7565\n",
            "Batch [23789/28711], Loss: 1.0234\n",
            "Batch [23790/28711], Loss: 0.9845\n",
            "Batch [23791/28711], Loss: 1.0162\n",
            "Batch [23792/28711], Loss: 0.8953\n",
            "Batch [23793/28711], Loss: 0.6551\n",
            "Batch [23794/28711], Loss: 0.7155\n",
            "Batch [23795/28711], Loss: 0.6846\n",
            "Batch [23796/28711], Loss: 0.8526\n",
            "Batch [23797/28711], Loss: 0.9905\n",
            "Batch [23798/28711], Loss: 1.1369\n",
            "Batch [23799/28711], Loss: 0.6934\n",
            "Batch [23800/28711], Loss: 0.8743\n",
            "Batch [23801/28711], Loss: 1.0506\n",
            "Batch [23802/28711], Loss: 0.9007\n",
            "Batch [23803/28711], Loss: 0.8683\n",
            "Batch [23804/28711], Loss: 0.9856\n",
            "Batch [23805/28711], Loss: 0.8832\n",
            "Batch [23806/28711], Loss: 1.1217\n",
            "Batch [23807/28711], Loss: 1.3031\n",
            "Batch [23808/28711], Loss: 0.8427\n",
            "Batch [23809/28711], Loss: 0.9881\n",
            "Batch [23810/28711], Loss: 0.7043\n",
            "Batch [23811/28711], Loss: 1.0956\n",
            "Batch [23812/28711], Loss: 0.9686\n",
            "Batch [23813/28711], Loss: 1.0760\n",
            "Batch [23814/28711], Loss: 1.0303\n",
            "Batch [23815/28711], Loss: 0.9862\n",
            "Batch [23816/28711], Loss: 0.7947\n",
            "Batch [23817/28711], Loss: 1.3474\n",
            "Batch [23818/28711], Loss: 0.8244\n",
            "Batch [23819/28711], Loss: 1.0281\n",
            "Batch [23820/28711], Loss: 0.7549\n",
            "Batch [23821/28711], Loss: 1.0509\n",
            "Batch [23822/28711], Loss: 1.0970\n",
            "Batch [23823/28711], Loss: 1.0980\n",
            "Batch [23824/28711], Loss: 0.5585\n",
            "Batch [23825/28711], Loss: 0.9026\n",
            "Batch [23826/28711], Loss: 1.0376\n",
            "Batch [23827/28711], Loss: 0.9419\n",
            "Batch [23828/28711], Loss: 0.8719\n",
            "Batch [23829/28711], Loss: 0.7678\n",
            "Batch [23830/28711], Loss: 0.8565\n",
            "Batch [23831/28711], Loss: 0.5437\n",
            "Batch [23832/28711], Loss: 0.9230\n",
            "Batch [23833/28711], Loss: 1.1346\n",
            "Batch [23834/28711], Loss: 0.8791\n",
            "Batch [23835/28711], Loss: 0.7416\n",
            "Batch [23836/28711], Loss: 1.2617\n",
            "Batch [23837/28711], Loss: 0.7208\n",
            "Batch [23838/28711], Loss: 0.6316\n",
            "Batch [23839/28711], Loss: 1.0711\n",
            "Batch [23840/28711], Loss: 0.8736\n",
            "Batch [23841/28711], Loss: 0.7786\n",
            "Batch [23842/28711], Loss: 0.9265\n",
            "Batch [23843/28711], Loss: 1.0719\n",
            "Batch [23844/28711], Loss: 0.9505\n",
            "Batch [23845/28711], Loss: 1.1159\n",
            "Batch [23846/28711], Loss: 0.9123\n",
            "Batch [23847/28711], Loss: 0.9489\n",
            "Batch [23848/28711], Loss: 0.9815\n",
            "Batch [23849/28711], Loss: 0.7968\n",
            "Batch [23850/28711], Loss: 0.8243\n",
            "Batch [23851/28711], Loss: 0.9765\n",
            "Batch [23852/28711], Loss: 0.9331\n",
            "Batch [23853/28711], Loss: 0.9157\n",
            "Batch [23854/28711], Loss: 1.0164\n",
            "Batch [23855/28711], Loss: 1.0458\n",
            "Batch [23856/28711], Loss: 0.7871\n",
            "Batch [23857/28711], Loss: 0.7329\n",
            "Batch [23858/28711], Loss: 0.8537\n",
            "Batch [23859/28711], Loss: 0.7840\n",
            "Batch [23860/28711], Loss: 0.9351\n",
            "Batch [23861/28711], Loss: 1.0542\n",
            "Batch [23862/28711], Loss: 0.9377\n",
            "Batch [23863/28711], Loss: 0.8619\n",
            "Batch [23864/28711], Loss: 0.8103\n",
            "Batch [23865/28711], Loss: 0.9785\n",
            "Batch [23866/28711], Loss: 0.6423\n",
            "Batch [23867/28711], Loss: 1.1000\n",
            "Batch [23868/28711], Loss: 0.8334\n",
            "Batch [23869/28711], Loss: 0.7011\n",
            "Batch [23870/28711], Loss: 0.9155\n",
            "Batch [23871/28711], Loss: 0.6510\n",
            "Batch [23872/28711], Loss: 0.9322\n",
            "Batch [23873/28711], Loss: 1.0058\n",
            "Batch [23874/28711], Loss: 1.0633\n",
            "Batch [23875/28711], Loss: 0.8942\n",
            "Batch [23876/28711], Loss: 0.8328\n",
            "Batch [23877/28711], Loss: 0.6805\n",
            "Batch [23878/28711], Loss: 0.8466\n",
            "Batch [23879/28711], Loss: 0.9839\n",
            "Batch [23880/28711], Loss: 0.8752\n",
            "Batch [23881/28711], Loss: 0.8178\n",
            "Batch [23882/28711], Loss: 1.2683\n",
            "Batch [23883/28711], Loss: 1.0024\n",
            "Batch [23884/28711], Loss: 1.1136\n",
            "Batch [23885/28711], Loss: 0.9376\n",
            "Batch [23886/28711], Loss: 0.9535\n",
            "Batch [23887/28711], Loss: 0.7738\n",
            "Batch [23888/28711], Loss: 0.9839\n",
            "Batch [23889/28711], Loss: 0.8954\n",
            "Batch [23890/28711], Loss: 0.9989\n",
            "Batch [23891/28711], Loss: 0.7419\n",
            "Batch [23892/28711], Loss: 1.0067\n",
            "Batch [23893/28711], Loss: 1.1619\n",
            "Batch [23894/28711], Loss: 0.9643\n",
            "Batch [23895/28711], Loss: 1.2130\n",
            "Batch [23896/28711], Loss: 0.7801\n",
            "Batch [23897/28711], Loss: 1.0051\n",
            "Batch [23898/28711], Loss: 0.6606\n",
            "Batch [23899/28711], Loss: 0.9091\n",
            "Batch [23900/28711], Loss: 1.1216\n",
            "Batch [23901/28711], Loss: 1.2785\n",
            "Batch [23902/28711], Loss: 0.9486\n",
            "Batch [23903/28711], Loss: 0.7775\n",
            "Batch [23904/28711], Loss: 0.6921\n",
            "Batch [23905/28711], Loss: 0.8359\n",
            "Batch [23906/28711], Loss: 0.7051\n",
            "Batch [23907/28711], Loss: 0.9081\n",
            "Batch [23908/28711], Loss: 0.6730\n",
            "Batch [23909/28711], Loss: 0.7040\n",
            "Batch [23910/28711], Loss: 1.3005\n",
            "Batch [23911/28711], Loss: 0.9438\n",
            "Batch [23912/28711], Loss: 0.8702\n",
            "Batch [23913/28711], Loss: 0.7320\n",
            "Batch [23914/28711], Loss: 0.7018\n",
            "Batch [23915/28711], Loss: 1.0681\n",
            "Batch [23916/28711], Loss: 0.6791\n",
            "Batch [23917/28711], Loss: 1.0364\n",
            "Batch [23918/28711], Loss: 1.0309\n",
            "Batch [23919/28711], Loss: 0.9981\n",
            "Batch [23920/28711], Loss: 0.8458\n",
            "Batch [23921/28711], Loss: 0.8052\n",
            "Batch [23922/28711], Loss: 1.0178\n",
            "Batch [23923/28711], Loss: 0.7084\n",
            "Batch [23924/28711], Loss: 1.1099\n",
            "Batch [23925/28711], Loss: 1.0593\n",
            "Batch [23926/28711], Loss: 1.0276\n",
            "Batch [23927/28711], Loss: 0.9040\n",
            "Batch [23928/28711], Loss: 0.7747\n",
            "Batch [23929/28711], Loss: 1.0471\n",
            "Batch [23930/28711], Loss: 0.7047\n",
            "Batch [23931/28711], Loss: 0.6991\n",
            "Batch [23932/28711], Loss: 0.9415\n",
            "Batch [23933/28711], Loss: 1.0895\n",
            "Batch [23934/28711], Loss: 0.9624\n",
            "Batch [23935/28711], Loss: 1.2534\n",
            "Batch [23936/28711], Loss: 0.6535\n",
            "Batch [23937/28711], Loss: 0.8580\n",
            "Batch [23938/28711], Loss: 0.7697\n",
            "Batch [23939/28711], Loss: 0.8603\n",
            "Batch [23940/28711], Loss: 0.7903\n",
            "Batch [23941/28711], Loss: 0.8974\n",
            "Batch [23942/28711], Loss: 0.8848\n",
            "Batch [23943/28711], Loss: 0.7537\n",
            "Batch [23944/28711], Loss: 0.8470\n",
            "Batch [23945/28711], Loss: 0.9316\n",
            "Batch [23946/28711], Loss: 0.9728\n",
            "Batch [23947/28711], Loss: 0.8765\n",
            "Batch [23948/28711], Loss: 0.8505\n",
            "Batch [23949/28711], Loss: 0.9853\n",
            "Batch [23950/28711], Loss: 1.0933\n",
            "Batch [23951/28711], Loss: 0.7875\n",
            "Batch [23952/28711], Loss: 1.0609\n",
            "Batch [23953/28711], Loss: 0.9577\n",
            "Batch [23954/28711], Loss: 0.7229\n",
            "Batch [23955/28711], Loss: 0.6458\n",
            "Batch [23956/28711], Loss: 0.7910\n",
            "Batch [23957/28711], Loss: 0.8539\n",
            "Batch [23958/28711], Loss: 0.7787\n",
            "Batch [23959/28711], Loss: 1.0576\n",
            "Batch [23960/28711], Loss: 1.1468\n",
            "Batch [23961/28711], Loss: 0.7765\n",
            "Batch [23962/28711], Loss: 0.8730\n",
            "Batch [23963/28711], Loss: 0.8454\n",
            "Batch [23964/28711], Loss: 0.7967\n",
            "Batch [23965/28711], Loss: 1.1689\n",
            "Batch [23966/28711], Loss: 0.9516\n",
            "Batch [23967/28711], Loss: 0.9410\n",
            "Batch [23968/28711], Loss: 1.0126\n",
            "Batch [23969/28711], Loss: 1.0086\n",
            "Batch [23970/28711], Loss: 0.8468\n",
            "Batch [23971/28711], Loss: 0.7468\n",
            "Batch [23972/28711], Loss: 0.8198\n",
            "Batch [23973/28711], Loss: 0.8176\n",
            "Batch [23974/28711], Loss: 0.8865\n",
            "Batch [23975/28711], Loss: 0.8744\n",
            "Batch [23976/28711], Loss: 0.9562\n",
            "Batch [23977/28711], Loss: 0.8385\n",
            "Batch [23978/28711], Loss: 0.9846\n",
            "Batch [23979/28711], Loss: 0.8079\n",
            "Batch [23980/28711], Loss: 0.8242\n",
            "Batch [23981/28711], Loss: 0.9128\n",
            "Batch [23982/28711], Loss: 0.7962\n",
            "Batch [23983/28711], Loss: 0.6145\n",
            "Batch [23984/28711], Loss: 0.6916\n",
            "Batch [23985/28711], Loss: 0.9108\n",
            "Batch [23986/28711], Loss: 1.2828\n",
            "Batch [23987/28711], Loss: 0.6714\n",
            "Batch [23988/28711], Loss: 1.0069\n",
            "Batch [23989/28711], Loss: 0.7614\n",
            "Batch [23990/28711], Loss: 1.1026\n",
            "Batch [23991/28711], Loss: 0.9806\n",
            "Batch [23992/28711], Loss: 0.7692\n",
            "Batch [23993/28711], Loss: 1.0703\n",
            "Batch [23994/28711], Loss: 0.8273\n",
            "Batch [23995/28711], Loss: 0.9945\n",
            "Batch [23996/28711], Loss: 1.1457\n",
            "Batch [23997/28711], Loss: 0.5745\n",
            "Batch [23998/28711], Loss: 0.9544\n",
            "Batch [23999/28711], Loss: 1.0754\n",
            "Batch [24000/28711], Loss: 0.7651\n",
            "Batch [24001/28711], Loss: 0.9309\n",
            "Batch [24002/28711], Loss: 0.8171\n",
            "Batch [24003/28711], Loss: 0.8085\n",
            "Batch [24004/28711], Loss: 0.8294\n",
            "Batch [24005/28711], Loss: 0.7177\n",
            "Batch [24006/28711], Loss: 1.0128\n",
            "Batch [24007/28711], Loss: 0.8638\n",
            "Batch [24008/28711], Loss: 0.8498\n",
            "Batch [24009/28711], Loss: 0.6581\n",
            "Batch [24010/28711], Loss: 1.0034\n",
            "Batch [24011/28711], Loss: 0.6472\n",
            "Batch [24012/28711], Loss: 1.1048\n",
            "Batch [24013/28711], Loss: 1.0639\n",
            "Batch [24014/28711], Loss: 1.1106\n",
            "Batch [24015/28711], Loss: 0.8708\n",
            "Batch [24016/28711], Loss: 0.8433\n",
            "Batch [24017/28711], Loss: 0.8423\n",
            "Batch [24018/28711], Loss: 1.1345\n",
            "Batch [24019/28711], Loss: 0.8890\n",
            "Batch [24020/28711], Loss: 1.1915\n",
            "Batch [24021/28711], Loss: 0.9718\n",
            "Batch [24022/28711], Loss: 0.9949\n",
            "Batch [24023/28711], Loss: 0.8381\n",
            "Batch [24024/28711], Loss: 0.9280\n",
            "Batch [24025/28711], Loss: 1.1935\n",
            "Batch [24026/28711], Loss: 0.7224\n",
            "Batch [24027/28711], Loss: 1.0158\n",
            "Batch [24028/28711], Loss: 0.8496\n",
            "Batch [24029/28711], Loss: 0.9067\n",
            "Batch [24030/28711], Loss: 1.5548\n",
            "Batch [24031/28711], Loss: 1.0015\n",
            "Batch [24032/28711], Loss: 0.8306\n",
            "Batch [24033/28711], Loss: 1.0620\n",
            "Batch [24034/28711], Loss: 1.0572\n",
            "Batch [24035/28711], Loss: 0.6342\n",
            "Batch [24036/28711], Loss: 0.8257\n",
            "Batch [24037/28711], Loss: 0.6520\n",
            "Batch [24038/28711], Loss: 0.8097\n",
            "Batch [24039/28711], Loss: 1.0977\n",
            "Batch [24040/28711], Loss: 0.9189\n",
            "Batch [24041/28711], Loss: 1.0354\n",
            "Batch [24042/28711], Loss: 0.8068\n",
            "Batch [24043/28711], Loss: 0.8105\n",
            "Batch [24044/28711], Loss: 0.7537\n",
            "Batch [24045/28711], Loss: 1.1800\n",
            "Batch [24046/28711], Loss: 0.8685\n",
            "Batch [24047/28711], Loss: 0.6671\n",
            "Batch [24048/28711], Loss: 0.8183\n",
            "Batch [24049/28711], Loss: 0.8516\n",
            "Batch [24050/28711], Loss: 0.8849\n",
            "Batch [24051/28711], Loss: 0.7914\n",
            "Batch [24052/28711], Loss: 0.9452\n",
            "Batch [24053/28711], Loss: 0.8709\n",
            "Batch [24054/28711], Loss: 0.8384\n",
            "Batch [24055/28711], Loss: 0.9836\n",
            "Batch [24056/28711], Loss: 0.7403\n",
            "Batch [24057/28711], Loss: 0.7728\n",
            "Batch [24058/28711], Loss: 1.1275\n",
            "Batch [24059/28711], Loss: 1.0456\n",
            "Batch [24060/28711], Loss: 0.8322\n",
            "Batch [24061/28711], Loss: 0.8597\n",
            "Batch [24062/28711], Loss: 1.0655\n",
            "Batch [24063/28711], Loss: 0.8986\n",
            "Batch [24064/28711], Loss: 1.0265\n",
            "Batch [24065/28711], Loss: 0.9126\n",
            "Batch [24066/28711], Loss: 0.9071\n",
            "Batch [24067/28711], Loss: 0.9049\n",
            "Batch [24068/28711], Loss: 1.3002\n",
            "Batch [24069/28711], Loss: 1.0059\n",
            "Batch [24070/28711], Loss: 0.6930\n",
            "Batch [24071/28711], Loss: 0.8123\n",
            "Batch [24072/28711], Loss: 0.8429\n",
            "Batch [24073/28711], Loss: 1.1330\n",
            "Batch [24074/28711], Loss: 1.0418\n",
            "Batch [24075/28711], Loss: 1.0290\n",
            "Batch [24076/28711], Loss: 1.0115\n",
            "Batch [24077/28711], Loss: 0.8312\n",
            "Batch [24078/28711], Loss: 0.8596\n",
            "Batch [24079/28711], Loss: 1.1850\n",
            "Batch [24080/28711], Loss: 0.9197\n",
            "Batch [24081/28711], Loss: 0.7806\n",
            "Batch [24082/28711], Loss: 0.9186\n",
            "Batch [24083/28711], Loss: 0.7686\n",
            "Batch [24084/28711], Loss: 0.8027\n",
            "Batch [24085/28711], Loss: 0.8103\n",
            "Batch [24086/28711], Loss: 1.2881\n",
            "Batch [24087/28711], Loss: 1.0520\n",
            "Batch [24088/28711], Loss: 0.6541\n",
            "Batch [24089/28711], Loss: 0.9050\n",
            "Batch [24090/28711], Loss: 0.7999\n",
            "Batch [24091/28711], Loss: 0.8922\n",
            "Batch [24092/28711], Loss: 0.8731\n",
            "Batch [24093/28711], Loss: 0.9972\n",
            "Batch [24094/28711], Loss: 1.0488\n",
            "Batch [24095/28711], Loss: 0.7164\n",
            "Batch [24096/28711], Loss: 0.5833\n",
            "Batch [24097/28711], Loss: 0.7896\n",
            "Batch [24098/28711], Loss: 1.3503\n",
            "Batch [24099/28711], Loss: 0.8652\n",
            "Batch [24100/28711], Loss: 1.1482\n",
            "Batch [24101/28711], Loss: 1.0201\n",
            "Batch [24102/28711], Loss: 0.6072\n",
            "Batch [24103/28711], Loss: 1.1406\n",
            "Batch [24104/28711], Loss: 1.0915\n",
            "Batch [24105/28711], Loss: 1.1428\n",
            "Batch [24106/28711], Loss: 0.8926\n",
            "Batch [24107/28711], Loss: 0.9443\n",
            "Batch [24108/28711], Loss: 1.0517\n",
            "Batch [24109/28711], Loss: 1.0271\n",
            "Batch [24110/28711], Loss: 0.8641\n",
            "Batch [24111/28711], Loss: 0.6909\n",
            "Batch [24112/28711], Loss: 0.9122\n",
            "Batch [24113/28711], Loss: 0.7618\n",
            "Batch [24114/28711], Loss: 0.8548\n",
            "Batch [24115/28711], Loss: 0.8872\n",
            "Batch [24116/28711], Loss: 0.9675\n",
            "Batch [24117/28711], Loss: 0.6958\n",
            "Batch [24118/28711], Loss: 0.8540\n",
            "Batch [24119/28711], Loss: 0.9329\n",
            "Batch [24120/28711], Loss: 0.6560\n",
            "Batch [24121/28711], Loss: 1.0474\n",
            "Batch [24122/28711], Loss: 0.9651\n",
            "Batch [24123/28711], Loss: 0.9214\n",
            "Batch [24124/28711], Loss: 0.7200\n",
            "Batch [24125/28711], Loss: 0.7667\n",
            "Batch [24126/28711], Loss: 1.0939\n",
            "Batch [24127/28711], Loss: 1.0054\n",
            "Batch [24128/28711], Loss: 1.0442\n",
            "Batch [24129/28711], Loss: 0.5428\n",
            "Batch [24130/28711], Loss: 0.8010\n",
            "Batch [24131/28711], Loss: 0.9739\n",
            "Batch [24132/28711], Loss: 0.9972\n",
            "Batch [24133/28711], Loss: 0.7062\n",
            "Batch [24134/28711], Loss: 0.8542\n",
            "Batch [24135/28711], Loss: 0.8515\n",
            "Batch [24136/28711], Loss: 0.8644\n",
            "Batch [24137/28711], Loss: 0.7552\n",
            "Batch [24138/28711], Loss: 0.9789\n",
            "Batch [24139/28711], Loss: 0.9328\n",
            "Batch [24140/28711], Loss: 0.6768\n",
            "Batch [24141/28711], Loss: 0.7120\n",
            "Batch [24142/28711], Loss: 0.9064\n",
            "Batch [24143/28711], Loss: 1.1344\n",
            "Batch [24144/28711], Loss: 0.9127\n",
            "Batch [24145/28711], Loss: 0.7438\n",
            "Batch [24146/28711], Loss: 0.9664\n",
            "Batch [24147/28711], Loss: 1.0288\n",
            "Batch [24148/28711], Loss: 1.1826\n",
            "Batch [24149/28711], Loss: 0.8449\n",
            "Batch [24150/28711], Loss: 1.0836\n",
            "Batch [24151/28711], Loss: 0.9568\n",
            "Batch [24152/28711], Loss: 0.7938\n",
            "Batch [24153/28711], Loss: 0.9169\n",
            "Batch [24154/28711], Loss: 0.7414\n",
            "Batch [24155/28711], Loss: 0.8582\n",
            "Batch [24156/28711], Loss: 0.8498\n",
            "Batch [24157/28711], Loss: 1.0945\n",
            "Batch [24158/28711], Loss: 0.7750\n",
            "Batch [24159/28711], Loss: 0.8451\n",
            "Batch [24160/28711], Loss: 0.9850\n",
            "Batch [24161/28711], Loss: 0.9114\n",
            "Batch [24162/28711], Loss: 0.9450\n",
            "Batch [24163/28711], Loss: 0.8720\n",
            "Batch [24164/28711], Loss: 1.0105\n",
            "Batch [24165/28711], Loss: 0.8715\n",
            "Batch [24166/28711], Loss: 0.8211\n",
            "Batch [24167/28711], Loss: 1.0041\n",
            "Batch [24168/28711], Loss: 0.9285\n",
            "Batch [24169/28711], Loss: 0.7445\n",
            "Batch [24170/28711], Loss: 1.1987\n",
            "Batch [24171/28711], Loss: 0.8547\n",
            "Batch [24172/28711], Loss: 0.5858\n",
            "Batch [24173/28711], Loss: 0.8156\n",
            "Batch [24174/28711], Loss: 0.9571\n",
            "Batch [24175/28711], Loss: 1.0342\n",
            "Batch [24176/28711], Loss: 0.9533\n",
            "Batch [24177/28711], Loss: 0.9618\n",
            "Batch [24178/28711], Loss: 0.9891\n",
            "Batch [24179/28711], Loss: 0.7105\n",
            "Batch [24180/28711], Loss: 1.0507\n",
            "Batch [24181/28711], Loss: 0.8174\n",
            "Batch [24182/28711], Loss: 1.1991\n",
            "Batch [24183/28711], Loss: 0.8200\n",
            "Batch [24184/28711], Loss: 1.0208\n",
            "Batch [24185/28711], Loss: 0.7333\n",
            "Batch [24186/28711], Loss: 0.8897\n",
            "Batch [24187/28711], Loss: 0.8295\n",
            "Batch [24188/28711], Loss: 1.2819\n",
            "Batch [24189/28711], Loss: 1.1053\n",
            "Batch [24190/28711], Loss: 1.0725\n",
            "Batch [24191/28711], Loss: 0.9216\n",
            "Batch [24192/28711], Loss: 1.2226\n",
            "Batch [24193/28711], Loss: 1.0303\n",
            "Batch [24194/28711], Loss: 0.8539\n",
            "Batch [24195/28711], Loss: 1.0442\n",
            "Batch [24196/28711], Loss: 0.7943\n",
            "Batch [24197/28711], Loss: 1.0429\n",
            "Batch [24198/28711], Loss: 1.0744\n",
            "Batch [24199/28711], Loss: 1.1716\n",
            "Batch [24200/28711], Loss: 1.1764\n",
            "Batch [24201/28711], Loss: 0.7797\n",
            "Batch [24202/28711], Loss: 0.8348\n",
            "Batch [24203/28711], Loss: 0.8250\n",
            "Batch [24204/28711], Loss: 0.6404\n",
            "Batch [24205/28711], Loss: 0.9034\n",
            "Batch [24206/28711], Loss: 0.6453\n",
            "Batch [24207/28711], Loss: 0.8989\n",
            "Batch [24208/28711], Loss: 1.0819\n",
            "Batch [24209/28711], Loss: 1.0646\n",
            "Batch [24210/28711], Loss: 0.9223\n",
            "Batch [24211/28711], Loss: 0.8918\n",
            "Batch [24212/28711], Loss: 0.8558\n",
            "Batch [24213/28711], Loss: 1.3175\n",
            "Batch [24214/28711], Loss: 1.1014\n",
            "Batch [24215/28711], Loss: 0.7684\n",
            "Batch [24216/28711], Loss: 1.1061\n",
            "Batch [24217/28711], Loss: 0.8029\n",
            "Batch [24218/28711], Loss: 1.1880\n",
            "Batch [24219/28711], Loss: 0.9942\n",
            "Batch [24220/28711], Loss: 0.7975\n",
            "Batch [24221/28711], Loss: 0.9928\n",
            "Batch [24222/28711], Loss: 0.7396\n",
            "Batch [24223/28711], Loss: 0.7732\n",
            "Batch [24224/28711], Loss: 0.9169\n",
            "Batch [24225/28711], Loss: 0.8578\n",
            "Batch [24226/28711], Loss: 0.7744\n",
            "Batch [24227/28711], Loss: 0.9734\n",
            "Batch [24228/28711], Loss: 0.9122\n",
            "Batch [24229/28711], Loss: 0.9741\n",
            "Batch [24230/28711], Loss: 0.9228\n",
            "Batch [24231/28711], Loss: 0.7417\n",
            "Batch [24232/28711], Loss: 0.8081\n",
            "Batch [24233/28711], Loss: 0.7065\n",
            "Batch [24234/28711], Loss: 1.0289\n",
            "Batch [24235/28711], Loss: 1.0933\n",
            "Batch [24236/28711], Loss: 0.8583\n",
            "Batch [24237/28711], Loss: 0.9389\n",
            "Batch [24238/28711], Loss: 1.0145\n",
            "Batch [24239/28711], Loss: 0.9092\n",
            "Batch [24240/28711], Loss: 1.2459\n",
            "Batch [24241/28711], Loss: 0.8234\n",
            "Batch [24242/28711], Loss: 1.1149\n",
            "Batch [24243/28711], Loss: 0.9114\n",
            "Batch [24244/28711], Loss: 0.8404\n",
            "Batch [24245/28711], Loss: 1.0127\n",
            "Batch [24246/28711], Loss: 0.8072\n",
            "Batch [24247/28711], Loss: 1.2062\n",
            "Batch [24248/28711], Loss: 0.9432\n",
            "Batch [24249/28711], Loss: 1.1522\n",
            "Batch [24250/28711], Loss: 0.9152\n",
            "Batch [24251/28711], Loss: 0.8165\n",
            "Batch [24252/28711], Loss: 0.9613\n",
            "Batch [24253/28711], Loss: 1.0948\n",
            "Batch [24254/28711], Loss: 0.8010\n",
            "Batch [24255/28711], Loss: 1.0791\n",
            "Batch [24256/28711], Loss: 0.8675\n",
            "Batch [24257/28711], Loss: 0.9820\n",
            "Batch [24258/28711], Loss: 1.0335\n",
            "Batch [24259/28711], Loss: 0.9515\n",
            "Batch [24260/28711], Loss: 1.1127\n",
            "Batch [24261/28711], Loss: 0.8176\n",
            "Batch [24262/28711], Loss: 1.0430\n",
            "Batch [24263/28711], Loss: 0.8935\n",
            "Batch [24264/28711], Loss: 0.9920\n",
            "Batch [24265/28711], Loss: 0.8171\n",
            "Batch [24266/28711], Loss: 0.6599\n",
            "Batch [24267/28711], Loss: 0.9204\n",
            "Batch [24268/28711], Loss: 1.0037\n",
            "Batch [24269/28711], Loss: 1.1315\n",
            "Batch [24270/28711], Loss: 0.9694\n",
            "Batch [24271/28711], Loss: 0.9783\n",
            "Batch [24272/28711], Loss: 0.9124\n",
            "Batch [24273/28711], Loss: 0.7916\n",
            "Batch [24274/28711], Loss: 0.9949\n",
            "Batch [24275/28711], Loss: 1.1024\n",
            "Batch [24276/28711], Loss: 0.9697\n",
            "Batch [24277/28711], Loss: 0.7889\n",
            "Batch [24278/28711], Loss: 0.8570\n",
            "Batch [24279/28711], Loss: 0.7629\n",
            "Batch [24280/28711], Loss: 0.8692\n",
            "Batch [24281/28711], Loss: 0.9613\n",
            "Batch [24282/28711], Loss: 0.9037\n",
            "Batch [24283/28711], Loss: 0.6163\n",
            "Batch [24284/28711], Loss: 1.3915\n",
            "Batch [24285/28711], Loss: 0.7734\n",
            "Batch [24286/28711], Loss: 0.8341\n",
            "Batch [24287/28711], Loss: 0.7470\n",
            "Batch [24288/28711], Loss: 0.7554\n",
            "Batch [24289/28711], Loss: 1.1524\n",
            "Batch [24290/28711], Loss: 0.8809\n",
            "Batch [24291/28711], Loss: 0.8495\n",
            "Batch [24292/28711], Loss: 1.1840\n",
            "Batch [24293/28711], Loss: 0.6128\n",
            "Batch [24294/28711], Loss: 1.2089\n",
            "Batch [24295/28711], Loss: 0.9440\n",
            "Batch [24296/28711], Loss: 0.8646\n",
            "Batch [24297/28711], Loss: 0.9011\n",
            "Batch [24298/28711], Loss: 1.0663\n",
            "Batch [24299/28711], Loss: 0.6986\n",
            "Batch [24300/28711], Loss: 0.8844\n",
            "Batch [24301/28711], Loss: 1.0335\n",
            "Batch [24302/28711], Loss: 0.8409\n",
            "Batch [24303/28711], Loss: 0.8945\n",
            "Batch [24304/28711], Loss: 1.2208\n",
            "Batch [24305/28711], Loss: 1.1943\n",
            "Batch [24306/28711], Loss: 1.2316\n",
            "Batch [24307/28711], Loss: 0.9233\n",
            "Batch [24308/28711], Loss: 0.8293\n",
            "Batch [24309/28711], Loss: 0.9990\n",
            "Batch [24310/28711], Loss: 0.8850\n",
            "Batch [24311/28711], Loss: 0.9144\n",
            "Batch [24312/28711], Loss: 1.1404\n",
            "Batch [24313/28711], Loss: 0.9823\n",
            "Batch [24314/28711], Loss: 0.6608\n",
            "Batch [24315/28711], Loss: 0.9330\n",
            "Batch [24316/28711], Loss: 0.8421\n",
            "Batch [24317/28711], Loss: 0.7918\n",
            "Batch [24318/28711], Loss: 0.8956\n",
            "Batch [24319/28711], Loss: 0.8743\n",
            "Batch [24320/28711], Loss: 1.1864\n",
            "Batch [24321/28711], Loss: 0.8466\n",
            "Batch [24322/28711], Loss: 0.8829\n",
            "Batch [24323/28711], Loss: 0.9175\n",
            "Batch [24324/28711], Loss: 0.9480\n",
            "Batch [24325/28711], Loss: 0.7821\n",
            "Batch [24326/28711], Loss: 0.8943\n",
            "Batch [24327/28711], Loss: 0.8610\n",
            "Batch [24328/28711], Loss: 0.7435\n",
            "Batch [24329/28711], Loss: 0.5830\n",
            "Batch [24330/28711], Loss: 0.9934\n",
            "Batch [24331/28711], Loss: 0.7848\n",
            "Batch [24332/28711], Loss: 0.8942\n",
            "Batch [24333/28711], Loss: 0.8356\n",
            "Batch [24334/28711], Loss: 0.6978\n",
            "Batch [24335/28711], Loss: 0.6400\n",
            "Batch [24336/28711], Loss: 0.7931\n",
            "Batch [24337/28711], Loss: 0.7948\n",
            "Batch [24338/28711], Loss: 0.7688\n",
            "Batch [24339/28711], Loss: 0.9997\n",
            "Batch [24340/28711], Loss: 1.0160\n",
            "Batch [24341/28711], Loss: 0.9316\n",
            "Batch [24342/28711], Loss: 0.6953\n",
            "Batch [24343/28711], Loss: 1.2109\n",
            "Batch [24344/28711], Loss: 0.8518\n",
            "Batch [24345/28711], Loss: 0.9940\n",
            "Batch [24346/28711], Loss: 0.7174\n",
            "Batch [24347/28711], Loss: 1.0512\n",
            "Batch [24348/28711], Loss: 1.0686\n",
            "Batch [24349/28711], Loss: 0.6182\n",
            "Batch [24350/28711], Loss: 0.8065\n",
            "Batch [24351/28711], Loss: 0.9668\n",
            "Batch [24352/28711], Loss: 0.9276\n",
            "Batch [24353/28711], Loss: 0.7591\n",
            "Batch [24354/28711], Loss: 1.0267\n",
            "Batch [24355/28711], Loss: 0.9379\n",
            "Batch [24356/28711], Loss: 0.7095\n",
            "Batch [24357/28711], Loss: 1.0530\n",
            "Batch [24358/28711], Loss: 0.9146\n",
            "Batch [24359/28711], Loss: 0.9567\n",
            "Batch [24360/28711], Loss: 0.8470\n",
            "Batch [24361/28711], Loss: 0.9887\n",
            "Batch [24362/28711], Loss: 0.8995\n",
            "Batch [24363/28711], Loss: 1.0498\n",
            "Batch [24364/28711], Loss: 0.8606\n",
            "Batch [24365/28711], Loss: 0.8543\n",
            "Batch [24366/28711], Loss: 0.7361\n",
            "Batch [24367/28711], Loss: 0.8080\n",
            "Batch [24368/28711], Loss: 0.9540\n",
            "Batch [24369/28711], Loss: 0.7608\n",
            "Batch [24370/28711], Loss: 0.9715\n",
            "Batch [24371/28711], Loss: 0.6154\n",
            "Batch [24372/28711], Loss: 1.1109\n",
            "Batch [24373/28711], Loss: 0.8337\n",
            "Batch [24374/28711], Loss: 0.9247\n",
            "Batch [24375/28711], Loss: 0.8974\n",
            "Batch [24376/28711], Loss: 0.9748\n",
            "Batch [24377/28711], Loss: 0.9008\n",
            "Batch [24378/28711], Loss: 0.9690\n",
            "Batch [24379/28711], Loss: 0.7374\n",
            "Batch [24380/28711], Loss: 1.0316\n",
            "Batch [24381/28711], Loss: 0.7903\n",
            "Batch [24382/28711], Loss: 0.7515\n",
            "Batch [24383/28711], Loss: 0.9299\n",
            "Batch [24384/28711], Loss: 0.8937\n",
            "Batch [24385/28711], Loss: 0.9300\n",
            "Batch [24386/28711], Loss: 0.9740\n",
            "Batch [24387/28711], Loss: 0.6453\n",
            "Batch [24388/28711], Loss: 1.0570\n",
            "Batch [24389/28711], Loss: 0.7615\n",
            "Batch [24390/28711], Loss: 0.8842\n",
            "Batch [24391/28711], Loss: 0.9518\n",
            "Batch [24392/28711], Loss: 1.0921\n",
            "Batch [24393/28711], Loss: 0.9244\n",
            "Batch [24394/28711], Loss: 1.3645\n",
            "Batch [24395/28711], Loss: 0.8644\n",
            "Batch [24396/28711], Loss: 0.7943\n",
            "Batch [24397/28711], Loss: 1.1267\n",
            "Batch [24398/28711], Loss: 0.6813\n",
            "Batch [24399/28711], Loss: 0.9702\n",
            "Batch [24400/28711], Loss: 0.7164\n",
            "Batch [24401/28711], Loss: 0.6711\n",
            "Batch [24402/28711], Loss: 0.7195\n",
            "Batch [24403/28711], Loss: 1.0067\n",
            "Batch [24404/28711], Loss: 1.0126\n",
            "Batch [24405/28711], Loss: 1.0735\n",
            "Batch [24406/28711], Loss: 0.9893\n",
            "Batch [24407/28711], Loss: 1.0299\n",
            "Batch [24408/28711], Loss: 1.1926\n",
            "Batch [24409/28711], Loss: 0.9834\n",
            "Batch [24410/28711], Loss: 0.6548\n",
            "Batch [24411/28711], Loss: 0.9226\n",
            "Batch [24412/28711], Loss: 1.0670\n",
            "Batch [24413/28711], Loss: 0.8795\n",
            "Batch [24414/28711], Loss: 1.1310\n",
            "Batch [24415/28711], Loss: 1.2427\n",
            "Batch [24416/28711], Loss: 0.8716\n",
            "Batch [24417/28711], Loss: 0.7540\n",
            "Batch [24418/28711], Loss: 0.7112\n",
            "Batch [24419/28711], Loss: 0.9279\n",
            "Batch [24420/28711], Loss: 0.9859\n",
            "Batch [24421/28711], Loss: 1.0689\n",
            "Batch [24422/28711], Loss: 0.8143\n",
            "Batch [24423/28711], Loss: 1.2033\n",
            "Batch [24424/28711], Loss: 0.8977\n",
            "Batch [24425/28711], Loss: 0.7281\n",
            "Batch [24426/28711], Loss: 1.0200\n",
            "Batch [24427/28711], Loss: 1.0260\n",
            "Batch [24428/28711], Loss: 0.7630\n",
            "Batch [24429/28711], Loss: 0.8618\n",
            "Batch [24430/28711], Loss: 0.7154\n",
            "Batch [24431/28711], Loss: 0.9839\n",
            "Batch [24432/28711], Loss: 0.9423\n",
            "Batch [24433/28711], Loss: 1.1892\n",
            "Batch [24434/28711], Loss: 0.9982\n",
            "Batch [24435/28711], Loss: 0.8339\n",
            "Batch [24436/28711], Loss: 0.7415\n",
            "Batch [24437/28711], Loss: 0.9452\n",
            "Batch [24438/28711], Loss: 1.2867\n",
            "Batch [24439/28711], Loss: 1.0731\n",
            "Batch [24440/28711], Loss: 0.9965\n",
            "Batch [24441/28711], Loss: 0.7370\n",
            "Batch [24442/28711], Loss: 0.7611\n",
            "Batch [24443/28711], Loss: 0.9003\n",
            "Batch [24444/28711], Loss: 1.0304\n",
            "Batch [24445/28711], Loss: 0.6151\n",
            "Batch [24446/28711], Loss: 0.6936\n",
            "Batch [24447/28711], Loss: 1.0239\n",
            "Batch [24448/28711], Loss: 0.6671\n",
            "Batch [24449/28711], Loss: 0.8095\n",
            "Batch [24450/28711], Loss: 0.7315\n",
            "Batch [24451/28711], Loss: 0.7933\n",
            "Batch [24452/28711], Loss: 0.8520\n",
            "Batch [24453/28711], Loss: 0.7251\n",
            "Batch [24454/28711], Loss: 0.7111\n",
            "Batch [24455/28711], Loss: 0.7156\n",
            "Batch [24456/28711], Loss: 0.8381\n",
            "Batch [24457/28711], Loss: 0.9009\n",
            "Batch [24458/28711], Loss: 1.0020\n",
            "Batch [24459/28711], Loss: 0.8961\n",
            "Batch [24460/28711], Loss: 0.8872\n",
            "Batch [24461/28711], Loss: 1.1597\n",
            "Batch [24462/28711], Loss: 1.0852\n",
            "Batch [24463/28711], Loss: 1.1034\n",
            "Batch [24464/28711], Loss: 0.5858\n",
            "Batch [24465/28711], Loss: 1.0729\n",
            "Batch [24466/28711], Loss: 0.9148\n",
            "Batch [24467/28711], Loss: 0.9711\n",
            "Batch [24468/28711], Loss: 0.5655\n",
            "Batch [24469/28711], Loss: 0.9167\n",
            "Batch [24470/28711], Loss: 0.9826\n",
            "Batch [24471/28711], Loss: 1.1873\n",
            "Batch [24472/28711], Loss: 1.1046\n",
            "Batch [24473/28711], Loss: 0.8393\n",
            "Batch [24474/28711], Loss: 0.8323\n",
            "Batch [24475/28711], Loss: 1.4247\n",
            "Batch [24476/28711], Loss: 1.1541\n",
            "Batch [24477/28711], Loss: 0.8243\n",
            "Batch [24478/28711], Loss: 0.8667\n",
            "Batch [24479/28711], Loss: 1.0382\n",
            "Batch [24480/28711], Loss: 1.1156\n",
            "Batch [24481/28711], Loss: 0.9729\n",
            "Batch [24482/28711], Loss: 0.7378\n",
            "Batch [24483/28711], Loss: 0.8962\n",
            "Batch [24484/28711], Loss: 0.7048\n",
            "Batch [24485/28711], Loss: 0.8089\n",
            "Batch [24486/28711], Loss: 0.8078\n",
            "Batch [24487/28711], Loss: 0.8569\n",
            "Batch [24488/28711], Loss: 0.8805\n",
            "Batch [24489/28711], Loss: 0.9587\n",
            "Batch [24490/28711], Loss: 0.5993\n",
            "Batch [24491/28711], Loss: 0.7367\n",
            "Batch [24492/28711], Loss: 0.9546\n",
            "Batch [24493/28711], Loss: 0.7965\n",
            "Batch [24494/28711], Loss: 1.0511\n",
            "Batch [24495/28711], Loss: 0.7752\n",
            "Batch [24496/28711], Loss: 0.7760\n",
            "Batch [24497/28711], Loss: 0.7938\n",
            "Batch [24498/28711], Loss: 0.8554\n",
            "Batch [24499/28711], Loss: 0.9325\n",
            "Batch [24500/28711], Loss: 0.8504\n",
            "Batch [24501/28711], Loss: 0.9073\n",
            "Batch [24502/28711], Loss: 0.8612\n",
            "Batch [24503/28711], Loss: 0.8032\n",
            "Batch [24504/28711], Loss: 1.0081\n",
            "Batch [24505/28711], Loss: 0.8822\n",
            "Batch [24506/28711], Loss: 1.1941\n",
            "Batch [24507/28711], Loss: 0.8807\n",
            "Batch [24508/28711], Loss: 0.8808\n",
            "Batch [24509/28711], Loss: 1.1109\n",
            "Batch [24510/28711], Loss: 0.8154\n",
            "Batch [24511/28711], Loss: 0.8878\n",
            "Batch [24512/28711], Loss: 0.8159\n",
            "Batch [24513/28711], Loss: 0.9896\n",
            "Batch [24514/28711], Loss: 0.7464\n",
            "Batch [24515/28711], Loss: 1.1206\n",
            "Batch [24516/28711], Loss: 0.6994\n",
            "Batch [24517/28711], Loss: 0.9253\n",
            "Batch [24518/28711], Loss: 1.0665\n",
            "Batch [24519/28711], Loss: 0.8570\n",
            "Batch [24520/28711], Loss: 0.9968\n",
            "Batch [24521/28711], Loss: 0.9148\n",
            "Batch [24522/28711], Loss: 0.9558\n",
            "Batch [24523/28711], Loss: 0.9625\n",
            "Batch [24524/28711], Loss: 0.8567\n",
            "Batch [24525/28711], Loss: 1.0992\n",
            "Batch [24526/28711], Loss: 0.8570\n",
            "Batch [24527/28711], Loss: 0.8400\n",
            "Batch [24528/28711], Loss: 1.0267\n",
            "Batch [24529/28711], Loss: 1.0698\n",
            "Batch [24530/28711], Loss: 0.8997\n",
            "Batch [24531/28711], Loss: 0.7955\n",
            "Batch [24532/28711], Loss: 0.8265\n",
            "Batch [24533/28711], Loss: 1.0459\n",
            "Batch [24534/28711], Loss: 1.0818\n",
            "Batch [24535/28711], Loss: 0.9425\n",
            "Batch [24536/28711], Loss: 0.7577\n",
            "Batch [24537/28711], Loss: 0.9583\n",
            "Batch [24538/28711], Loss: 0.9283\n",
            "Batch [24539/28711], Loss: 0.9277\n",
            "Batch [24540/28711], Loss: 1.0320\n",
            "Batch [24541/28711], Loss: 1.0170\n",
            "Batch [24542/28711], Loss: 0.8020\n",
            "Batch [24543/28711], Loss: 0.7900\n",
            "Batch [24544/28711], Loss: 0.9129\n",
            "Batch [24545/28711], Loss: 0.9783\n",
            "Batch [24546/28711], Loss: 0.9474\n",
            "Batch [24547/28711], Loss: 0.7326\n",
            "Batch [24548/28711], Loss: 1.2180\n",
            "Batch [24549/28711], Loss: 0.8323\n",
            "Batch [24550/28711], Loss: 0.9141\n",
            "Batch [24551/28711], Loss: 0.6597\n",
            "Batch [24552/28711], Loss: 1.1164\n",
            "Batch [24553/28711], Loss: 0.7855\n",
            "Batch [24554/28711], Loss: 0.7855\n",
            "Batch [24555/28711], Loss: 0.7904\n",
            "Batch [24556/28711], Loss: 0.9698\n",
            "Batch [24557/28711], Loss: 1.0246\n",
            "Batch [24558/28711], Loss: 1.0044\n",
            "Batch [24559/28711], Loss: 1.0208\n",
            "Batch [24560/28711], Loss: 0.8924\n",
            "Batch [24561/28711], Loss: 0.9160\n",
            "Batch [24562/28711], Loss: 0.8151\n",
            "Batch [24563/28711], Loss: 0.7372\n",
            "Batch [24564/28711], Loss: 0.9119\n",
            "Batch [24565/28711], Loss: 0.7794\n",
            "Batch [24566/28711], Loss: 0.6457\n",
            "Batch [24567/28711], Loss: 0.8339\n",
            "Batch [24568/28711], Loss: 0.7864\n",
            "Batch [24569/28711], Loss: 0.7245\n",
            "Batch [24570/28711], Loss: 0.6720\n",
            "Batch [24571/28711], Loss: 0.7876\n",
            "Batch [24572/28711], Loss: 0.7430\n",
            "Batch [24573/28711], Loss: 1.0345\n",
            "Batch [24574/28711], Loss: 1.0325\n",
            "Batch [24575/28711], Loss: 0.9314\n",
            "Batch [24576/28711], Loss: 0.8175\n",
            "Batch [24577/28711], Loss: 0.8732\n",
            "Batch [24578/28711], Loss: 1.0088\n",
            "Batch [24579/28711], Loss: 0.8225\n",
            "Batch [24580/28711], Loss: 1.1306\n",
            "Batch [24581/28711], Loss: 0.8928\n",
            "Batch [24582/28711], Loss: 0.8563\n",
            "Batch [24583/28711], Loss: 0.7680\n",
            "Batch [24584/28711], Loss: 0.7309\n",
            "Batch [24585/28711], Loss: 1.1435\n",
            "Batch [24586/28711], Loss: 0.9258\n",
            "Batch [24587/28711], Loss: 0.9742\n",
            "Batch [24588/28711], Loss: 0.6911\n",
            "Batch [24589/28711], Loss: 1.0545\n",
            "Batch [24590/28711], Loss: 0.9004\n",
            "Batch [24591/28711], Loss: 0.8676\n",
            "Batch [24592/28711], Loss: 1.1386\n",
            "Batch [24593/28711], Loss: 0.8595\n",
            "Batch [24594/28711], Loss: 0.8150\n",
            "Batch [24595/28711], Loss: 1.0011\n",
            "Batch [24596/28711], Loss: 1.0032\n",
            "Batch [24597/28711], Loss: 0.7156\n",
            "Batch [24598/28711], Loss: 0.9719\n",
            "Batch [24599/28711], Loss: 0.9858\n",
            "Batch [24600/28711], Loss: 0.6918\n",
            "Batch [24601/28711], Loss: 0.8891\n",
            "Batch [24602/28711], Loss: 0.8298\n",
            "Batch [24603/28711], Loss: 0.7652\n",
            "Batch [24604/28711], Loss: 0.9969\n",
            "Batch [24605/28711], Loss: 0.8896\n",
            "Batch [24606/28711], Loss: 0.9139\n",
            "Batch [24607/28711], Loss: 0.7328\n",
            "Batch [24608/28711], Loss: 0.7377\n",
            "Batch [24609/28711], Loss: 0.9434\n",
            "Batch [24610/28711], Loss: 0.8751\n",
            "Batch [24611/28711], Loss: 0.9960\n",
            "Batch [24612/28711], Loss: 1.1313\n",
            "Batch [24613/28711], Loss: 1.1932\n",
            "Batch [24614/28711], Loss: 1.0300\n",
            "Batch [24615/28711], Loss: 0.7932\n",
            "Batch [24616/28711], Loss: 1.0754\n",
            "Batch [24617/28711], Loss: 0.9353\n",
            "Batch [24618/28711], Loss: 0.7884\n",
            "Batch [24619/28711], Loss: 0.8603\n",
            "Batch [24620/28711], Loss: 0.9526\n",
            "Batch [24621/28711], Loss: 0.8812\n",
            "Batch [24622/28711], Loss: 0.9709\n",
            "Batch [24623/28711], Loss: 1.0226\n",
            "Batch [24624/28711], Loss: 0.9598\n",
            "Batch [24625/28711], Loss: 0.8595\n",
            "Batch [24626/28711], Loss: 0.9191\n",
            "Batch [24627/28711], Loss: 0.8675\n",
            "Batch [24628/28711], Loss: 0.9501\n",
            "Batch [24629/28711], Loss: 0.6927\n",
            "Batch [24630/28711], Loss: 0.8762\n",
            "Batch [24631/28711], Loss: 0.8752\n",
            "Batch [24632/28711], Loss: 0.8017\n",
            "Batch [24633/28711], Loss: 0.9378\n",
            "Batch [24634/28711], Loss: 0.8424\n",
            "Batch [24635/28711], Loss: 0.9852\n",
            "Batch [24636/28711], Loss: 1.0030\n",
            "Batch [24637/28711], Loss: 0.8469\n",
            "Batch [24638/28711], Loss: 0.9681\n",
            "Batch [24639/28711], Loss: 0.9193\n",
            "Batch [24640/28711], Loss: 0.7364\n",
            "Batch [24641/28711], Loss: 0.7854\n",
            "Batch [24642/28711], Loss: 0.9451\n",
            "Batch [24643/28711], Loss: 0.8675\n",
            "Batch [24644/28711], Loss: 0.8330\n",
            "Batch [24645/28711], Loss: 1.0284\n",
            "Batch [24646/28711], Loss: 0.8123\n",
            "Batch [24647/28711], Loss: 0.7524\n",
            "Batch [24648/28711], Loss: 0.7734\n",
            "Batch [24649/28711], Loss: 0.9479\n",
            "Batch [24650/28711], Loss: 1.2851\n",
            "Batch [24651/28711], Loss: 0.9673\n",
            "Batch [24652/28711], Loss: 0.9143\n",
            "Batch [24653/28711], Loss: 0.9255\n",
            "Batch [24654/28711], Loss: 0.9766\n",
            "Batch [24655/28711], Loss: 0.8600\n",
            "Batch [24656/28711], Loss: 0.8415\n",
            "Batch [24657/28711], Loss: 0.7547\n",
            "Batch [24658/28711], Loss: 1.0139\n",
            "Batch [24659/28711], Loss: 1.0907\n",
            "Batch [24660/28711], Loss: 1.0322\n",
            "Batch [24661/28711], Loss: 0.9367\n",
            "Batch [24662/28711], Loss: 0.8418\n",
            "Batch [24663/28711], Loss: 0.8107\n",
            "Batch [24664/28711], Loss: 0.6346\n",
            "Batch [24665/28711], Loss: 0.9296\n",
            "Batch [24666/28711], Loss: 1.2676\n",
            "Batch [24667/28711], Loss: 0.9817\n",
            "Batch [24668/28711], Loss: 0.8841\n",
            "Batch [24669/28711], Loss: 0.9583\n",
            "Batch [24670/28711], Loss: 0.6959\n",
            "Batch [24671/28711], Loss: 0.9128\n",
            "Batch [24672/28711], Loss: 1.0629\n",
            "Batch [24673/28711], Loss: 0.7421\n",
            "Batch [24674/28711], Loss: 1.2554\n",
            "Batch [24675/28711], Loss: 0.9753\n",
            "Batch [24676/28711], Loss: 0.9090\n",
            "Batch [24677/28711], Loss: 0.9280\n",
            "Batch [24678/28711], Loss: 0.7731\n",
            "Batch [24679/28711], Loss: 1.0561\n",
            "Batch [24680/28711], Loss: 0.8588\n",
            "Batch [24681/28711], Loss: 1.0766\n",
            "Batch [24682/28711], Loss: 0.8115\n",
            "Batch [24683/28711], Loss: 0.7224\n",
            "Batch [24684/28711], Loss: 0.8155\n",
            "Batch [24685/28711], Loss: 0.9300\n",
            "Batch [24686/28711], Loss: 1.0410\n",
            "Batch [24687/28711], Loss: 0.9515\n",
            "Batch [24688/28711], Loss: 1.6243\n",
            "Batch [24689/28711], Loss: 0.8519\n",
            "Batch [24690/28711], Loss: 0.9449\n",
            "Batch [24691/28711], Loss: 0.8009\n",
            "Batch [24692/28711], Loss: 0.8979\n",
            "Batch [24693/28711], Loss: 0.8562\n",
            "Batch [24694/28711], Loss: 0.8314\n",
            "Batch [24695/28711], Loss: 0.9492\n",
            "Batch [24696/28711], Loss: 0.8472\n",
            "Batch [24697/28711], Loss: 1.0392\n",
            "Batch [24698/28711], Loss: 0.9406\n",
            "Batch [24699/28711], Loss: 0.6766\n",
            "Batch [24700/28711], Loss: 0.9543\n",
            "Batch [24701/28711], Loss: 1.0130\n",
            "Batch [24702/28711], Loss: 0.8080\n",
            "Batch [24703/28711], Loss: 0.6304\n",
            "Batch [24704/28711], Loss: 0.8606\n",
            "Batch [24705/28711], Loss: 0.9041\n",
            "Batch [24706/28711], Loss: 1.0226\n",
            "Batch [24707/28711], Loss: 0.9374\n",
            "Batch [24708/28711], Loss: 0.8959\n",
            "Batch [24709/28711], Loss: 0.5387\n",
            "Batch [24710/28711], Loss: 0.7656\n",
            "Batch [24711/28711], Loss: 0.5608\n",
            "Batch [24712/28711], Loss: 0.7392\n",
            "Batch [24713/28711], Loss: 0.8173\n",
            "Batch [24714/28711], Loss: 0.8497\n",
            "Batch [24715/28711], Loss: 0.8798\n",
            "Batch [24716/28711], Loss: 1.0095\n",
            "Batch [24717/28711], Loss: 0.8519\n",
            "Batch [24718/28711], Loss: 0.8614\n",
            "Batch [24719/28711], Loss: 0.9412\n",
            "Batch [24720/28711], Loss: 0.8144\n",
            "Batch [24721/28711], Loss: 0.7907\n",
            "Batch [24722/28711], Loss: 0.8564\n",
            "Batch [24723/28711], Loss: 0.8326\n",
            "Batch [24724/28711], Loss: 0.7845\n",
            "Batch [24725/28711], Loss: 0.9509\n",
            "Batch [24726/28711], Loss: 1.0538\n",
            "Batch [24727/28711], Loss: 1.0555\n",
            "Batch [24728/28711], Loss: 1.2231\n",
            "Batch [24729/28711], Loss: 0.9564\n",
            "Batch [24730/28711], Loss: 1.3443\n",
            "Batch [24731/28711], Loss: 0.7528\n",
            "Batch [24732/28711], Loss: 0.7561\n",
            "Batch [24733/28711], Loss: 0.6282\n",
            "Batch [24734/28711], Loss: 1.0565\n",
            "Batch [24735/28711], Loss: 1.2004\n",
            "Batch [24736/28711], Loss: 1.1426\n",
            "Batch [24737/28711], Loss: 0.9807\n",
            "Batch [24738/28711], Loss: 1.0876\n",
            "Batch [24739/28711], Loss: 1.0116\n",
            "Batch [24740/28711], Loss: 1.0983\n",
            "Batch [24741/28711], Loss: 0.8648\n",
            "Batch [24742/28711], Loss: 0.8927\n",
            "Batch [24743/28711], Loss: 0.8537\n",
            "Batch [24744/28711], Loss: 0.8095\n",
            "Batch [24745/28711], Loss: 1.1237\n",
            "Batch [24746/28711], Loss: 0.6170\n",
            "Batch [24747/28711], Loss: 0.7478\n",
            "Batch [24748/28711], Loss: 0.8432\n",
            "Batch [24749/28711], Loss: 0.8753\n",
            "Batch [24750/28711], Loss: 0.9355\n",
            "Batch [24751/28711], Loss: 0.8661\n",
            "Batch [24752/28711], Loss: 1.0598\n",
            "Batch [24753/28711], Loss: 0.8969\n",
            "Batch [24754/28711], Loss: 0.9437\n",
            "Batch [24755/28711], Loss: 0.8899\n",
            "Batch [24756/28711], Loss: 0.8514\n",
            "Batch [24757/28711], Loss: 1.0212\n",
            "Batch [24758/28711], Loss: 0.9220\n",
            "Batch [24759/28711], Loss: 0.9185\n",
            "Batch [24760/28711], Loss: 1.0279\n",
            "Batch [24761/28711], Loss: 1.1559\n",
            "Batch [24762/28711], Loss: 0.8081\n",
            "Batch [24763/28711], Loss: 1.1009\n",
            "Batch [24764/28711], Loss: 1.2110\n",
            "Batch [24765/28711], Loss: 0.8099\n",
            "Batch [24766/28711], Loss: 1.0025\n",
            "Batch [24767/28711], Loss: 1.0161\n",
            "Batch [24768/28711], Loss: 1.2581\n",
            "Batch [24769/28711], Loss: 0.6497\n",
            "Batch [24770/28711], Loss: 0.8253\n",
            "Batch [24771/28711], Loss: 0.8265\n",
            "Batch [24772/28711], Loss: 0.8570\n",
            "Batch [24773/28711], Loss: 0.8871\n",
            "Batch [24774/28711], Loss: 0.9359\n",
            "Batch [24775/28711], Loss: 0.8607\n",
            "Batch [24776/28711], Loss: 0.6703\n",
            "Batch [24777/28711], Loss: 1.1023\n",
            "Batch [24778/28711], Loss: 1.2863\n",
            "Batch [24779/28711], Loss: 0.8625\n",
            "Batch [24780/28711], Loss: 0.9835\n",
            "Batch [24781/28711], Loss: 1.0642\n",
            "Batch [24782/28711], Loss: 0.9002\n",
            "Batch [24783/28711], Loss: 1.0939\n",
            "Batch [24784/28711], Loss: 1.1995\n",
            "Batch [24785/28711], Loss: 1.2075\n",
            "Batch [24786/28711], Loss: 0.8696\n",
            "Batch [24787/28711], Loss: 1.0534\n",
            "Batch [24788/28711], Loss: 0.9214\n",
            "Batch [24789/28711], Loss: 0.8130\n",
            "Batch [24790/28711], Loss: 0.7096\n",
            "Batch [24791/28711], Loss: 1.0719\n",
            "Batch [24792/28711], Loss: 0.9733\n",
            "Batch [24793/28711], Loss: 0.8458\n",
            "Batch [24794/28711], Loss: 1.2245\n",
            "Batch [24795/28711], Loss: 0.9909\n",
            "Batch [24796/28711], Loss: 0.9170\n",
            "Batch [24797/28711], Loss: 1.0970\n",
            "Batch [24798/28711], Loss: 0.9649\n",
            "Batch [24799/28711], Loss: 0.8872\n",
            "Batch [24800/28711], Loss: 0.7858\n",
            "Batch [24801/28711], Loss: 0.9700\n",
            "Batch [24802/28711], Loss: 0.7934\n",
            "Batch [24803/28711], Loss: 0.7606\n",
            "Batch [24804/28711], Loss: 1.1797\n",
            "Batch [24805/28711], Loss: 1.1060\n",
            "Batch [24806/28711], Loss: 1.0797\n",
            "Batch [24807/28711], Loss: 1.0944\n",
            "Batch [24808/28711], Loss: 0.7483\n",
            "Batch [24809/28711], Loss: 0.9291\n",
            "Batch [24810/28711], Loss: 1.1817\n",
            "Batch [24811/28711], Loss: 0.8941\n",
            "Batch [24812/28711], Loss: 0.7389\n",
            "Batch [24813/28711], Loss: 0.8405\n",
            "Batch [24814/28711], Loss: 0.8821\n",
            "Batch [24815/28711], Loss: 0.9104\n",
            "Batch [24816/28711], Loss: 0.7362\n",
            "Batch [24817/28711], Loss: 0.9980\n",
            "Batch [24818/28711], Loss: 0.6451\n",
            "Batch [24819/28711], Loss: 1.1117\n",
            "Batch [24820/28711], Loss: 0.8091\n",
            "Batch [24821/28711], Loss: 1.1065\n",
            "Batch [24822/28711], Loss: 0.7738\n",
            "Batch [24823/28711], Loss: 1.0441\n",
            "Batch [24824/28711], Loss: 0.9000\n",
            "Batch [24825/28711], Loss: 0.7389\n",
            "Batch [24826/28711], Loss: 0.9425\n",
            "Batch [24827/28711], Loss: 0.7081\n",
            "Batch [24828/28711], Loss: 0.9227\n",
            "Batch [24829/28711], Loss: 0.8358\n",
            "Batch [24830/28711], Loss: 1.2008\n",
            "Batch [24831/28711], Loss: 1.1639\n",
            "Batch [24832/28711], Loss: 0.9265\n",
            "Batch [24833/28711], Loss: 0.9862\n",
            "Batch [24834/28711], Loss: 1.1569\n",
            "Batch [24835/28711], Loss: 0.9723\n",
            "Batch [24836/28711], Loss: 0.9125\n",
            "Batch [24837/28711], Loss: 0.8300\n",
            "Batch [24838/28711], Loss: 1.0345\n",
            "Batch [24839/28711], Loss: 1.0209\n",
            "Batch [24840/28711], Loss: 1.0143\n",
            "Batch [24841/28711], Loss: 0.7401\n",
            "Batch [24842/28711], Loss: 0.8673\n",
            "Batch [24843/28711], Loss: 0.7875\n",
            "Batch [24844/28711], Loss: 0.9117\n",
            "Batch [24845/28711], Loss: 0.9972\n",
            "Batch [24846/28711], Loss: 1.0057\n",
            "Batch [24847/28711], Loss: 1.0012\n",
            "Batch [24848/28711], Loss: 0.9415\n",
            "Batch [24849/28711], Loss: 0.8607\n",
            "Batch [24850/28711], Loss: 0.9079\n",
            "Batch [24851/28711], Loss: 0.9574\n",
            "Batch [24852/28711], Loss: 0.7669\n",
            "Batch [24853/28711], Loss: 0.9344\n",
            "Batch [24854/28711], Loss: 0.9726\n",
            "Batch [24855/28711], Loss: 0.9235\n",
            "Batch [24856/28711], Loss: 1.1060\n",
            "Batch [24857/28711], Loss: 0.9404\n",
            "Batch [24858/28711], Loss: 1.0334\n",
            "Batch [24859/28711], Loss: 1.0114\n",
            "Batch [24860/28711], Loss: 0.9617\n",
            "Batch [24861/28711], Loss: 0.9891\n",
            "Batch [24862/28711], Loss: 0.8908\n",
            "Batch [24863/28711], Loss: 0.8347\n",
            "Batch [24864/28711], Loss: 1.2284\n",
            "Batch [24865/28711], Loss: 0.8982\n",
            "Batch [24866/28711], Loss: 0.7400\n",
            "Batch [24867/28711], Loss: 0.9322\n",
            "Batch [24868/28711], Loss: 0.8888\n",
            "Batch [24869/28711], Loss: 0.7502\n",
            "Batch [24870/28711], Loss: 0.9051\n",
            "Batch [24871/28711], Loss: 1.1485\n",
            "Batch [24872/28711], Loss: 0.8464\n",
            "Batch [24873/28711], Loss: 0.9388\n",
            "Batch [24874/28711], Loss: 1.0376\n",
            "Batch [24875/28711], Loss: 1.0714\n",
            "Batch [24876/28711], Loss: 0.8792\n",
            "Batch [24877/28711], Loss: 0.8260\n",
            "Batch [24878/28711], Loss: 1.0899\n",
            "Batch [24879/28711], Loss: 0.9508\n",
            "Batch [24880/28711], Loss: 0.9377\n",
            "Batch [24881/28711], Loss: 0.8633\n",
            "Batch [24882/28711], Loss: 0.8060\n",
            "Batch [24883/28711], Loss: 0.7263\n",
            "Batch [24884/28711], Loss: 1.1225\n",
            "Batch [24885/28711], Loss: 0.7379\n",
            "Batch [24886/28711], Loss: 0.8866\n",
            "Batch [24887/28711], Loss: 0.7913\n",
            "Batch [24888/28711], Loss: 0.9544\n",
            "Batch [24889/28711], Loss: 0.7803\n",
            "Batch [24890/28711], Loss: 1.3665\n",
            "Batch [24891/28711], Loss: 0.8947\n",
            "Batch [24892/28711], Loss: 0.8417\n",
            "Batch [24893/28711], Loss: 0.6337\n",
            "Batch [24894/28711], Loss: 1.0966\n",
            "Batch [24895/28711], Loss: 0.9210\n",
            "Batch [24896/28711], Loss: 0.9460\n",
            "Batch [24897/28711], Loss: 1.0106\n",
            "Batch [24898/28711], Loss: 0.9040\n",
            "Batch [24899/28711], Loss: 1.1095\n",
            "Batch [24900/28711], Loss: 0.8533\n",
            "Batch [24901/28711], Loss: 1.0337\n",
            "Batch [24902/28711], Loss: 0.9930\n",
            "Batch [24903/28711], Loss: 1.0250\n",
            "Batch [24904/28711], Loss: 1.0933\n",
            "Batch [24905/28711], Loss: 0.9207\n",
            "Batch [24906/28711], Loss: 0.8876\n",
            "Batch [24907/28711], Loss: 0.7839\n",
            "Batch [24908/28711], Loss: 0.9179\n",
            "Batch [24909/28711], Loss: 1.1550\n",
            "Batch [24910/28711], Loss: 0.8236\n",
            "Batch [24911/28711], Loss: 1.1693\n",
            "Batch [24912/28711], Loss: 1.1749\n",
            "Batch [24913/28711], Loss: 0.8396\n",
            "Batch [24914/28711], Loss: 0.8801\n",
            "Batch [24915/28711], Loss: 0.9057\n",
            "Batch [24916/28711], Loss: 1.1494\n",
            "Batch [24917/28711], Loss: 0.9243\n",
            "Batch [24918/28711], Loss: 0.7438\n",
            "Batch [24919/28711], Loss: 0.8576\n",
            "Batch [24920/28711], Loss: 1.1951\n",
            "Batch [24921/28711], Loss: 0.9133\n",
            "Batch [24922/28711], Loss: 0.8334\n",
            "Batch [24923/28711], Loss: 0.8236\n",
            "Batch [24924/28711], Loss: 0.9472\n",
            "Batch [24925/28711], Loss: 1.1970\n",
            "Batch [24926/28711], Loss: 1.0672\n",
            "Batch [24927/28711], Loss: 0.8569\n",
            "Batch [24928/28711], Loss: 0.7786\n",
            "Batch [24929/28711], Loss: 0.7624\n",
            "Batch [24930/28711], Loss: 1.2288\n",
            "Batch [24931/28711], Loss: 0.8488\n",
            "Batch [24932/28711], Loss: 1.0016\n",
            "Batch [24933/28711], Loss: 0.6950\n",
            "Batch [24934/28711], Loss: 0.8176\n",
            "Batch [24935/28711], Loss: 0.9235\n",
            "Batch [24936/28711], Loss: 0.8161\n",
            "Batch [24937/28711], Loss: 0.7814\n",
            "Batch [24938/28711], Loss: 0.8579\n",
            "Batch [24939/28711], Loss: 0.8010\n",
            "Batch [24940/28711], Loss: 0.8282\n",
            "Batch [24941/28711], Loss: 0.7097\n",
            "Batch [24942/28711], Loss: 0.9715\n",
            "Batch [24943/28711], Loss: 1.2793\n",
            "Batch [24944/28711], Loss: 0.8785\n",
            "Batch [24945/28711], Loss: 1.1032\n",
            "Batch [24946/28711], Loss: 1.1522\n",
            "Batch [24947/28711], Loss: 0.8825\n",
            "Batch [24948/28711], Loss: 0.8826\n",
            "Batch [24949/28711], Loss: 0.8131\n",
            "Batch [24950/28711], Loss: 1.1489\n",
            "Batch [24951/28711], Loss: 0.8560\n",
            "Batch [24952/28711], Loss: 1.3848\n",
            "Batch [24953/28711], Loss: 1.1371\n",
            "Batch [24954/28711], Loss: 1.0406\n",
            "Batch [24955/28711], Loss: 0.7239\n",
            "Batch [24956/28711], Loss: 0.7874\n",
            "Batch [24957/28711], Loss: 0.9139\n",
            "Batch [24958/28711], Loss: 0.9319\n",
            "Batch [24959/28711], Loss: 1.2378\n",
            "Batch [24960/28711], Loss: 0.6994\n",
            "Batch [24961/28711], Loss: 1.0870\n",
            "Batch [24962/28711], Loss: 0.9146\n",
            "Batch [24963/28711], Loss: 0.9236\n",
            "Batch [24964/28711], Loss: 1.1177\n",
            "Batch [24965/28711], Loss: 0.7896\n",
            "Batch [24966/28711], Loss: 0.7176\n",
            "Batch [24967/28711], Loss: 0.7613\n",
            "Batch [24968/28711], Loss: 0.9049\n",
            "Batch [24969/28711], Loss: 0.8951\n",
            "Batch [24970/28711], Loss: 0.7995\n",
            "Batch [24971/28711], Loss: 0.9985\n",
            "Batch [24972/28711], Loss: 1.1110\n",
            "Batch [24973/28711], Loss: 0.8711\n",
            "Batch [24974/28711], Loss: 0.9583\n",
            "Batch [24975/28711], Loss: 0.9412\n",
            "Batch [24976/28711], Loss: 1.0717\n",
            "Batch [24977/28711], Loss: 0.8009\n",
            "Batch [24978/28711], Loss: 0.6645\n",
            "Batch [24979/28711], Loss: 0.8230\n",
            "Batch [24980/28711], Loss: 0.6836\n",
            "Batch [24981/28711], Loss: 1.0236\n",
            "Batch [24982/28711], Loss: 0.9047\n",
            "Batch [24983/28711], Loss: 0.7483\n",
            "Batch [24984/28711], Loss: 1.4944\n",
            "Batch [24985/28711], Loss: 0.7915\n",
            "Batch [24986/28711], Loss: 0.9345\n",
            "Batch [24987/28711], Loss: 1.1039\n",
            "Batch [24988/28711], Loss: 0.9988\n",
            "Batch [24989/28711], Loss: 0.7379\n",
            "Batch [24990/28711], Loss: 1.0611\n",
            "Batch [24991/28711], Loss: 0.9955\n",
            "Batch [24992/28711], Loss: 1.0811\n",
            "Batch [24993/28711], Loss: 0.8684\n",
            "Batch [24994/28711], Loss: 0.8721\n",
            "Batch [24995/28711], Loss: 0.8825\n",
            "Batch [24996/28711], Loss: 0.7989\n",
            "Batch [24997/28711], Loss: 0.8480\n",
            "Batch [24998/28711], Loss: 0.9590\n",
            "Batch [24999/28711], Loss: 0.9712\n",
            "Batch [25000/28711], Loss: 1.0892\n",
            "Batch [25001/28711], Loss: 0.9870\n",
            "Batch [25002/28711], Loss: 1.0991\n",
            "Batch [25003/28711], Loss: 0.8941\n",
            "Batch [25004/28711], Loss: 0.7948\n",
            "Batch [25005/28711], Loss: 0.8302\n",
            "Batch [25006/28711], Loss: 1.1969\n",
            "Batch [25007/28711], Loss: 0.8575\n",
            "Batch [25008/28711], Loss: 0.6224\n",
            "Batch [25009/28711], Loss: 1.0665\n",
            "Batch [25010/28711], Loss: 0.7769\n",
            "Batch [25011/28711], Loss: 0.9906\n",
            "Batch [25012/28711], Loss: 0.9089\n",
            "Batch [25013/28711], Loss: 0.9854\n",
            "Batch [25014/28711], Loss: 0.9440\n",
            "Batch [25015/28711], Loss: 0.9379\n",
            "Batch [25016/28711], Loss: 1.0152\n",
            "Batch [25017/28711], Loss: 0.9125\n",
            "Batch [25018/28711], Loss: 1.0126\n",
            "Batch [25019/28711], Loss: 0.9146\n",
            "Batch [25020/28711], Loss: 1.0493\n",
            "Batch [25021/28711], Loss: 0.9311\n",
            "Batch [25022/28711], Loss: 0.8570\n",
            "Batch [25023/28711], Loss: 1.1965\n",
            "Batch [25024/28711], Loss: 0.8912\n",
            "Batch [25025/28711], Loss: 0.8013\n",
            "Batch [25026/28711], Loss: 1.0244\n",
            "Batch [25027/28711], Loss: 0.8572\n",
            "Batch [25028/28711], Loss: 0.9350\n",
            "Batch [25029/28711], Loss: 0.6128\n",
            "Batch [25030/28711], Loss: 0.8969\n",
            "Batch [25031/28711], Loss: 1.2037\n",
            "Batch [25032/28711], Loss: 1.0964\n",
            "Batch [25033/28711], Loss: 0.8832\n",
            "Batch [25034/28711], Loss: 1.0622\n",
            "Batch [25035/28711], Loss: 0.8379\n",
            "Batch [25036/28711], Loss: 0.8606\n",
            "Batch [25037/28711], Loss: 1.2420\n",
            "Batch [25038/28711], Loss: 0.7986\n",
            "Batch [25039/28711], Loss: 0.9540\n",
            "Batch [25040/28711], Loss: 0.8975\n",
            "Batch [25041/28711], Loss: 0.9708\n",
            "Batch [25042/28711], Loss: 0.6696\n",
            "Batch [25043/28711], Loss: 1.0462\n",
            "Batch [25044/28711], Loss: 1.0692\n",
            "Batch [25045/28711], Loss: 0.8833\n",
            "Batch [25046/28711], Loss: 0.8425\n",
            "Batch [25047/28711], Loss: 0.8831\n",
            "Batch [25048/28711], Loss: 0.8141\n",
            "Batch [25049/28711], Loss: 0.7810\n",
            "Batch [25050/28711], Loss: 0.8981\n",
            "Batch [25051/28711], Loss: 0.6353\n",
            "Batch [25052/28711], Loss: 0.8388\n",
            "Batch [25053/28711], Loss: 1.0935\n",
            "Batch [25054/28711], Loss: 0.6194\n",
            "Batch [25055/28711], Loss: 0.8198\n",
            "Batch [25056/28711], Loss: 0.8733\n",
            "Batch [25057/28711], Loss: 1.0103\n",
            "Batch [25058/28711], Loss: 0.8328\n",
            "Batch [25059/28711], Loss: 1.1353\n",
            "Batch [25060/28711], Loss: 0.7052\n",
            "Batch [25061/28711], Loss: 0.9383\n",
            "Batch [25062/28711], Loss: 1.0300\n",
            "Batch [25063/28711], Loss: 0.9636\n",
            "Batch [25064/28711], Loss: 0.7841\n",
            "Batch [25065/28711], Loss: 1.0225\n",
            "Batch [25066/28711], Loss: 0.9912\n",
            "Batch [25067/28711], Loss: 0.8761\n",
            "Batch [25068/28711], Loss: 0.7621\n",
            "Batch [25069/28711], Loss: 0.9751\n",
            "Batch [25070/28711], Loss: 1.3555\n",
            "Batch [25071/28711], Loss: 0.6694\n",
            "Batch [25072/28711], Loss: 1.1780\n",
            "Batch [25073/28711], Loss: 1.0760\n",
            "Batch [25074/28711], Loss: 1.1212\n",
            "Batch [25075/28711], Loss: 0.9945\n",
            "Batch [25076/28711], Loss: 0.9129\n",
            "Batch [25077/28711], Loss: 0.9570\n",
            "Batch [25078/28711], Loss: 1.0185\n",
            "Batch [25079/28711], Loss: 0.9423\n",
            "Batch [25080/28711], Loss: 1.1602\n",
            "Batch [25081/28711], Loss: 1.0545\n",
            "Batch [25082/28711], Loss: 0.7252\n",
            "Batch [25083/28711], Loss: 0.9517\n",
            "Batch [25084/28711], Loss: 1.0607\n",
            "Batch [25085/28711], Loss: 0.7752\n",
            "Batch [25086/28711], Loss: 0.8275\n",
            "Batch [25087/28711], Loss: 1.1324\n",
            "Batch [25088/28711], Loss: 0.9113\n",
            "Batch [25089/28711], Loss: 0.8009\n",
            "Batch [25090/28711], Loss: 0.8953\n",
            "Batch [25091/28711], Loss: 0.7933\n",
            "Batch [25092/28711], Loss: 0.7802\n",
            "Batch [25093/28711], Loss: 1.1323\n",
            "Batch [25094/28711], Loss: 0.8705\n",
            "Batch [25095/28711], Loss: 1.0897\n",
            "Batch [25096/28711], Loss: 0.7737\n",
            "Batch [25097/28711], Loss: 0.9696\n",
            "Batch [25098/28711], Loss: 1.1013\n",
            "Batch [25099/28711], Loss: 1.0472\n",
            "Batch [25100/28711], Loss: 0.9573\n",
            "Batch [25101/28711], Loss: 0.8553\n",
            "Batch [25102/28711], Loss: 1.1853\n",
            "Batch [25103/28711], Loss: 1.1657\n",
            "Batch [25104/28711], Loss: 1.0370\n",
            "Batch [25105/28711], Loss: 1.2086\n",
            "Batch [25106/28711], Loss: 1.1690\n",
            "Batch [25107/28711], Loss: 1.0257\n",
            "Batch [25108/28711], Loss: 0.9258\n",
            "Batch [25109/28711], Loss: 0.8554\n",
            "Batch [25110/28711], Loss: 0.9681\n",
            "Batch [25111/28711], Loss: 1.0964\n",
            "Batch [25112/28711], Loss: 0.7998\n",
            "Batch [25113/28711], Loss: 0.7793\n",
            "Batch [25114/28711], Loss: 0.9859\n",
            "Batch [25115/28711], Loss: 1.3403\n",
            "Batch [25116/28711], Loss: 1.1140\n",
            "Batch [25117/28711], Loss: 1.0780\n",
            "Batch [25118/28711], Loss: 0.8786\n",
            "Batch [25119/28711], Loss: 1.0405\n",
            "Batch [25120/28711], Loss: 1.2753\n",
            "Batch [25121/28711], Loss: 0.8633\n",
            "Batch [25122/28711], Loss: 0.7237\n",
            "Batch [25123/28711], Loss: 1.1488\n",
            "Batch [25124/28711], Loss: 0.9232\n",
            "Batch [25125/28711], Loss: 1.1221\n",
            "Batch [25126/28711], Loss: 0.9052\n",
            "Batch [25127/28711], Loss: 0.8900\n",
            "Batch [25128/28711], Loss: 0.7284\n",
            "Batch [25129/28711], Loss: 1.0041\n",
            "Batch [25130/28711], Loss: 0.9686\n",
            "Batch [25131/28711], Loss: 0.9919\n",
            "Batch [25132/28711], Loss: 1.0617\n",
            "Batch [25133/28711], Loss: 0.7466\n",
            "Batch [25134/28711], Loss: 1.2048\n",
            "Batch [25135/28711], Loss: 1.1601\n",
            "Batch [25136/28711], Loss: 0.7491\n",
            "Batch [25137/28711], Loss: 1.1161\n",
            "Batch [25138/28711], Loss: 0.7293\n",
            "Batch [25139/28711], Loss: 0.9048\n",
            "Batch [25140/28711], Loss: 0.9316\n",
            "Batch [25141/28711], Loss: 0.9349\n",
            "Batch [25142/28711], Loss: 0.7739\n",
            "Batch [25143/28711], Loss: 0.8749\n",
            "Batch [25144/28711], Loss: 0.7932\n",
            "Batch [25145/28711], Loss: 0.9892\n",
            "Batch [25146/28711], Loss: 0.9471\n",
            "Batch [25147/28711], Loss: 0.9427\n",
            "Batch [25148/28711], Loss: 0.9531\n",
            "Batch [25149/28711], Loss: 1.0527\n",
            "Batch [25150/28711], Loss: 0.8358\n",
            "Batch [25151/28711], Loss: 1.1924\n",
            "Batch [25152/28711], Loss: 1.0653\n",
            "Batch [25153/28711], Loss: 0.7431\n",
            "Batch [25154/28711], Loss: 0.7619\n",
            "Batch [25155/28711], Loss: 1.0164\n",
            "Batch [25156/28711], Loss: 0.8013\n",
            "Batch [25157/28711], Loss: 0.9414\n",
            "Batch [25158/28711], Loss: 0.8049\n",
            "Batch [25159/28711], Loss: 0.7503\n",
            "Batch [25160/28711], Loss: 0.7043\n",
            "Batch [25161/28711], Loss: 0.9623\n",
            "Batch [25162/28711], Loss: 0.8776\n",
            "Batch [25163/28711], Loss: 0.6889\n",
            "Batch [25164/28711], Loss: 0.9081\n",
            "Batch [25165/28711], Loss: 0.8552\n",
            "Batch [25166/28711], Loss: 0.9712\n",
            "Batch [25167/28711], Loss: 1.1771\n",
            "Batch [25168/28711], Loss: 1.0906\n",
            "Batch [25169/28711], Loss: 1.0018\n",
            "Batch [25170/28711], Loss: 1.0867\n",
            "Batch [25171/28711], Loss: 0.9176\n",
            "Batch [25172/28711], Loss: 1.0363\n",
            "Batch [25173/28711], Loss: 0.8173\n",
            "Batch [25174/28711], Loss: 1.0185\n",
            "Batch [25175/28711], Loss: 0.7940\n",
            "Batch [25176/28711], Loss: 1.0121\n",
            "Batch [25177/28711], Loss: 1.2564\n",
            "Batch [25178/28711], Loss: 0.9252\n",
            "Batch [25179/28711], Loss: 1.0520\n",
            "Batch [25180/28711], Loss: 0.6385\n",
            "Batch [25181/28711], Loss: 0.8263\n",
            "Batch [25182/28711], Loss: 0.9577\n",
            "Batch [25183/28711], Loss: 0.8923\n",
            "Batch [25184/28711], Loss: 1.0073\n",
            "Batch [25185/28711], Loss: 1.1431\n",
            "Batch [25186/28711], Loss: 0.9754\n",
            "Batch [25187/28711], Loss: 0.7266\n",
            "Batch [25188/28711], Loss: 0.8346\n",
            "Batch [25189/28711], Loss: 0.7299\n",
            "Batch [25190/28711], Loss: 0.8061\n",
            "Batch [25191/28711], Loss: 0.9816\n",
            "Batch [25192/28711], Loss: 0.8027\n",
            "Batch [25193/28711], Loss: 0.6520\n",
            "Batch [25194/28711], Loss: 1.1714\n",
            "Batch [25195/28711], Loss: 0.8611\n",
            "Batch [25196/28711], Loss: 1.0355\n",
            "Batch [25197/28711], Loss: 1.0663\n",
            "Batch [25198/28711], Loss: 0.9858\n",
            "Batch [25199/28711], Loss: 1.1240\n",
            "Batch [25200/28711], Loss: 0.7613\n",
            "Batch [25201/28711], Loss: 0.9103\n",
            "Batch [25202/28711], Loss: 0.8530\n",
            "Batch [25203/28711], Loss: 0.6945\n",
            "Batch [25204/28711], Loss: 0.9395\n",
            "Batch [25205/28711], Loss: 0.9662\n",
            "Batch [25206/28711], Loss: 1.0314\n",
            "Batch [25207/28711], Loss: 0.8390\n",
            "Batch [25208/28711], Loss: 1.3162\n",
            "Batch [25209/28711], Loss: 0.8709\n",
            "Batch [25210/28711], Loss: 0.8797\n",
            "Batch [25211/28711], Loss: 0.9014\n",
            "Batch [25212/28711], Loss: 1.0002\n",
            "Batch [25213/28711], Loss: 0.7188\n",
            "Batch [25214/28711], Loss: 0.8930\n",
            "Batch [25215/28711], Loss: 1.1701\n",
            "Batch [25216/28711], Loss: 0.9480\n",
            "Batch [25217/28711], Loss: 0.6490\n",
            "Batch [25218/28711], Loss: 0.8161\n",
            "Batch [25219/28711], Loss: 1.1834\n",
            "Batch [25220/28711], Loss: 0.8682\n",
            "Batch [25221/28711], Loss: 0.7700\n",
            "Batch [25222/28711], Loss: 0.7834\n",
            "Batch [25223/28711], Loss: 0.9943\n",
            "Batch [25224/28711], Loss: 1.1975\n",
            "Batch [25225/28711], Loss: 1.3143\n",
            "Batch [25226/28711], Loss: 0.9633\n",
            "Batch [25227/28711], Loss: 1.1888\n",
            "Batch [25228/28711], Loss: 1.0888\n",
            "Batch [25229/28711], Loss: 0.9838\n",
            "Batch [25230/28711], Loss: 0.9035\n",
            "Batch [25231/28711], Loss: 1.0126\n",
            "Batch [25232/28711], Loss: 0.7063\n",
            "Batch [25233/28711], Loss: 1.1356\n",
            "Batch [25234/28711], Loss: 1.2449\n",
            "Batch [25235/28711], Loss: 1.0634\n",
            "Batch [25236/28711], Loss: 0.7424\n",
            "Batch [25237/28711], Loss: 1.0657\n",
            "Batch [25238/28711], Loss: 0.7947\n",
            "Batch [25239/28711], Loss: 1.1293\n",
            "Batch [25240/28711], Loss: 1.0495\n",
            "Batch [25241/28711], Loss: 1.4965\n",
            "Batch [25242/28711], Loss: 1.0603\n",
            "Batch [25243/28711], Loss: 1.1024\n",
            "Batch [25244/28711], Loss: 0.8587\n",
            "Batch [25245/28711], Loss: 0.8876\n",
            "Batch [25246/28711], Loss: 0.7467\n",
            "Batch [25247/28711], Loss: 0.9812\n",
            "Batch [25248/28711], Loss: 0.6715\n",
            "Batch [25249/28711], Loss: 0.7267\n",
            "Batch [25250/28711], Loss: 1.0491\n",
            "Batch [25251/28711], Loss: 1.0477\n",
            "Batch [25252/28711], Loss: 0.7088\n",
            "Batch [25253/28711], Loss: 1.1961\n",
            "Batch [25254/28711], Loss: 1.0923\n",
            "Batch [25255/28711], Loss: 0.9530\n",
            "Batch [25256/28711], Loss: 0.8527\n",
            "Batch [25257/28711], Loss: 0.7381\n",
            "Batch [25258/28711], Loss: 1.2085\n",
            "Batch [25259/28711], Loss: 0.7868\n",
            "Batch [25260/28711], Loss: 1.0370\n",
            "Batch [25261/28711], Loss: 0.7068\n",
            "Batch [25262/28711], Loss: 0.9328\n",
            "Batch [25263/28711], Loss: 1.1985\n",
            "Batch [25264/28711], Loss: 1.0997\n",
            "Batch [25265/28711], Loss: 0.7433\n",
            "Batch [25266/28711], Loss: 0.6291\n",
            "Batch [25267/28711], Loss: 1.2272\n",
            "Batch [25268/28711], Loss: 0.7519\n",
            "Batch [25269/28711], Loss: 1.1706\n",
            "Batch [25270/28711], Loss: 0.5877\n",
            "Batch [25271/28711], Loss: 0.9577\n",
            "Batch [25272/28711], Loss: 0.7685\n",
            "Batch [25273/28711], Loss: 0.9223\n",
            "Batch [25274/28711], Loss: 0.8469\n",
            "Batch [25275/28711], Loss: 1.1114\n",
            "Batch [25276/28711], Loss: 0.9829\n",
            "Batch [25277/28711], Loss: 0.7988\n",
            "Batch [25278/28711], Loss: 0.8804\n",
            "Batch [25279/28711], Loss: 1.0495\n",
            "Batch [25280/28711], Loss: 0.7687\n",
            "Batch [25281/28711], Loss: 0.8146\n",
            "Batch [25282/28711], Loss: 1.0741\n",
            "Batch [25283/28711], Loss: 0.9314\n",
            "Batch [25284/28711], Loss: 1.1151\n",
            "Batch [25285/28711], Loss: 0.5983\n",
            "Batch [25286/28711], Loss: 1.0344\n",
            "Batch [25287/28711], Loss: 0.8056\n",
            "Batch [25288/28711], Loss: 0.7696\n",
            "Batch [25289/28711], Loss: 0.6764\n",
            "Batch [25290/28711], Loss: 0.8514\n",
            "Batch [25291/28711], Loss: 0.9611\n",
            "Batch [25292/28711], Loss: 0.6932\n",
            "Batch [25293/28711], Loss: 1.0294\n",
            "Batch [25294/28711], Loss: 0.8533\n",
            "Batch [25295/28711], Loss: 1.0898\n",
            "Batch [25296/28711], Loss: 0.9264\n",
            "Batch [25297/28711], Loss: 0.9626\n",
            "Batch [25298/28711], Loss: 1.1546\n",
            "Batch [25299/28711], Loss: 0.8031\n",
            "Batch [25300/28711], Loss: 0.8370\n",
            "Batch [25301/28711], Loss: 0.8239\n",
            "Batch [25302/28711], Loss: 0.9909\n",
            "Batch [25303/28711], Loss: 0.7269\n",
            "Batch [25304/28711], Loss: 0.8622\n",
            "Batch [25305/28711], Loss: 1.1504\n",
            "Batch [25306/28711], Loss: 0.8482\n",
            "Batch [25307/28711], Loss: 0.7676\n",
            "Batch [25308/28711], Loss: 0.8833\n",
            "Batch [25309/28711], Loss: 0.7712\n",
            "Batch [25310/28711], Loss: 1.0491\n",
            "Batch [25311/28711], Loss: 1.1550\n",
            "Batch [25312/28711], Loss: 1.2128\n",
            "Batch [25313/28711], Loss: 0.9035\n",
            "Batch [25314/28711], Loss: 0.8661\n",
            "Batch [25315/28711], Loss: 0.9187\n",
            "Batch [25316/28711], Loss: 0.9675\n",
            "Batch [25317/28711], Loss: 0.8823\n",
            "Batch [25318/28711], Loss: 0.6713\n",
            "Batch [25319/28711], Loss: 0.7801\n",
            "Batch [25320/28711], Loss: 1.0069\n",
            "Batch [25321/28711], Loss: 0.9490\n",
            "Batch [25322/28711], Loss: 0.9398\n",
            "Batch [25323/28711], Loss: 0.8211\n",
            "Batch [25324/28711], Loss: 0.7742\n",
            "Batch [25325/28711], Loss: 1.2427\n",
            "Batch [25326/28711], Loss: 0.8601\n",
            "Batch [25327/28711], Loss: 0.9399\n",
            "Batch [25328/28711], Loss: 0.9001\n",
            "Batch [25329/28711], Loss: 0.9187\n",
            "Batch [25330/28711], Loss: 0.9013\n",
            "Batch [25331/28711], Loss: 0.7469\n",
            "Batch [25332/28711], Loss: 0.9169\n",
            "Batch [25333/28711], Loss: 1.1579\n",
            "Batch [25334/28711], Loss: 0.6052\n",
            "Batch [25335/28711], Loss: 0.8542\n",
            "Batch [25336/28711], Loss: 0.8658\n",
            "Batch [25337/28711], Loss: 0.8193\n",
            "Batch [25338/28711], Loss: 0.7942\n",
            "Batch [25339/28711], Loss: 0.8435\n",
            "Batch [25340/28711], Loss: 0.8839\n",
            "Batch [25341/28711], Loss: 0.6140\n",
            "Batch [25342/28711], Loss: 0.9941\n",
            "Batch [25343/28711], Loss: 1.1127\n",
            "Batch [25344/28711], Loss: 0.7984\n",
            "Batch [25345/28711], Loss: 1.0191\n",
            "Batch [25346/28711], Loss: 0.8307\n",
            "Batch [25347/28711], Loss: 1.0547\n",
            "Batch [25348/28711], Loss: 1.1100\n",
            "Batch [25349/28711], Loss: 1.1294\n",
            "Batch [25350/28711], Loss: 1.0516\n",
            "Batch [25351/28711], Loss: 0.8691\n",
            "Batch [25352/28711], Loss: 1.0609\n",
            "Batch [25353/28711], Loss: 1.0225\n",
            "Batch [25354/28711], Loss: 1.1136\n",
            "Batch [25355/28711], Loss: 0.7623\n",
            "Batch [25356/28711], Loss: 0.9053\n",
            "Batch [25357/28711], Loss: 0.8611\n",
            "Batch [25358/28711], Loss: 0.7300\n",
            "Batch [25359/28711], Loss: 0.8346\n",
            "Batch [25360/28711], Loss: 0.8693\n",
            "Batch [25361/28711], Loss: 0.9402\n",
            "Batch [25362/28711], Loss: 0.5695\n",
            "Batch [25363/28711], Loss: 1.1830\n",
            "Batch [25364/28711], Loss: 0.8625\n",
            "Batch [25365/28711], Loss: 0.6657\n",
            "Batch [25366/28711], Loss: 1.0583\n",
            "Batch [25367/28711], Loss: 0.9926\n",
            "Batch [25368/28711], Loss: 0.9171\n",
            "Batch [25369/28711], Loss: 0.9536\n",
            "Batch [25370/28711], Loss: 1.0176\n",
            "Batch [25371/28711], Loss: 1.1577\n",
            "Batch [25372/28711], Loss: 0.7020\n",
            "Batch [25373/28711], Loss: 0.8911\n",
            "Batch [25374/28711], Loss: 0.7904\n",
            "Batch [25375/28711], Loss: 0.8148\n",
            "Batch [25376/28711], Loss: 1.0735\n",
            "Batch [25377/28711], Loss: 0.9023\n",
            "Batch [25378/28711], Loss: 0.9190\n",
            "Batch [25379/28711], Loss: 1.1218\n",
            "Batch [25380/28711], Loss: 0.8486\n",
            "Batch [25381/28711], Loss: 0.8403\n",
            "Batch [25382/28711], Loss: 0.5461\n",
            "Batch [25383/28711], Loss: 1.3328\n",
            "Batch [25384/28711], Loss: 1.0228\n",
            "Batch [25385/28711], Loss: 0.9679\n",
            "Batch [25386/28711], Loss: 0.6237\n",
            "Batch [25387/28711], Loss: 1.0115\n",
            "Batch [25388/28711], Loss: 0.7518\n",
            "Batch [25389/28711], Loss: 0.9101\n",
            "Batch [25390/28711], Loss: 0.8861\n",
            "Batch [25391/28711], Loss: 0.9937\n",
            "Batch [25392/28711], Loss: 0.9516\n",
            "Batch [25393/28711], Loss: 0.8924\n",
            "Batch [25394/28711], Loss: 0.8831\n",
            "Batch [25395/28711], Loss: 0.7637\n",
            "Batch [25396/28711], Loss: 0.7176\n",
            "Batch [25397/28711], Loss: 1.2186\n",
            "Batch [25398/28711], Loss: 0.6224\n",
            "Batch [25399/28711], Loss: 0.7151\n",
            "Batch [25400/28711], Loss: 0.9811\n",
            "Batch [25401/28711], Loss: 0.7065\n",
            "Batch [25402/28711], Loss: 0.9642\n",
            "Batch [25403/28711], Loss: 0.6379\n",
            "Batch [25404/28711], Loss: 0.7398\n",
            "Batch [25405/28711], Loss: 0.9306\n",
            "Batch [25406/28711], Loss: 0.9116\n",
            "Batch [25407/28711], Loss: 1.0021\n",
            "Batch [25408/28711], Loss: 0.8351\n",
            "Batch [25409/28711], Loss: 0.8847\n",
            "Batch [25410/28711], Loss: 0.8567\n",
            "Batch [25411/28711], Loss: 0.7776\n",
            "Batch [25412/28711], Loss: 0.8296\n",
            "Batch [25413/28711], Loss: 0.8274\n",
            "Batch [25414/28711], Loss: 1.1627\n",
            "Batch [25415/28711], Loss: 0.9814\n",
            "Batch [25416/28711], Loss: 1.0094\n",
            "Batch [25417/28711], Loss: 1.0452\n",
            "Batch [25418/28711], Loss: 0.7350\n",
            "Batch [25419/28711], Loss: 0.8738\n",
            "Batch [25420/28711], Loss: 0.9199\n",
            "Batch [25421/28711], Loss: 1.0336\n",
            "Batch [25422/28711], Loss: 1.1270\n",
            "Batch [25423/28711], Loss: 0.8783\n",
            "Batch [25424/28711], Loss: 0.7934\n",
            "Batch [25425/28711], Loss: 0.8938\n",
            "Batch [25426/28711], Loss: 0.9299\n",
            "Batch [25427/28711], Loss: 0.9349\n",
            "Batch [25428/28711], Loss: 0.8879\n",
            "Batch [25429/28711], Loss: 0.7883\n",
            "Batch [25430/28711], Loss: 1.0395\n",
            "Batch [25431/28711], Loss: 0.8762\n",
            "Batch [25432/28711], Loss: 1.0221\n",
            "Batch [25433/28711], Loss: 0.7742\n",
            "Batch [25434/28711], Loss: 1.1402\n",
            "Batch [25435/28711], Loss: 0.9784\n",
            "Batch [25436/28711], Loss: 0.7446\n",
            "Batch [25437/28711], Loss: 1.1603\n",
            "Batch [25438/28711], Loss: 0.8741\n",
            "Batch [25439/28711], Loss: 1.0955\n",
            "Batch [25440/28711], Loss: 1.0459\n",
            "Batch [25441/28711], Loss: 0.7919\n",
            "Batch [25442/28711], Loss: 0.8288\n",
            "Batch [25443/28711], Loss: 1.0320\n",
            "Batch [25444/28711], Loss: 0.8895\n",
            "Batch [25445/28711], Loss: 0.9944\n",
            "Batch [25446/28711], Loss: 0.8433\n",
            "Batch [25447/28711], Loss: 0.7768\n",
            "Batch [25448/28711], Loss: 0.9151\n",
            "Batch [25449/28711], Loss: 0.7765\n",
            "Batch [25450/28711], Loss: 0.9660\n",
            "Batch [25451/28711], Loss: 0.8315\n",
            "Batch [25452/28711], Loss: 0.7851\n",
            "Batch [25453/28711], Loss: 0.8830\n",
            "Batch [25454/28711], Loss: 1.0363\n",
            "Batch [25455/28711], Loss: 1.0577\n",
            "Batch [25456/28711], Loss: 1.0887\n",
            "Batch [25457/28711], Loss: 1.2341\n",
            "Batch [25458/28711], Loss: 1.0574\n",
            "Batch [25459/28711], Loss: 0.7753\n",
            "Batch [25460/28711], Loss: 0.9529\n",
            "Batch [25461/28711], Loss: 0.7627\n",
            "Batch [25462/28711], Loss: 0.9216\n",
            "Batch [25463/28711], Loss: 0.6490\n",
            "Batch [25464/28711], Loss: 0.8715\n",
            "Batch [25465/28711], Loss: 0.9710\n",
            "Batch [25466/28711], Loss: 0.9952\n",
            "Batch [25467/28711], Loss: 0.8563\n",
            "Batch [25468/28711], Loss: 0.9643\n",
            "Batch [25469/28711], Loss: 1.0581\n",
            "Batch [25470/28711], Loss: 1.0061\n",
            "Batch [25471/28711], Loss: 0.9374\n",
            "Batch [25472/28711], Loss: 0.8045\n",
            "Batch [25473/28711], Loss: 0.8084\n",
            "Batch [25474/28711], Loss: 0.9407\n",
            "Batch [25475/28711], Loss: 0.8453\n",
            "Batch [25476/28711], Loss: 0.7193\n",
            "Batch [25477/28711], Loss: 1.2394\n",
            "Batch [25478/28711], Loss: 0.9722\n",
            "Batch [25479/28711], Loss: 1.2432\n",
            "Batch [25480/28711], Loss: 1.0617\n",
            "Batch [25481/28711], Loss: 0.9704\n",
            "Batch [25482/28711], Loss: 0.8971\n",
            "Batch [25483/28711], Loss: 1.1782\n",
            "Batch [25484/28711], Loss: 1.1369\n",
            "Batch [25485/28711], Loss: 0.8814\n",
            "Batch [25486/28711], Loss: 1.0738\n",
            "Batch [25487/28711], Loss: 0.8753\n",
            "Batch [25488/28711], Loss: 1.0373\n",
            "Batch [25489/28711], Loss: 0.8124\n",
            "Batch [25490/28711], Loss: 1.1514\n",
            "Batch [25491/28711], Loss: 0.9544\n",
            "Batch [25492/28711], Loss: 1.0714\n",
            "Batch [25493/28711], Loss: 0.9665\n",
            "Batch [25494/28711], Loss: 0.9398\n",
            "Batch [25495/28711], Loss: 0.8584\n",
            "Batch [25496/28711], Loss: 0.8972\n",
            "Batch [25497/28711], Loss: 0.7134\n",
            "Batch [25498/28711], Loss: 0.7637\n",
            "Batch [25499/28711], Loss: 0.8413\n",
            "Batch [25500/28711], Loss: 0.6912\n",
            "Batch [25501/28711], Loss: 0.6200\n",
            "Batch [25502/28711], Loss: 1.0548\n",
            "Batch [25503/28711], Loss: 0.8219\n",
            "Batch [25504/28711], Loss: 0.6754\n",
            "Batch [25505/28711], Loss: 1.0171\n",
            "Batch [25506/28711], Loss: 0.6060\n",
            "Batch [25507/28711], Loss: 0.9338\n",
            "Batch [25508/28711], Loss: 0.9190\n",
            "Batch [25509/28711], Loss: 1.0598\n",
            "Batch [25510/28711], Loss: 0.9554\n",
            "Batch [25511/28711], Loss: 1.0076\n",
            "Batch [25512/28711], Loss: 0.7977\n",
            "Batch [25513/28711], Loss: 0.9781\n",
            "Batch [25514/28711], Loss: 0.8253\n",
            "Batch [25515/28711], Loss: 0.7087\n",
            "Batch [25516/28711], Loss: 0.9743\n",
            "Batch [25517/28711], Loss: 0.9413\n",
            "Batch [25518/28711], Loss: 0.8533\n",
            "Batch [25519/28711], Loss: 0.7424\n",
            "Batch [25520/28711], Loss: 0.6520\n",
            "Batch [25521/28711], Loss: 0.8536\n",
            "Batch [25522/28711], Loss: 1.1240\n",
            "Batch [25523/28711], Loss: 0.7654\n",
            "Batch [25524/28711], Loss: 1.1135\n",
            "Batch [25525/28711], Loss: 0.9081\n",
            "Batch [25526/28711], Loss: 1.2213\n",
            "Batch [25527/28711], Loss: 0.8866\n",
            "Batch [25528/28711], Loss: 0.9161\n",
            "Batch [25529/28711], Loss: 1.1139\n",
            "Batch [25530/28711], Loss: 0.7477\n",
            "Batch [25531/28711], Loss: 0.8454\n",
            "Batch [25532/28711], Loss: 0.9871\n",
            "Batch [25533/28711], Loss: 1.2633\n",
            "Batch [25534/28711], Loss: 1.0427\n",
            "Batch [25535/28711], Loss: 1.0192\n",
            "Batch [25536/28711], Loss: 1.0055\n",
            "Batch [25537/28711], Loss: 1.0257\n",
            "Batch [25538/28711], Loss: 0.9626\n",
            "Batch [25539/28711], Loss: 0.8779\n",
            "Batch [25540/28711], Loss: 0.9426\n",
            "Batch [25541/28711], Loss: 1.0058\n",
            "Batch [25542/28711], Loss: 0.7240\n",
            "Batch [25543/28711], Loss: 0.8329\n",
            "Batch [25544/28711], Loss: 0.7598\n",
            "Batch [25545/28711], Loss: 0.8704\n",
            "Batch [25546/28711], Loss: 1.0441\n",
            "Batch [25547/28711], Loss: 0.5962\n",
            "Batch [25548/28711], Loss: 0.9564\n",
            "Batch [25549/28711], Loss: 1.1150\n",
            "Batch [25550/28711], Loss: 1.0827\n",
            "Batch [25551/28711], Loss: 1.0073\n",
            "Batch [25552/28711], Loss: 0.8761\n",
            "Batch [25553/28711], Loss: 0.6342\n",
            "Batch [25554/28711], Loss: 0.8252\n",
            "Batch [25555/28711], Loss: 0.9134\n",
            "Batch [25556/28711], Loss: 1.0686\n",
            "Batch [25557/28711], Loss: 0.8594\n",
            "Batch [25558/28711], Loss: 0.8690\n",
            "Batch [25559/28711], Loss: 0.7935\n",
            "Batch [25560/28711], Loss: 1.0352\n",
            "Batch [25561/28711], Loss: 0.9248\n",
            "Batch [25562/28711], Loss: 0.7834\n",
            "Batch [25563/28711], Loss: 0.8666\n",
            "Batch [25564/28711], Loss: 0.8117\n",
            "Batch [25565/28711], Loss: 0.8970\n",
            "Batch [25566/28711], Loss: 0.8360\n",
            "Batch [25567/28711], Loss: 1.1964\n",
            "Batch [25568/28711], Loss: 1.0440\n",
            "Batch [25569/28711], Loss: 1.0124\n",
            "Batch [25570/28711], Loss: 0.7740\n",
            "Batch [25571/28711], Loss: 1.1513\n",
            "Batch [25572/28711], Loss: 0.5599\n",
            "Batch [25573/28711], Loss: 0.6713\n",
            "Batch [25574/28711], Loss: 1.1089\n",
            "Batch [25575/28711], Loss: 0.9720\n",
            "Batch [25576/28711], Loss: 0.8311\n",
            "Batch [25577/28711], Loss: 1.1767\n",
            "Batch [25578/28711], Loss: 1.0683\n",
            "Batch [25579/28711], Loss: 1.1235\n",
            "Batch [25580/28711], Loss: 0.8264\n",
            "Batch [25581/28711], Loss: 0.8269\n",
            "Batch [25582/28711], Loss: 0.8363\n",
            "Batch [25583/28711], Loss: 0.9587\n",
            "Batch [25584/28711], Loss: 1.0453\n",
            "Batch [25585/28711], Loss: 0.7418\n",
            "Batch [25586/28711], Loss: 0.9409\n",
            "Batch [25587/28711], Loss: 1.3111\n",
            "Batch [25588/28711], Loss: 1.0143\n",
            "Batch [25589/28711], Loss: 1.0955\n",
            "Batch [25590/28711], Loss: 0.8030\n",
            "Batch [25591/28711], Loss: 0.7917\n",
            "Batch [25592/28711], Loss: 0.8901\n",
            "Batch [25593/28711], Loss: 0.7805\n",
            "Batch [25594/28711], Loss: 0.8004\n",
            "Batch [25595/28711], Loss: 0.9080\n",
            "Batch [25596/28711], Loss: 0.7009\n",
            "Batch [25597/28711], Loss: 0.8526\n",
            "Batch [25598/28711], Loss: 1.0056\n",
            "Batch [25599/28711], Loss: 1.2207\n",
            "Batch [25600/28711], Loss: 0.9122\n",
            "Batch [25601/28711], Loss: 0.9447\n",
            "Batch [25602/28711], Loss: 0.7185\n",
            "Batch [25603/28711], Loss: 0.9444\n",
            "Batch [25604/28711], Loss: 0.7210\n",
            "Batch [25605/28711], Loss: 1.0185\n",
            "Batch [25606/28711], Loss: 1.1915\n",
            "Batch [25607/28711], Loss: 0.5758\n",
            "Batch [25608/28711], Loss: 0.9260\n",
            "Batch [25609/28711], Loss: 0.9562\n",
            "Batch [25610/28711], Loss: 0.6681\n",
            "Batch [25611/28711], Loss: 1.1957\n",
            "Batch [25612/28711], Loss: 0.8616\n",
            "Batch [25613/28711], Loss: 0.8918\n",
            "Batch [25614/28711], Loss: 1.0855\n",
            "Batch [25615/28711], Loss: 1.0869\n",
            "Batch [25616/28711], Loss: 0.6724\n",
            "Batch [25617/28711], Loss: 0.7376\n",
            "Batch [25618/28711], Loss: 0.7802\n",
            "Batch [25619/28711], Loss: 0.9185\n",
            "Batch [25620/28711], Loss: 0.8151\n",
            "Batch [25621/28711], Loss: 1.0454\n",
            "Batch [25622/28711], Loss: 0.8719\n",
            "Batch [25623/28711], Loss: 1.0462\n",
            "Batch [25624/28711], Loss: 0.9650\n",
            "Batch [25625/28711], Loss: 0.9320\n",
            "Batch [25626/28711], Loss: 1.0104\n",
            "Batch [25627/28711], Loss: 0.6685\n",
            "Batch [25628/28711], Loss: 1.1318\n",
            "Batch [25629/28711], Loss: 0.8159\n",
            "Batch [25630/28711], Loss: 0.8750\n",
            "Batch [25631/28711], Loss: 1.0732\n",
            "Batch [25632/28711], Loss: 0.6296\n",
            "Batch [25633/28711], Loss: 1.1739\n",
            "Batch [25634/28711], Loss: 0.7933\n",
            "Batch [25635/28711], Loss: 0.8637\n",
            "Batch [25636/28711], Loss: 0.9752\n",
            "Batch [25637/28711], Loss: 0.9025\n",
            "Batch [25638/28711], Loss: 1.0659\n",
            "Batch [25639/28711], Loss: 0.9818\n",
            "Batch [25640/28711], Loss: 1.1662\n",
            "Batch [25641/28711], Loss: 1.0367\n",
            "Batch [25642/28711], Loss: 0.7848\n",
            "Batch [25643/28711], Loss: 0.9228\n",
            "Batch [25644/28711], Loss: 0.7960\n",
            "Batch [25645/28711], Loss: 1.1592\n",
            "Batch [25646/28711], Loss: 0.8031\n",
            "Batch [25647/28711], Loss: 0.7609\n",
            "Batch [25648/28711], Loss: 1.1643\n",
            "Batch [25649/28711], Loss: 0.9423\n",
            "Batch [25650/28711], Loss: 1.0827\n",
            "Batch [25651/28711], Loss: 0.9553\n",
            "Batch [25652/28711], Loss: 0.6371\n",
            "Batch [25653/28711], Loss: 1.0782\n",
            "Batch [25654/28711], Loss: 0.8064\n",
            "Batch [25655/28711], Loss: 0.6623\n",
            "Batch [25656/28711], Loss: 0.6837\n",
            "Batch [25657/28711], Loss: 0.8190\n",
            "Batch [25658/28711], Loss: 1.0774\n",
            "Batch [25659/28711], Loss: 0.7983\n",
            "Batch [25660/28711], Loss: 0.7052\n",
            "Batch [25661/28711], Loss: 0.8078\n",
            "Batch [25662/28711], Loss: 1.2551\n",
            "Batch [25663/28711], Loss: 0.9069\n",
            "Batch [25664/28711], Loss: 0.8221\n",
            "Batch [25665/28711], Loss: 0.8724\n",
            "Batch [25666/28711], Loss: 0.9857\n",
            "Batch [25667/28711], Loss: 0.9843\n",
            "Batch [25668/28711], Loss: 0.8794\n",
            "Batch [25669/28711], Loss: 0.9063\n",
            "Batch [25670/28711], Loss: 0.6863\n",
            "Batch [25671/28711], Loss: 0.4774\n",
            "Batch [25672/28711], Loss: 0.7028\n",
            "Batch [25673/28711], Loss: 0.8457\n",
            "Batch [25674/28711], Loss: 0.8142\n",
            "Batch [25675/28711], Loss: 0.9771\n",
            "Batch [25676/28711], Loss: 0.6675\n",
            "Batch [25677/28711], Loss: 0.9323\n",
            "Batch [25678/28711], Loss: 1.0824\n",
            "Batch [25679/28711], Loss: 1.0376\n",
            "Batch [25680/28711], Loss: 1.0007\n",
            "Batch [25681/28711], Loss: 1.0986\n",
            "Batch [25682/28711], Loss: 0.9047\n",
            "Batch [25683/28711], Loss: 0.6738\n",
            "Batch [25684/28711], Loss: 0.7041\n",
            "Batch [25685/28711], Loss: 1.2138\n",
            "Batch [25686/28711], Loss: 1.2319\n",
            "Batch [25687/28711], Loss: 0.8785\n",
            "Batch [25688/28711], Loss: 0.8050\n",
            "Batch [25689/28711], Loss: 0.9777\n",
            "Batch [25690/28711], Loss: 0.5933\n",
            "Batch [25691/28711], Loss: 0.9607\n",
            "Batch [25692/28711], Loss: 1.0153\n",
            "Batch [25693/28711], Loss: 1.0514\n",
            "Batch [25694/28711], Loss: 0.9842\n",
            "Batch [25695/28711], Loss: 0.9625\n",
            "Batch [25696/28711], Loss: 0.9216\n",
            "Batch [25697/28711], Loss: 1.0308\n",
            "Batch [25698/28711], Loss: 0.8334\n",
            "Batch [25699/28711], Loss: 0.8460\n",
            "Batch [25700/28711], Loss: 1.0811\n",
            "Batch [25701/28711], Loss: 0.7628\n",
            "Batch [25702/28711], Loss: 0.8892\n",
            "Batch [25703/28711], Loss: 1.0360\n",
            "Batch [25704/28711], Loss: 0.9052\n",
            "Batch [25705/28711], Loss: 1.1736\n",
            "Batch [25706/28711], Loss: 0.7909\n",
            "Batch [25707/28711], Loss: 0.9193\n",
            "Batch [25708/28711], Loss: 0.7669\n",
            "Batch [25709/28711], Loss: 1.2095\n",
            "Batch [25710/28711], Loss: 0.5740\n",
            "Batch [25711/28711], Loss: 0.9016\n",
            "Batch [25712/28711], Loss: 1.1372\n",
            "Batch [25713/28711], Loss: 0.8741\n",
            "Batch [25714/28711], Loss: 1.0362\n",
            "Batch [25715/28711], Loss: 0.9141\n",
            "Batch [25716/28711], Loss: 0.7716\n",
            "Batch [25717/28711], Loss: 1.0638\n",
            "Batch [25718/28711], Loss: 0.9262\n",
            "Batch [25719/28711], Loss: 0.8746\n",
            "Batch [25720/28711], Loss: 0.6516\n",
            "Batch [25721/28711], Loss: 0.6750\n",
            "Batch [25722/28711], Loss: 0.8099\n",
            "Batch [25723/28711], Loss: 0.7706\n",
            "Batch [25724/28711], Loss: 0.8989\n",
            "Batch [25725/28711], Loss: 1.3585\n",
            "Batch [25726/28711], Loss: 0.8550\n",
            "Batch [25727/28711], Loss: 0.7928\n",
            "Batch [25728/28711], Loss: 1.0071\n",
            "Batch [25729/28711], Loss: 1.0275\n",
            "Batch [25730/28711], Loss: 1.0165\n",
            "Batch [25731/28711], Loss: 0.8511\n",
            "Batch [25732/28711], Loss: 0.6963\n",
            "Batch [25733/28711], Loss: 0.7706\n",
            "Batch [25734/28711], Loss: 0.8085\n",
            "Batch [25735/28711], Loss: 0.9250\n",
            "Batch [25736/28711], Loss: 0.6803\n",
            "Batch [25737/28711], Loss: 0.9319\n",
            "Batch [25738/28711], Loss: 0.9120\n",
            "Batch [25739/28711], Loss: 0.7434\n",
            "Batch [25740/28711], Loss: 1.3364\n",
            "Batch [25741/28711], Loss: 0.9794\n",
            "Batch [25742/28711], Loss: 1.2546\n",
            "Batch [25743/28711], Loss: 0.8396\n",
            "Batch [25744/28711], Loss: 0.6847\n",
            "Batch [25745/28711], Loss: 0.9289\n",
            "Batch [25746/28711], Loss: 0.9217\n",
            "Batch [25747/28711], Loss: 1.1173\n",
            "Batch [25748/28711], Loss: 0.8317\n",
            "Batch [25749/28711], Loss: 0.6876\n",
            "Batch [25750/28711], Loss: 0.8705\n",
            "Batch [25751/28711], Loss: 1.1117\n",
            "Batch [25752/28711], Loss: 0.7394\n",
            "Batch [25753/28711], Loss: 1.0962\n",
            "Batch [25754/28711], Loss: 0.9387\n",
            "Batch [25755/28711], Loss: 1.0581\n",
            "Batch [25756/28711], Loss: 0.7192\n",
            "Batch [25757/28711], Loss: 1.1136\n",
            "Batch [25758/28711], Loss: 0.8908\n",
            "Batch [25759/28711], Loss: 0.7943\n",
            "Batch [25760/28711], Loss: 0.9513\n",
            "Batch [25761/28711], Loss: 1.1157\n",
            "Batch [25762/28711], Loss: 0.9556\n",
            "Batch [25763/28711], Loss: 0.9920\n",
            "Batch [25764/28711], Loss: 0.8721\n",
            "Batch [25765/28711], Loss: 0.8761\n",
            "Batch [25766/28711], Loss: 0.7665\n",
            "Batch [25767/28711], Loss: 0.9199\n",
            "Batch [25768/28711], Loss: 0.8360\n",
            "Batch [25769/28711], Loss: 1.0736\n",
            "Batch [25770/28711], Loss: 0.9806\n",
            "Batch [25771/28711], Loss: 0.9401\n",
            "Batch [25772/28711], Loss: 0.8408\n",
            "Batch [25773/28711], Loss: 0.7844\n",
            "Batch [25774/28711], Loss: 1.1459\n",
            "Batch [25775/28711], Loss: 0.7886\n",
            "Batch [25776/28711], Loss: 0.9476\n",
            "Batch [25777/28711], Loss: 0.8154\n",
            "Batch [25778/28711], Loss: 0.7150\n",
            "Batch [25779/28711], Loss: 0.9629\n",
            "Batch [25780/28711], Loss: 1.0362\n",
            "Batch [25781/28711], Loss: 0.8733\n",
            "Batch [25782/28711], Loss: 1.0844\n",
            "Batch [25783/28711], Loss: 0.9258\n",
            "Batch [25784/28711], Loss: 0.8238\n",
            "Batch [25785/28711], Loss: 1.2331\n",
            "Batch [25786/28711], Loss: 0.9991\n",
            "Batch [25787/28711], Loss: 0.8623\n",
            "Batch [25788/28711], Loss: 0.9641\n",
            "Batch [25789/28711], Loss: 0.8114\n",
            "Batch [25790/28711], Loss: 0.9707\n",
            "Batch [25791/28711], Loss: 0.8580\n",
            "Batch [25792/28711], Loss: 0.9985\n",
            "Batch [25793/28711], Loss: 1.0216\n",
            "Batch [25794/28711], Loss: 0.9444\n",
            "Batch [25795/28711], Loss: 0.9185\n",
            "Batch [25796/28711], Loss: 0.9478\n",
            "Batch [25797/28711], Loss: 0.7512\n",
            "Batch [25798/28711], Loss: 1.0805\n",
            "Batch [25799/28711], Loss: 0.8406\n",
            "Batch [25800/28711], Loss: 1.0744\n",
            "Batch [25801/28711], Loss: 0.7536\n",
            "Batch [25802/28711], Loss: 0.8790\n",
            "Batch [25803/28711], Loss: 1.0984\n",
            "Batch [25804/28711], Loss: 1.0896\n",
            "Batch [25805/28711], Loss: 0.7386\n",
            "Batch [25806/28711], Loss: 0.9763\n",
            "Batch [25807/28711], Loss: 0.7647\n",
            "Batch [25808/28711], Loss: 1.0530\n",
            "Batch [25809/28711], Loss: 0.8454\n",
            "Batch [25810/28711], Loss: 0.8660\n",
            "Batch [25811/28711], Loss: 0.9217\n",
            "Batch [25812/28711], Loss: 1.0715\n",
            "Batch [25813/28711], Loss: 0.9144\n",
            "Batch [25814/28711], Loss: 1.0102\n",
            "Batch [25815/28711], Loss: 0.7998\n",
            "Batch [25816/28711], Loss: 0.9944\n",
            "Batch [25817/28711], Loss: 1.0094\n",
            "Batch [25818/28711], Loss: 1.0144\n",
            "Batch [25819/28711], Loss: 0.9739\n",
            "Batch [25820/28711], Loss: 0.7747\n",
            "Batch [25821/28711], Loss: 0.8767\n",
            "Batch [25822/28711], Loss: 0.7648\n",
            "Batch [25823/28711], Loss: 1.1355\n",
            "Batch [25824/28711], Loss: 1.0548\n",
            "Batch [25825/28711], Loss: 0.7784\n",
            "Batch [25826/28711], Loss: 1.0954\n",
            "Batch [25827/28711], Loss: 0.9790\n",
            "Batch [25828/28711], Loss: 1.2893\n",
            "Batch [25829/28711], Loss: 0.8183\n",
            "Batch [25830/28711], Loss: 0.8204\n",
            "Batch [25831/28711], Loss: 1.0107\n",
            "Batch [25832/28711], Loss: 0.9985\n",
            "Batch [25833/28711], Loss: 0.8902\n",
            "Batch [25834/28711], Loss: 0.6764\n",
            "Batch [25835/28711], Loss: 0.7046\n",
            "Batch [25836/28711], Loss: 1.0012\n",
            "Batch [25837/28711], Loss: 0.8539\n",
            "Batch [25838/28711], Loss: 0.8063\n",
            "Batch [25839/28711], Loss: 0.7658\n",
            "Batch [25840/28711], Loss: 0.7933\n",
            "Batch [25841/28711], Loss: 1.0012\n",
            "Batch [25842/28711], Loss: 0.8322\n",
            "Batch [25843/28711], Loss: 0.6493\n",
            "Batch [25844/28711], Loss: 0.9134\n",
            "Batch [25845/28711], Loss: 1.1816\n",
            "Batch [25846/28711], Loss: 1.1223\n",
            "Batch [25847/28711], Loss: 0.7811\n",
            "Batch [25848/28711], Loss: 0.8621\n",
            "Batch [25849/28711], Loss: 0.9698\n",
            "Batch [25850/28711], Loss: 0.8335\n",
            "Batch [25851/28711], Loss: 0.9630\n",
            "Batch [25852/28711], Loss: 1.2075\n",
            "Batch [25853/28711], Loss: 1.0964\n",
            "Batch [25854/28711], Loss: 0.7801\n",
            "Batch [25855/28711], Loss: 1.0206\n",
            "Batch [25856/28711], Loss: 0.7913\n",
            "Batch [25857/28711], Loss: 1.1358\n",
            "Batch [25858/28711], Loss: 0.8189\n",
            "Batch [25859/28711], Loss: 0.9685\n",
            "Batch [25860/28711], Loss: 1.1360\n",
            "Batch [25861/28711], Loss: 0.9238\n",
            "Batch [25862/28711], Loss: 1.0414\n",
            "Batch [25863/28711], Loss: 0.9511\n",
            "Batch [25864/28711], Loss: 0.9963\n",
            "Batch [25865/28711], Loss: 1.1212\n",
            "Batch [25866/28711], Loss: 0.8374\n",
            "Batch [25867/28711], Loss: 0.9860\n",
            "Batch [25868/28711], Loss: 0.8136\n",
            "Batch [25869/28711], Loss: 0.7453\n",
            "Batch [25870/28711], Loss: 0.8223\n",
            "Batch [25871/28711], Loss: 0.6233\n",
            "Batch [25872/28711], Loss: 0.8677\n",
            "Batch [25873/28711], Loss: 1.4182\n",
            "Batch [25874/28711], Loss: 1.0652\n",
            "Batch [25875/28711], Loss: 1.0210\n",
            "Batch [25876/28711], Loss: 0.8840\n",
            "Batch [25877/28711], Loss: 0.7692\n",
            "Batch [25878/28711], Loss: 0.9133\n",
            "Batch [25879/28711], Loss: 0.8197\n",
            "Batch [25880/28711], Loss: 0.5598\n",
            "Batch [25881/28711], Loss: 0.9155\n",
            "Batch [25882/28711], Loss: 1.0588\n",
            "Batch [25883/28711], Loss: 0.7622\n",
            "Batch [25884/28711], Loss: 0.6001\n",
            "Batch [25885/28711], Loss: 0.6542\n",
            "Batch [25886/28711], Loss: 0.7289\n",
            "Batch [25887/28711], Loss: 0.9266\n",
            "Batch [25888/28711], Loss: 0.9717\n",
            "Batch [25889/28711], Loss: 0.8457\n",
            "Batch [25890/28711], Loss: 1.0989\n",
            "Batch [25891/28711], Loss: 0.6399\n",
            "Batch [25892/28711], Loss: 1.1047\n",
            "Batch [25893/28711], Loss: 0.8189\n",
            "Batch [25894/28711], Loss: 1.0749\n",
            "Batch [25895/28711], Loss: 0.8275\n",
            "Batch [25896/28711], Loss: 0.7741\n",
            "Batch [25897/28711], Loss: 1.1513\n",
            "Batch [25898/28711], Loss: 0.9766\n",
            "Batch [25899/28711], Loss: 1.1733\n",
            "Batch [25900/28711], Loss: 0.7984\n",
            "Batch [25901/28711], Loss: 0.7973\n",
            "Batch [25902/28711], Loss: 0.9257\n",
            "Batch [25903/28711], Loss: 0.7942\n",
            "Batch [25904/28711], Loss: 0.7544\n",
            "Batch [25905/28711], Loss: 0.8029\n",
            "Batch [25906/28711], Loss: 1.0873\n",
            "Batch [25907/28711], Loss: 1.2020\n",
            "Batch [25908/28711], Loss: 0.7565\n",
            "Batch [25909/28711], Loss: 0.9854\n",
            "Batch [25910/28711], Loss: 0.9639\n",
            "Batch [25911/28711], Loss: 0.9602\n",
            "Batch [25912/28711], Loss: 0.8367\n",
            "Batch [25913/28711], Loss: 0.9399\n",
            "Batch [25914/28711], Loss: 0.9289\n",
            "Batch [25915/28711], Loss: 0.6502\n",
            "Batch [25916/28711], Loss: 0.8121\n",
            "Batch [25917/28711], Loss: 1.3253\n",
            "Batch [25918/28711], Loss: 0.8364\n",
            "Batch [25919/28711], Loss: 0.7702\n",
            "Batch [25920/28711], Loss: 0.8877\n",
            "Batch [25921/28711], Loss: 0.8245\n",
            "Batch [25922/28711], Loss: 0.8715\n",
            "Batch [25923/28711], Loss: 0.7833\n",
            "Batch [25924/28711], Loss: 0.7405\n",
            "Batch [25925/28711], Loss: 1.2152\n",
            "Batch [25926/28711], Loss: 0.4771\n",
            "Batch [25927/28711], Loss: 0.8879\n",
            "Batch [25928/28711], Loss: 0.9085\n",
            "Batch [25929/28711], Loss: 0.8664\n",
            "Batch [25930/28711], Loss: 0.7153\n",
            "Batch [25931/28711], Loss: 0.8997\n",
            "Batch [25932/28711], Loss: 0.8333\n",
            "Batch [25933/28711], Loss: 0.7710\n",
            "Batch [25934/28711], Loss: 1.1936\n",
            "Batch [25935/28711], Loss: 1.1374\n",
            "Batch [25936/28711], Loss: 1.1154\n",
            "Batch [25937/28711], Loss: 0.5797\n",
            "Batch [25938/28711], Loss: 1.0601\n",
            "Batch [25939/28711], Loss: 0.7576\n",
            "Batch [25940/28711], Loss: 0.7437\n",
            "Batch [25941/28711], Loss: 0.8716\n",
            "Batch [25942/28711], Loss: 0.7734\n",
            "Batch [25943/28711], Loss: 0.7296\n",
            "Batch [25944/28711], Loss: 1.0507\n",
            "Batch [25945/28711], Loss: 1.0853\n",
            "Batch [25946/28711], Loss: 0.6959\n",
            "Batch [25947/28711], Loss: 0.9846\n",
            "Batch [25948/28711], Loss: 0.8032\n",
            "Batch [25949/28711], Loss: 0.9345\n",
            "Batch [25950/28711], Loss: 1.1118\n",
            "Batch [25951/28711], Loss: 0.9332\n",
            "Batch [25952/28711], Loss: 0.7225\n",
            "Batch [25953/28711], Loss: 0.7156\n",
            "Batch [25954/28711], Loss: 0.9902\n",
            "Batch [25955/28711], Loss: 0.6805\n",
            "Batch [25956/28711], Loss: 1.0237\n",
            "Batch [25957/28711], Loss: 1.0138\n",
            "Batch [25958/28711], Loss: 0.8235\n",
            "Batch [25959/28711], Loss: 0.8432\n",
            "Batch [25960/28711], Loss: 0.7144\n",
            "Batch [25961/28711], Loss: 0.7397\n",
            "Batch [25962/28711], Loss: 0.8466\n",
            "Batch [25963/28711], Loss: 0.7123\n",
            "Batch [25964/28711], Loss: 0.8581\n",
            "Batch [25965/28711], Loss: 0.8651\n",
            "Batch [25966/28711], Loss: 1.0544\n",
            "Batch [25967/28711], Loss: 1.0900\n",
            "Batch [25968/28711], Loss: 0.9226\n",
            "Batch [25969/28711], Loss: 1.0005\n",
            "Batch [25970/28711], Loss: 1.0365\n",
            "Batch [25971/28711], Loss: 0.9247\n",
            "Batch [25972/28711], Loss: 1.0057\n",
            "Batch [25973/28711], Loss: 0.9054\n",
            "Batch [25974/28711], Loss: 1.0168\n",
            "Batch [25975/28711], Loss: 1.2629\n",
            "Batch [25976/28711], Loss: 0.7896\n",
            "Batch [25977/28711], Loss: 0.7340\n",
            "Batch [25978/28711], Loss: 1.0212\n",
            "Batch [25979/28711], Loss: 0.9709\n",
            "Batch [25980/28711], Loss: 0.8028\n",
            "Batch [25981/28711], Loss: 0.9147\n",
            "Batch [25982/28711], Loss: 0.8589\n",
            "Batch [25983/28711], Loss: 0.8397\n",
            "Batch [25984/28711], Loss: 1.2942\n",
            "Batch [25985/28711], Loss: 0.7471\n",
            "Batch [25986/28711], Loss: 0.9697\n",
            "Batch [25987/28711], Loss: 0.9630\n",
            "Batch [25988/28711], Loss: 0.9689\n",
            "Batch [25989/28711], Loss: 0.9045\n",
            "Batch [25990/28711], Loss: 1.0574\n",
            "Batch [25991/28711], Loss: 0.7275\n",
            "Batch [25992/28711], Loss: 0.7473\n",
            "Batch [25993/28711], Loss: 0.6669\n",
            "Batch [25994/28711], Loss: 1.1070\n",
            "Batch [25995/28711], Loss: 0.7821\n",
            "Batch [25996/28711], Loss: 0.9534\n",
            "Batch [25997/28711], Loss: 0.9295\n",
            "Batch [25998/28711], Loss: 1.2765\n",
            "Batch [25999/28711], Loss: 1.1572\n",
            "Batch [26000/28711], Loss: 0.9245\n",
            "Batch [26001/28711], Loss: 0.8773\n",
            "Batch [26002/28711], Loss: 1.0303\n",
            "Batch [26003/28711], Loss: 1.3049\n",
            "Batch [26004/28711], Loss: 0.7618\n",
            "Batch [26005/28711], Loss: 0.9797\n",
            "Batch [26006/28711], Loss: 0.7821\n",
            "Batch [26007/28711], Loss: 0.8775\n",
            "Batch [26008/28711], Loss: 0.9095\n",
            "Batch [26009/28711], Loss: 1.1148\n",
            "Batch [26010/28711], Loss: 0.7822\n",
            "Batch [26011/28711], Loss: 1.0069\n",
            "Batch [26012/28711], Loss: 1.1434\n",
            "Batch [26013/28711], Loss: 1.0433\n",
            "Batch [26014/28711], Loss: 0.9224\n",
            "Batch [26015/28711], Loss: 0.8850\n",
            "Batch [26016/28711], Loss: 0.7044\n",
            "Batch [26017/28711], Loss: 1.1428\n",
            "Batch [26018/28711], Loss: 0.9297\n",
            "Batch [26019/28711], Loss: 1.0103\n",
            "Batch [26020/28711], Loss: 0.9496\n",
            "Batch [26021/28711], Loss: 0.8040\n",
            "Batch [26022/28711], Loss: 0.8843\n",
            "Batch [26023/28711], Loss: 0.9950\n",
            "Batch [26024/28711], Loss: 0.8812\n",
            "Batch [26025/28711], Loss: 0.9273\n",
            "Batch [26026/28711], Loss: 1.1529\n",
            "Batch [26027/28711], Loss: 0.8803\n",
            "Batch [26028/28711], Loss: 0.9911\n",
            "Batch [26029/28711], Loss: 0.8757\n",
            "Batch [26030/28711], Loss: 1.0580\n",
            "Batch [26031/28711], Loss: 0.8248\n",
            "Batch [26032/28711], Loss: 0.8870\n",
            "Batch [26033/28711], Loss: 0.7161\n",
            "Batch [26034/28711], Loss: 0.8713\n",
            "Batch [26035/28711], Loss: 0.8017\n",
            "Batch [26036/28711], Loss: 0.7623\n",
            "Batch [26037/28711], Loss: 1.2175\n",
            "Batch [26038/28711], Loss: 0.8256\n",
            "Batch [26039/28711], Loss: 0.5540\n",
            "Batch [26040/28711], Loss: 0.7880\n",
            "Batch [26041/28711], Loss: 1.0506\n",
            "Batch [26042/28711], Loss: 0.7802\n",
            "Batch [26043/28711], Loss: 0.9858\n",
            "Batch [26044/28711], Loss: 1.0157\n",
            "Batch [26045/28711], Loss: 1.4150\n",
            "Batch [26046/28711], Loss: 0.7726\n",
            "Batch [26047/28711], Loss: 0.7206\n",
            "Batch [26048/28711], Loss: 0.9616\n",
            "Batch [26049/28711], Loss: 1.1130\n",
            "Batch [26050/28711], Loss: 0.7539\n",
            "Batch [26051/28711], Loss: 0.8981\n",
            "Batch [26052/28711], Loss: 0.8676\n",
            "Batch [26053/28711], Loss: 1.0580\n",
            "Batch [26054/28711], Loss: 0.7821\n",
            "Batch [26055/28711], Loss: 0.9537\n",
            "Batch [26056/28711], Loss: 0.9080\n",
            "Batch [26057/28711], Loss: 0.9885\n",
            "Batch [26058/28711], Loss: 1.0342\n",
            "Batch [26059/28711], Loss: 0.8544\n",
            "Batch [26060/28711], Loss: 0.6609\n",
            "Batch [26061/28711], Loss: 1.0102\n",
            "Batch [26062/28711], Loss: 0.7930\n",
            "Batch [26063/28711], Loss: 0.7595\n",
            "Batch [26064/28711], Loss: 1.1524\n",
            "Batch [26065/28711], Loss: 0.8848\n",
            "Batch [26066/28711], Loss: 0.8465\n",
            "Batch [26067/28711], Loss: 1.1055\n",
            "Batch [26068/28711], Loss: 1.0337\n",
            "Batch [26069/28711], Loss: 1.0663\n",
            "Batch [26070/28711], Loss: 0.7666\n",
            "Batch [26071/28711], Loss: 1.1337\n",
            "Batch [26072/28711], Loss: 0.7776\n",
            "Batch [26073/28711], Loss: 1.0552\n",
            "Batch [26074/28711], Loss: 0.9414\n",
            "Batch [26075/28711], Loss: 0.9532\n",
            "Batch [26076/28711], Loss: 0.7902\n",
            "Batch [26077/28711], Loss: 0.7299\n",
            "Batch [26078/28711], Loss: 0.7681\n",
            "Batch [26079/28711], Loss: 0.6117\n",
            "Batch [26080/28711], Loss: 0.7913\n",
            "Batch [26081/28711], Loss: 0.9532\n",
            "Batch [26082/28711], Loss: 0.8271\n",
            "Batch [26083/28711], Loss: 0.8420\n",
            "Batch [26084/28711], Loss: 1.1648\n",
            "Batch [26085/28711], Loss: 0.8203\n",
            "Batch [26086/28711], Loss: 0.7741\n",
            "Batch [26087/28711], Loss: 0.9680\n",
            "Batch [26088/28711], Loss: 0.6324\n",
            "Batch [26089/28711], Loss: 0.9500\n",
            "Batch [26090/28711], Loss: 0.9177\n",
            "Batch [26091/28711], Loss: 0.9304\n",
            "Batch [26092/28711], Loss: 1.0281\n",
            "Batch [26093/28711], Loss: 1.1756\n",
            "Batch [26094/28711], Loss: 0.9359\n",
            "Batch [26095/28711], Loss: 1.1914\n",
            "Batch [26096/28711], Loss: 0.9149\n",
            "Batch [26097/28711], Loss: 1.1446\n",
            "Batch [26098/28711], Loss: 1.0773\n",
            "Batch [26099/28711], Loss: 0.7415\n",
            "Batch [26100/28711], Loss: 0.9234\n",
            "Batch [26101/28711], Loss: 1.0518\n",
            "Batch [26102/28711], Loss: 0.9203\n",
            "Batch [26103/28711], Loss: 0.8494\n",
            "Batch [26104/28711], Loss: 1.0927\n",
            "Batch [26105/28711], Loss: 0.7806\n",
            "Batch [26106/28711], Loss: 0.9552\n",
            "Batch [26107/28711], Loss: 0.9756\n",
            "Batch [26108/28711], Loss: 0.9438\n",
            "Batch [26109/28711], Loss: 0.9796\n",
            "Batch [26110/28711], Loss: 1.0806\n",
            "Batch [26111/28711], Loss: 0.9008\n",
            "Batch [26112/28711], Loss: 0.8172\n",
            "Batch [26113/28711], Loss: 0.8010\n",
            "Batch [26114/28711], Loss: 0.9999\n",
            "Batch [26115/28711], Loss: 0.9756\n",
            "Batch [26116/28711], Loss: 0.7924\n",
            "Batch [26117/28711], Loss: 0.9734\n",
            "Batch [26118/28711], Loss: 0.8912\n",
            "Batch [26119/28711], Loss: 0.9558\n",
            "Batch [26120/28711], Loss: 1.0635\n",
            "Batch [26121/28711], Loss: 0.7459\n",
            "Batch [26122/28711], Loss: 1.1164\n",
            "Batch [26123/28711], Loss: 1.1397\n",
            "Batch [26124/28711], Loss: 1.1588\n",
            "Batch [26125/28711], Loss: 0.8675\n",
            "Batch [26126/28711], Loss: 1.1055\n",
            "Batch [26127/28711], Loss: 0.8382\n",
            "Batch [26128/28711], Loss: 0.7657\n",
            "Batch [26129/28711], Loss: 0.7031\n",
            "Batch [26130/28711], Loss: 0.9051\n",
            "Batch [26131/28711], Loss: 0.7049\n",
            "Batch [26132/28711], Loss: 0.8116\n",
            "Batch [26133/28711], Loss: 0.9341\n",
            "Batch [26134/28711], Loss: 0.8823\n",
            "Batch [26135/28711], Loss: 1.0024\n",
            "Batch [26136/28711], Loss: 0.5881\n",
            "Batch [26137/28711], Loss: 0.9979\n",
            "Batch [26138/28711], Loss: 0.9523\n",
            "Batch [26139/28711], Loss: 0.6524\n",
            "Batch [26140/28711], Loss: 0.9198\n",
            "Batch [26141/28711], Loss: 1.0303\n",
            "Batch [26142/28711], Loss: 0.9001\n",
            "Batch [26143/28711], Loss: 0.9966\n",
            "Batch [26144/28711], Loss: 0.8412\n",
            "Batch [26145/28711], Loss: 0.9202\n",
            "Batch [26146/28711], Loss: 0.8072\n",
            "Batch [26147/28711], Loss: 1.2005\n",
            "Batch [26148/28711], Loss: 0.7429\n",
            "Batch [26149/28711], Loss: 0.7547\n",
            "Batch [26150/28711], Loss: 1.0706\n",
            "Batch [26151/28711], Loss: 0.8015\n",
            "Batch [26152/28711], Loss: 0.9079\n",
            "Batch [26153/28711], Loss: 1.0726\n",
            "Batch [26154/28711], Loss: 1.0102\n",
            "Batch [26155/28711], Loss: 0.9264\n",
            "Batch [26156/28711], Loss: 0.7152\n",
            "Batch [26157/28711], Loss: 0.7330\n",
            "Batch [26158/28711], Loss: 0.7661\n",
            "Batch [26159/28711], Loss: 0.9915\n",
            "Batch [26160/28711], Loss: 0.7214\n",
            "Batch [26161/28711], Loss: 0.8709\n",
            "Batch [26162/28711], Loss: 0.9024\n",
            "Batch [26163/28711], Loss: 0.7481\n",
            "Batch [26164/28711], Loss: 0.9404\n",
            "Batch [26165/28711], Loss: 0.8942\n",
            "Batch [26166/28711], Loss: 0.8441\n",
            "Batch [26167/28711], Loss: 1.0155\n",
            "Batch [26168/28711], Loss: 0.7749\n",
            "Batch [26169/28711], Loss: 0.9919\n",
            "Batch [26170/28711], Loss: 0.7018\n",
            "Batch [26171/28711], Loss: 0.9648\n",
            "Batch [26172/28711], Loss: 0.7845\n",
            "Batch [26173/28711], Loss: 0.7448\n",
            "Batch [26174/28711], Loss: 0.8508\n",
            "Batch [26175/28711], Loss: 0.8649\n",
            "Batch [26176/28711], Loss: 0.7791\n",
            "Batch [26177/28711], Loss: 1.0403\n",
            "Batch [26178/28711], Loss: 0.9157\n",
            "Batch [26179/28711], Loss: 1.0143\n",
            "Batch [26180/28711], Loss: 0.7215\n",
            "Batch [26181/28711], Loss: 0.9312\n",
            "Batch [26182/28711], Loss: 0.7641\n",
            "Batch [26183/28711], Loss: 0.7487\n",
            "Batch [26184/28711], Loss: 1.0024\n",
            "Batch [26185/28711], Loss: 0.9328\n",
            "Batch [26186/28711], Loss: 1.0875\n",
            "Batch [26187/28711], Loss: 0.8884\n",
            "Batch [26188/28711], Loss: 0.7921\n",
            "Batch [26189/28711], Loss: 0.8383\n",
            "Batch [26190/28711], Loss: 0.9519\n",
            "Batch [26191/28711], Loss: 0.7306\n",
            "Batch [26192/28711], Loss: 0.8582\n",
            "Batch [26193/28711], Loss: 0.9652\n",
            "Batch [26194/28711], Loss: 0.8895\n",
            "Batch [26195/28711], Loss: 0.9963\n",
            "Batch [26196/28711], Loss: 0.6968\n",
            "Batch [26197/28711], Loss: 0.7581\n",
            "Batch [26198/28711], Loss: 1.0458\n",
            "Batch [26199/28711], Loss: 1.1807\n",
            "Batch [26200/28711], Loss: 0.8577\n",
            "Batch [26201/28711], Loss: 0.7473\n",
            "Batch [26202/28711], Loss: 1.2982\n",
            "Batch [26203/28711], Loss: 0.6811\n",
            "Batch [26204/28711], Loss: 0.9286\n",
            "Batch [26205/28711], Loss: 0.8963\n",
            "Batch [26206/28711], Loss: 0.8479\n",
            "Batch [26207/28711], Loss: 0.7837\n",
            "Batch [26208/28711], Loss: 0.8426\n",
            "Batch [26209/28711], Loss: 0.8142\n",
            "Batch [26210/28711], Loss: 1.2452\n",
            "Batch [26211/28711], Loss: 1.1544\n",
            "Batch [26212/28711], Loss: 0.8824\n",
            "Batch [26213/28711], Loss: 1.2636\n",
            "Batch [26214/28711], Loss: 0.6133\n",
            "Batch [26215/28711], Loss: 0.8441\n",
            "Batch [26216/28711], Loss: 0.7919\n",
            "Batch [26217/28711], Loss: 1.0047\n",
            "Batch [26218/28711], Loss: 0.9939\n",
            "Batch [26219/28711], Loss: 1.1417\n",
            "Batch [26220/28711], Loss: 0.9701\n",
            "Batch [26221/28711], Loss: 0.7587\n",
            "Batch [26222/28711], Loss: 0.9590\n",
            "Batch [26223/28711], Loss: 0.8640\n",
            "Batch [26224/28711], Loss: 0.9234\n",
            "Batch [26225/28711], Loss: 1.1044\n",
            "Batch [26226/28711], Loss: 0.8853\n",
            "Batch [26227/28711], Loss: 0.6516\n",
            "Batch [26228/28711], Loss: 1.0044\n",
            "Batch [26229/28711], Loss: 0.7828\n",
            "Batch [26230/28711], Loss: 0.8844\n",
            "Batch [26231/28711], Loss: 0.7733\n",
            "Batch [26232/28711], Loss: 1.0454\n",
            "Batch [26233/28711], Loss: 0.9041\n",
            "Batch [26234/28711], Loss: 0.8801\n",
            "Batch [26235/28711], Loss: 0.8742\n",
            "Batch [26236/28711], Loss: 0.8489\n",
            "Batch [26237/28711], Loss: 0.9966\n",
            "Batch [26238/28711], Loss: 0.6938\n",
            "Batch [26239/28711], Loss: 0.8765\n",
            "Batch [26240/28711], Loss: 0.8410\n",
            "Batch [26241/28711], Loss: 0.7620\n",
            "Batch [26242/28711], Loss: 0.8089\n",
            "Batch [26243/28711], Loss: 0.5375\n",
            "Batch [26244/28711], Loss: 0.8534\n",
            "Batch [26245/28711], Loss: 0.8297\n",
            "Batch [26246/28711], Loss: 0.8874\n",
            "Batch [26247/28711], Loss: 0.8307\n",
            "Batch [26248/28711], Loss: 0.9137\n",
            "Batch [26249/28711], Loss: 0.9251\n",
            "Batch [26250/28711], Loss: 0.7705\n",
            "Batch [26251/28711], Loss: 0.7692\n",
            "Batch [26252/28711], Loss: 0.9414\n",
            "Batch [26253/28711], Loss: 1.1230\n",
            "Batch [26254/28711], Loss: 1.1106\n",
            "Batch [26255/28711], Loss: 0.7029\n",
            "Batch [26256/28711], Loss: 0.8612\n",
            "Batch [26257/28711], Loss: 0.8650\n",
            "Batch [26258/28711], Loss: 0.9279\n",
            "Batch [26259/28711], Loss: 1.0134\n",
            "Batch [26260/28711], Loss: 0.8159\n",
            "Batch [26261/28711], Loss: 1.0285\n",
            "Batch [26262/28711], Loss: 0.8394\n",
            "Batch [26263/28711], Loss: 0.6586\n",
            "Batch [26264/28711], Loss: 1.2151\n",
            "Batch [26265/28711], Loss: 0.6800\n",
            "Batch [26266/28711], Loss: 0.6748\n",
            "Batch [26267/28711], Loss: 0.9848\n",
            "Batch [26268/28711], Loss: 1.0743\n",
            "Batch [26269/28711], Loss: 1.1451\n",
            "Batch [26270/28711], Loss: 0.7485\n",
            "Batch [26271/28711], Loss: 1.1075\n",
            "Batch [26272/28711], Loss: 0.9095\n",
            "Batch [26273/28711], Loss: 1.1101\n",
            "Batch [26274/28711], Loss: 0.9710\n",
            "Batch [26275/28711], Loss: 0.7122\n",
            "Batch [26276/28711], Loss: 0.9756\n",
            "Batch [26277/28711], Loss: 0.9792\n",
            "Batch [26278/28711], Loss: 0.7889\n",
            "Batch [26279/28711], Loss: 1.0092\n",
            "Batch [26280/28711], Loss: 0.6763\n",
            "Batch [26281/28711], Loss: 0.6739\n",
            "Batch [26282/28711], Loss: 0.7375\n",
            "Batch [26283/28711], Loss: 1.0680\n",
            "Batch [26284/28711], Loss: 1.0348\n",
            "Batch [26285/28711], Loss: 0.9331\n",
            "Batch [26286/28711], Loss: 1.1050\n",
            "Batch [26287/28711], Loss: 0.9581\n",
            "Batch [26288/28711], Loss: 0.8236\n",
            "Batch [26289/28711], Loss: 1.0500\n",
            "Batch [26290/28711], Loss: 0.9366\n",
            "Batch [26291/28711], Loss: 1.0111\n",
            "Batch [26292/28711], Loss: 0.5913\n",
            "Batch [26293/28711], Loss: 1.0015\n",
            "Batch [26294/28711], Loss: 0.8585\n",
            "Batch [26295/28711], Loss: 1.0351\n",
            "Batch [26296/28711], Loss: 0.6975\n",
            "Batch [26297/28711], Loss: 1.0733\n",
            "Batch [26298/28711], Loss: 0.9541\n",
            "Batch [26299/28711], Loss: 0.8712\n",
            "Batch [26300/28711], Loss: 1.2447\n",
            "Batch [26301/28711], Loss: 0.9041\n",
            "Batch [26302/28711], Loss: 1.0144\n",
            "Batch [26303/28711], Loss: 1.1590\n",
            "Batch [26304/28711], Loss: 0.9629\n",
            "Batch [26305/28711], Loss: 0.8274\n",
            "Batch [26306/28711], Loss: 0.9136\n",
            "Batch [26307/28711], Loss: 0.8686\n",
            "Batch [26308/28711], Loss: 0.7316\n",
            "Batch [26309/28711], Loss: 0.8670\n",
            "Batch [26310/28711], Loss: 1.1882\n",
            "Batch [26311/28711], Loss: 1.0514\n",
            "Batch [26312/28711], Loss: 1.1080\n",
            "Batch [26313/28711], Loss: 0.6859\n",
            "Batch [26314/28711], Loss: 0.9195\n",
            "Batch [26315/28711], Loss: 0.5717\n",
            "Batch [26316/28711], Loss: 1.1165\n",
            "Batch [26317/28711], Loss: 0.8653\n",
            "Batch [26318/28711], Loss: 0.7095\n",
            "Batch [26319/28711], Loss: 0.9095\n",
            "Batch [26320/28711], Loss: 0.8438\n",
            "Batch [26321/28711], Loss: 1.0809\n",
            "Batch [26322/28711], Loss: 0.8955\n",
            "Batch [26323/28711], Loss: 0.8011\n",
            "Batch [26324/28711], Loss: 1.0150\n",
            "Batch [26325/28711], Loss: 0.7027\n",
            "Batch [26326/28711], Loss: 0.9757\n",
            "Batch [26327/28711], Loss: 0.8542\n",
            "Batch [26328/28711], Loss: 0.8354\n",
            "Batch [26329/28711], Loss: 0.8633\n",
            "Batch [26330/28711], Loss: 0.9956\n",
            "Batch [26331/28711], Loss: 0.8158\n",
            "Batch [26332/28711], Loss: 1.0090\n",
            "Batch [26333/28711], Loss: 0.8409\n",
            "Batch [26334/28711], Loss: 0.9955\n",
            "Batch [26335/28711], Loss: 0.6765\n",
            "Batch [26336/28711], Loss: 0.7740\n",
            "Batch [26337/28711], Loss: 0.5435\n",
            "Batch [26338/28711], Loss: 1.0761\n",
            "Batch [26339/28711], Loss: 1.0551\n",
            "Batch [26340/28711], Loss: 1.0557\n",
            "Batch [26341/28711], Loss: 0.8554\n",
            "Batch [26342/28711], Loss: 0.9007\n",
            "Batch [26343/28711], Loss: 0.7404\n",
            "Batch [26344/28711], Loss: 0.8107\n",
            "Batch [26345/28711], Loss: 1.0007\n",
            "Batch [26346/28711], Loss: 0.9041\n",
            "Batch [26347/28711], Loss: 0.8610\n",
            "Batch [26348/28711], Loss: 1.0605\n",
            "Batch [26349/28711], Loss: 0.5796\n",
            "Batch [26350/28711], Loss: 0.7553\n",
            "Batch [26351/28711], Loss: 0.9677\n",
            "Batch [26352/28711], Loss: 0.9750\n",
            "Batch [26353/28711], Loss: 0.9840\n",
            "Batch [26354/28711], Loss: 0.9243\n",
            "Batch [26355/28711], Loss: 0.9440\n",
            "Batch [26356/28711], Loss: 0.7704\n",
            "Batch [26357/28711], Loss: 1.1704\n",
            "Batch [26358/28711], Loss: 0.7772\n",
            "Batch [26359/28711], Loss: 0.8643\n",
            "Batch [26360/28711], Loss: 0.7003\n",
            "Batch [26361/28711], Loss: 0.6433\n",
            "Batch [26362/28711], Loss: 0.8062\n",
            "Batch [26363/28711], Loss: 0.8587\n",
            "Batch [26364/28711], Loss: 0.9615\n",
            "Batch [26365/28711], Loss: 0.8739\n",
            "Batch [26366/28711], Loss: 0.7334\n",
            "Batch [26367/28711], Loss: 0.9414\n",
            "Batch [26368/28711], Loss: 0.9411\n",
            "Batch [26369/28711], Loss: 0.9239\n",
            "Batch [26370/28711], Loss: 1.0234\n",
            "Batch [26371/28711], Loss: 0.8938\n",
            "Batch [26372/28711], Loss: 0.7088\n",
            "Batch [26373/28711], Loss: 0.8990\n",
            "Batch [26374/28711], Loss: 0.8724\n",
            "Batch [26375/28711], Loss: 0.7426\n",
            "Batch [26376/28711], Loss: 0.9936\n",
            "Batch [26377/28711], Loss: 0.8319\n",
            "Batch [26378/28711], Loss: 0.7881\n",
            "Batch [26379/28711], Loss: 0.9140\n",
            "Batch [26380/28711], Loss: 0.8580\n",
            "Batch [26381/28711], Loss: 0.9411\n",
            "Batch [26382/28711], Loss: 1.1160\n",
            "Batch [26383/28711], Loss: 0.8880\n",
            "Batch [26384/28711], Loss: 1.1505\n",
            "Batch [26385/28711], Loss: 1.0536\n",
            "Batch [26386/28711], Loss: 0.9455\n",
            "Batch [26387/28711], Loss: 1.0487\n",
            "Batch [26388/28711], Loss: 1.0676\n",
            "Batch [26389/28711], Loss: 0.9167\n",
            "Batch [26390/28711], Loss: 1.0617\n",
            "Batch [26391/28711], Loss: 0.6584\n",
            "Batch [26392/28711], Loss: 0.8414\n",
            "Batch [26393/28711], Loss: 0.8685\n",
            "Batch [26394/28711], Loss: 0.9319\n",
            "Batch [26395/28711], Loss: 0.8425\n",
            "Batch [26396/28711], Loss: 0.9651\n",
            "Batch [26397/28711], Loss: 1.0819\n",
            "Batch [26398/28711], Loss: 0.7894\n",
            "Batch [26399/28711], Loss: 1.0797\n",
            "Batch [26400/28711], Loss: 0.9358\n",
            "Batch [26401/28711], Loss: 1.1025\n",
            "Batch [26402/28711], Loss: 1.1172\n",
            "Batch [26403/28711], Loss: 0.9750\n",
            "Batch [26404/28711], Loss: 0.7823\n",
            "Batch [26405/28711], Loss: 0.7190\n",
            "Batch [26406/28711], Loss: 0.9171\n",
            "Batch [26407/28711], Loss: 0.8103\n",
            "Batch [26408/28711], Loss: 1.0424\n",
            "Batch [26409/28711], Loss: 1.0542\n",
            "Batch [26410/28711], Loss: 0.7791\n",
            "Batch [26411/28711], Loss: 1.0426\n",
            "Batch [26412/28711], Loss: 0.8717\n",
            "Batch [26413/28711], Loss: 0.6199\n",
            "Batch [26414/28711], Loss: 0.8502\n",
            "Batch [26415/28711], Loss: 0.7640\n",
            "Batch [26416/28711], Loss: 0.8221\n",
            "Batch [26417/28711], Loss: 0.8849\n",
            "Batch [26418/28711], Loss: 0.8545\n",
            "Batch [26419/28711], Loss: 1.0958\n",
            "Batch [26420/28711], Loss: 0.9287\n",
            "Batch [26421/28711], Loss: 1.3426\n",
            "Batch [26422/28711], Loss: 1.1091\n",
            "Batch [26423/28711], Loss: 0.8008\n",
            "Batch [26424/28711], Loss: 0.6168\n",
            "Batch [26425/28711], Loss: 0.8739\n",
            "Batch [26426/28711], Loss: 0.8109\n",
            "Batch [26427/28711], Loss: 0.7431\n",
            "Batch [26428/28711], Loss: 0.7738\n",
            "Batch [26429/28711], Loss: 0.9135\n",
            "Batch [26430/28711], Loss: 0.8151\n",
            "Batch [26431/28711], Loss: 0.7585\n",
            "Batch [26432/28711], Loss: 1.1420\n",
            "Batch [26433/28711], Loss: 0.8954\n",
            "Batch [26434/28711], Loss: 0.9774\n",
            "Batch [26435/28711], Loss: 1.0443\n",
            "Batch [26436/28711], Loss: 0.9291\n",
            "Batch [26437/28711], Loss: 0.9009\n",
            "Batch [26438/28711], Loss: 1.1941\n",
            "Batch [26439/28711], Loss: 0.9031\n",
            "Batch [26440/28711], Loss: 0.8991\n",
            "Batch [26441/28711], Loss: 0.8757\n",
            "Batch [26442/28711], Loss: 0.9952\n",
            "Batch [26443/28711], Loss: 0.9223\n",
            "Batch [26444/28711], Loss: 1.0436\n",
            "Batch [26445/28711], Loss: 1.1105\n",
            "Batch [26446/28711], Loss: 0.8446\n",
            "Batch [26447/28711], Loss: 0.8364\n",
            "Batch [26448/28711], Loss: 0.8292\n",
            "Batch [26449/28711], Loss: 0.8472\n",
            "Batch [26450/28711], Loss: 1.0426\n",
            "Batch [26451/28711], Loss: 0.9154\n",
            "Batch [26452/28711], Loss: 0.7343\n",
            "Batch [26453/28711], Loss: 0.9319\n",
            "Batch [26454/28711], Loss: 0.9856\n",
            "Batch [26455/28711], Loss: 0.6133\n",
            "Batch [26456/28711], Loss: 0.8888\n",
            "Batch [26457/28711], Loss: 1.1759\n",
            "Batch [26458/28711], Loss: 0.8522\n",
            "Batch [26459/28711], Loss: 0.8223\n",
            "Batch [26460/28711], Loss: 0.8766\n",
            "Batch [26461/28711], Loss: 0.8009\n",
            "Batch [26462/28711], Loss: 0.7669\n",
            "Batch [26463/28711], Loss: 0.6414\n",
            "Batch [26464/28711], Loss: 0.9327\n",
            "Batch [26465/28711], Loss: 0.7081\n",
            "Batch [26466/28711], Loss: 1.1102\n",
            "Batch [26467/28711], Loss: 1.1969\n",
            "Batch [26468/28711], Loss: 1.2338\n",
            "Batch [26469/28711], Loss: 0.8691\n",
            "Batch [26470/28711], Loss: 0.9093\n",
            "Batch [26471/28711], Loss: 0.8637\n",
            "Batch [26472/28711], Loss: 1.0777\n",
            "Batch [26473/28711], Loss: 0.9187\n",
            "Batch [26474/28711], Loss: 1.1634\n",
            "Batch [26475/28711], Loss: 1.1385\n",
            "Batch [26476/28711], Loss: 0.9054\n",
            "Batch [26477/28711], Loss: 0.9283\n",
            "Batch [26478/28711], Loss: 0.8025\n",
            "Batch [26479/28711], Loss: 1.3661\n",
            "Batch [26480/28711], Loss: 1.0552\n",
            "Batch [26481/28711], Loss: 0.9908\n",
            "Batch [26482/28711], Loss: 1.1905\n",
            "Batch [26483/28711], Loss: 1.0677\n",
            "Batch [26484/28711], Loss: 1.2545\n",
            "Batch [26485/28711], Loss: 0.7539\n",
            "Batch [26486/28711], Loss: 1.1324\n",
            "Batch [26487/28711], Loss: 0.7519\n",
            "Batch [26488/28711], Loss: 0.8255\n",
            "Batch [26489/28711], Loss: 0.8037\n",
            "Batch [26490/28711], Loss: 1.1215\n",
            "Batch [26491/28711], Loss: 1.0619\n",
            "Batch [26492/28711], Loss: 1.0156\n",
            "Batch [26493/28711], Loss: 0.9498\n",
            "Batch [26494/28711], Loss: 0.8721\n",
            "Batch [26495/28711], Loss: 0.9959\n",
            "Batch [26496/28711], Loss: 1.1016\n",
            "Batch [26497/28711], Loss: 0.9385\n",
            "Batch [26498/28711], Loss: 0.9725\n",
            "Batch [26499/28711], Loss: 1.0339\n",
            "Batch [26500/28711], Loss: 0.8572\n",
            "Batch [26501/28711], Loss: 0.7145\n",
            "Batch [26502/28711], Loss: 0.9976\n",
            "Batch [26503/28711], Loss: 0.8664\n",
            "Batch [26504/28711], Loss: 0.8870\n",
            "Batch [26505/28711], Loss: 0.9317\n",
            "Batch [26506/28711], Loss: 0.7270\n",
            "Batch [26507/28711], Loss: 0.8637\n",
            "Batch [26508/28711], Loss: 0.8845\n",
            "Batch [26509/28711], Loss: 0.8044\n",
            "Batch [26510/28711], Loss: 1.2005\n",
            "Batch [26511/28711], Loss: 0.9977\n",
            "Batch [26512/28711], Loss: 1.0811\n",
            "Batch [26513/28711], Loss: 0.6767\n",
            "Batch [26514/28711], Loss: 0.9369\n",
            "Batch [26515/28711], Loss: 0.7599\n",
            "Batch [26516/28711], Loss: 0.8121\n",
            "Batch [26517/28711], Loss: 0.6716\n",
            "Batch [26518/28711], Loss: 1.2246\n",
            "Batch [26519/28711], Loss: 1.0811\n",
            "Batch [26520/28711], Loss: 0.7605\n",
            "Batch [26521/28711], Loss: 1.0758\n",
            "Batch [26522/28711], Loss: 1.0272\n",
            "Batch [26523/28711], Loss: 1.1146\n",
            "Batch [26524/28711], Loss: 0.8258\n",
            "Batch [26525/28711], Loss: 1.0277\n",
            "Batch [26526/28711], Loss: 0.9806\n",
            "Batch [26527/28711], Loss: 1.0872\n",
            "Batch [26528/28711], Loss: 0.6127\n",
            "Batch [26529/28711], Loss: 0.7553\n",
            "Batch [26530/28711], Loss: 1.1946\n",
            "Batch [26531/28711], Loss: 0.9196\n",
            "Batch [26532/28711], Loss: 0.9362\n",
            "Batch [26533/28711], Loss: 0.7193\n",
            "Batch [26534/28711], Loss: 0.8728\n",
            "Batch [26535/28711], Loss: 1.0497\n",
            "Batch [26536/28711], Loss: 0.8495\n",
            "Batch [26537/28711], Loss: 0.9164\n",
            "Batch [26538/28711], Loss: 0.6487\n",
            "Batch [26539/28711], Loss: 0.7757\n",
            "Batch [26540/28711], Loss: 0.7936\n",
            "Batch [26541/28711], Loss: 1.0682\n",
            "Batch [26542/28711], Loss: 1.0206\n",
            "Batch [26543/28711], Loss: 1.2235\n",
            "Batch [26544/28711], Loss: 0.8038\n",
            "Batch [26545/28711], Loss: 0.8008\n",
            "Batch [26546/28711], Loss: 1.1881\n",
            "Batch [26547/28711], Loss: 0.7046\n",
            "Batch [26548/28711], Loss: 0.9958\n",
            "Batch [26549/28711], Loss: 0.8297\n",
            "Batch [26550/28711], Loss: 1.2281\n",
            "Batch [26551/28711], Loss: 1.0231\n",
            "Batch [26552/28711], Loss: 1.0995\n",
            "Batch [26553/28711], Loss: 1.0575\n",
            "Batch [26554/28711], Loss: 0.8960\n",
            "Batch [26555/28711], Loss: 0.7527\n",
            "Batch [26556/28711], Loss: 0.7863\n",
            "Batch [26557/28711], Loss: 1.1901\n",
            "Batch [26558/28711], Loss: 0.8591\n",
            "Batch [26559/28711], Loss: 0.7459\n",
            "Batch [26560/28711], Loss: 0.9189\n",
            "Batch [26561/28711], Loss: 0.9310\n",
            "Batch [26562/28711], Loss: 0.9475\n",
            "Batch [26563/28711], Loss: 0.9499\n",
            "Batch [26564/28711], Loss: 0.8201\n",
            "Batch [26565/28711], Loss: 0.9996\n",
            "Batch [26566/28711], Loss: 0.9015\n",
            "Batch [26567/28711], Loss: 1.0587\n",
            "Batch [26568/28711], Loss: 1.0926\n",
            "Batch [26569/28711], Loss: 0.7405\n",
            "Batch [26570/28711], Loss: 0.9971\n",
            "Batch [26571/28711], Loss: 1.0207\n",
            "Batch [26572/28711], Loss: 1.2381\n",
            "Batch [26573/28711], Loss: 1.0418\n",
            "Batch [26574/28711], Loss: 1.0799\n",
            "Batch [26575/28711], Loss: 0.7676\n",
            "Batch [26576/28711], Loss: 0.8774\n",
            "Batch [26577/28711], Loss: 0.7880\n",
            "Batch [26578/28711], Loss: 0.7981\n",
            "Batch [26579/28711], Loss: 0.9206\n",
            "Batch [26580/28711], Loss: 0.8423\n",
            "Batch [26581/28711], Loss: 0.8233\n",
            "Batch [26582/28711], Loss: 0.6995\n",
            "Batch [26583/28711], Loss: 1.0146\n",
            "Batch [26584/28711], Loss: 0.8113\n",
            "Batch [26585/28711], Loss: 0.9915\n",
            "Batch [26586/28711], Loss: 0.8437\n",
            "Batch [26587/28711], Loss: 1.1491\n",
            "Batch [26588/28711], Loss: 1.1961\n",
            "Batch [26589/28711], Loss: 0.8517\n",
            "Batch [26590/28711], Loss: 0.9686\n",
            "Batch [26591/28711], Loss: 1.2145\n",
            "Batch [26592/28711], Loss: 0.9423\n",
            "Batch [26593/28711], Loss: 1.0404\n",
            "Batch [26594/28711], Loss: 0.8267\n",
            "Batch [26595/28711], Loss: 0.7992\n",
            "Batch [26596/28711], Loss: 1.0879\n",
            "Batch [26597/28711], Loss: 0.7806\n",
            "Batch [26598/28711], Loss: 1.1775\n",
            "Batch [26599/28711], Loss: 0.9739\n",
            "Batch [26600/28711], Loss: 0.9712\n",
            "Batch [26601/28711], Loss: 0.7760\n",
            "Batch [26602/28711], Loss: 0.7967\n",
            "Batch [26603/28711], Loss: 1.0421\n",
            "Batch [26604/28711], Loss: 1.0405\n",
            "Batch [26605/28711], Loss: 0.9478\n",
            "Batch [26606/28711], Loss: 1.0669\n",
            "Batch [26607/28711], Loss: 0.6241\n",
            "Batch [26608/28711], Loss: 0.8053\n",
            "Batch [26609/28711], Loss: 0.8973\n",
            "Batch [26610/28711], Loss: 1.1527\n",
            "Batch [26611/28711], Loss: 1.0098\n",
            "Batch [26612/28711], Loss: 1.0811\n",
            "Batch [26613/28711], Loss: 0.7957\n",
            "Batch [26614/28711], Loss: 0.9599\n",
            "Batch [26615/28711], Loss: 1.0569\n",
            "Batch [26616/28711], Loss: 0.8625\n",
            "Batch [26617/28711], Loss: 0.8011\n",
            "Batch [26618/28711], Loss: 0.7887\n",
            "Batch [26619/28711], Loss: 0.9821\n",
            "Batch [26620/28711], Loss: 1.0006\n",
            "Batch [26621/28711], Loss: 0.8694\n",
            "Batch [26622/28711], Loss: 0.7795\n",
            "Batch [26623/28711], Loss: 1.1764\n",
            "Batch [26624/28711], Loss: 0.6689\n",
            "Batch [26625/28711], Loss: 0.9821\n",
            "Batch [26626/28711], Loss: 0.8760\n",
            "Batch [26627/28711], Loss: 1.0521\n",
            "Batch [26628/28711], Loss: 1.1166\n",
            "Batch [26629/28711], Loss: 1.1400\n",
            "Batch [26630/28711], Loss: 0.9925\n",
            "Batch [26631/28711], Loss: 0.8167\n",
            "Batch [26632/28711], Loss: 0.7552\n",
            "Batch [26633/28711], Loss: 0.9153\n",
            "Batch [26634/28711], Loss: 1.1010\n",
            "Batch [26635/28711], Loss: 0.8491\n",
            "Batch [26636/28711], Loss: 0.8970\n",
            "Batch [26637/28711], Loss: 0.8033\n",
            "Batch [26638/28711], Loss: 0.8286\n",
            "Batch [26639/28711], Loss: 0.9186\n",
            "Batch [26640/28711], Loss: 0.7451\n",
            "Batch [26641/28711], Loss: 1.0910\n",
            "Batch [26642/28711], Loss: 1.1542\n",
            "Batch [26643/28711], Loss: 0.7829\n",
            "Batch [26644/28711], Loss: 0.7704\n",
            "Batch [26645/28711], Loss: 1.0147\n",
            "Batch [26646/28711], Loss: 1.0306\n",
            "Batch [26647/28711], Loss: 0.7178\n",
            "Batch [26648/28711], Loss: 0.7696\n",
            "Batch [26649/28711], Loss: 0.8799\n",
            "Batch [26650/28711], Loss: 1.1921\n",
            "Batch [26651/28711], Loss: 0.8086\n",
            "Batch [26652/28711], Loss: 0.9895\n",
            "Batch [26653/28711], Loss: 0.7178\n",
            "Batch [26654/28711], Loss: 1.1843\n",
            "Batch [26655/28711], Loss: 0.7290\n",
            "Batch [26656/28711], Loss: 0.7509\n",
            "Batch [26657/28711], Loss: 0.7237\n",
            "Batch [26658/28711], Loss: 0.7521\n",
            "Batch [26659/28711], Loss: 0.7202\n",
            "Batch [26660/28711], Loss: 1.0048\n",
            "Batch [26661/28711], Loss: 1.1319\n",
            "Batch [26662/28711], Loss: 0.9551\n",
            "Batch [26663/28711], Loss: 1.0871\n",
            "Batch [26664/28711], Loss: 1.1215\n",
            "Batch [26665/28711], Loss: 0.7437\n",
            "Batch [26666/28711], Loss: 0.9308\n",
            "Batch [26667/28711], Loss: 0.9440\n",
            "Batch [26668/28711], Loss: 1.0464\n",
            "Batch [26669/28711], Loss: 1.0218\n",
            "Batch [26670/28711], Loss: 0.9607\n",
            "Batch [26671/28711], Loss: 1.1383\n",
            "Batch [26672/28711], Loss: 1.0293\n",
            "Batch [26673/28711], Loss: 0.8875\n",
            "Batch [26674/28711], Loss: 0.8525\n",
            "Batch [26675/28711], Loss: 1.1204\n",
            "Batch [26676/28711], Loss: 0.7579\n",
            "Batch [26677/28711], Loss: 0.9386\n",
            "Batch [26678/28711], Loss: 0.8581\n",
            "Batch [26679/28711], Loss: 0.9719\n",
            "Batch [26680/28711], Loss: 0.8462\n",
            "Batch [26681/28711], Loss: 0.9475\n",
            "Batch [26682/28711], Loss: 0.8478\n",
            "Batch [26683/28711], Loss: 1.0040\n",
            "Batch [26684/28711], Loss: 0.7434\n",
            "Batch [26685/28711], Loss: 1.1553\n",
            "Batch [26686/28711], Loss: 0.7163\n",
            "Batch [26687/28711], Loss: 1.1848\n",
            "Batch [26688/28711], Loss: 0.8994\n",
            "Batch [26689/28711], Loss: 1.0070\n",
            "Batch [26690/28711], Loss: 0.8207\n",
            "Batch [26691/28711], Loss: 0.8094\n",
            "Batch [26692/28711], Loss: 1.0763\n",
            "Batch [26693/28711], Loss: 0.7730\n",
            "Batch [26694/28711], Loss: 0.6896\n",
            "Batch [26695/28711], Loss: 0.8427\n",
            "Batch [26696/28711], Loss: 0.8325\n",
            "Batch [26697/28711], Loss: 0.8457\n",
            "Batch [26698/28711], Loss: 0.8986\n",
            "Batch [26699/28711], Loss: 1.1469\n",
            "Batch [26700/28711], Loss: 1.2182\n",
            "Batch [26701/28711], Loss: 0.9224\n",
            "Batch [26702/28711], Loss: 1.0475\n",
            "Batch [26703/28711], Loss: 0.6631\n",
            "Batch [26704/28711], Loss: 0.7562\n",
            "Batch [26705/28711], Loss: 1.1687\n",
            "Batch [26706/28711], Loss: 1.0211\n",
            "Batch [26707/28711], Loss: 1.0119\n",
            "Batch [26708/28711], Loss: 0.9249\n",
            "Batch [26709/28711], Loss: 0.7794\n",
            "Batch [26710/28711], Loss: 1.1000\n",
            "Batch [26711/28711], Loss: 1.0604\n",
            "Batch [26712/28711], Loss: 0.7468\n",
            "Batch [26713/28711], Loss: 0.8559\n",
            "Batch [26714/28711], Loss: 0.8570\n",
            "Batch [26715/28711], Loss: 0.8663\n",
            "Batch [26716/28711], Loss: 0.7956\n",
            "Batch [26717/28711], Loss: 0.8583\n",
            "Batch [26718/28711], Loss: 0.8569\n",
            "Batch [26719/28711], Loss: 0.8151\n",
            "Batch [26720/28711], Loss: 0.7968\n",
            "Batch [26721/28711], Loss: 0.9714\n",
            "Batch [26722/28711], Loss: 1.6391\n",
            "Batch [26723/28711], Loss: 0.9834\n",
            "Batch [26724/28711], Loss: 1.5176\n",
            "Batch [26725/28711], Loss: 0.6967\n",
            "Batch [26726/28711], Loss: 0.9164\n",
            "Batch [26727/28711], Loss: 0.8238\n",
            "Batch [26728/28711], Loss: 0.7093\n",
            "Batch [26729/28711], Loss: 0.8476\n",
            "Batch [26730/28711], Loss: 0.7017\n",
            "Batch [26731/28711], Loss: 1.0626\n",
            "Batch [26732/28711], Loss: 0.9729\n",
            "Batch [26733/28711], Loss: 0.9664\n",
            "Batch [26734/28711], Loss: 0.8515\n",
            "Batch [26735/28711], Loss: 0.7778\n",
            "Batch [26736/28711], Loss: 1.0178\n",
            "Batch [26737/28711], Loss: 1.1096\n",
            "Batch [26738/28711], Loss: 0.9383\n",
            "Batch [26739/28711], Loss: 0.9102\n",
            "Batch [26740/28711], Loss: 0.9293\n",
            "Batch [26741/28711], Loss: 1.1794\n",
            "Batch [26742/28711], Loss: 0.7724\n",
            "Batch [26743/28711], Loss: 0.8066\n",
            "Batch [26744/28711], Loss: 0.7309\n",
            "Batch [26745/28711], Loss: 1.1044\n",
            "Batch [26746/28711], Loss: 1.0169\n",
            "Batch [26747/28711], Loss: 0.9306\n",
            "Batch [26748/28711], Loss: 0.8777\n",
            "Batch [26749/28711], Loss: 0.7510\n",
            "Batch [26750/28711], Loss: 0.6576\n",
            "Batch [26751/28711], Loss: 0.5971\n",
            "Batch [26752/28711], Loss: 0.8664\n",
            "Batch [26753/28711], Loss: 0.9124\n",
            "Batch [26754/28711], Loss: 0.9462\n",
            "Batch [26755/28711], Loss: 1.0208\n",
            "Batch [26756/28711], Loss: 1.1424\n",
            "Batch [26757/28711], Loss: 0.7821\n",
            "Batch [26758/28711], Loss: 1.0197\n",
            "Batch [26759/28711], Loss: 1.2576\n",
            "Batch [26760/28711], Loss: 1.0175\n",
            "Batch [26761/28711], Loss: 0.9598\n",
            "Batch [26762/28711], Loss: 0.8637\n",
            "Batch [26763/28711], Loss: 0.8603\n",
            "Batch [26764/28711], Loss: 0.8901\n",
            "Batch [26765/28711], Loss: 0.8461\n",
            "Batch [26766/28711], Loss: 0.7458\n",
            "Batch [26767/28711], Loss: 1.2938\n",
            "Batch [26768/28711], Loss: 1.3099\n",
            "Batch [26769/28711], Loss: 0.9650\n",
            "Batch [26770/28711], Loss: 0.8111\n",
            "Batch [26771/28711], Loss: 0.7901\n",
            "Batch [26772/28711], Loss: 0.8591\n",
            "Batch [26773/28711], Loss: 0.7724\n",
            "Batch [26774/28711], Loss: 0.8037\n",
            "Batch [26775/28711], Loss: 1.0534\n",
            "Batch [26776/28711], Loss: 1.0938\n",
            "Batch [26777/28711], Loss: 1.1841\n",
            "Batch [26778/28711], Loss: 0.8775\n",
            "Batch [26779/28711], Loss: 0.7240\n",
            "Batch [26780/28711], Loss: 1.0589\n",
            "Batch [26781/28711], Loss: 0.7257\n",
            "Batch [26782/28711], Loss: 1.2235\n",
            "Batch [26783/28711], Loss: 1.0280\n",
            "Batch [26784/28711], Loss: 0.9003\n",
            "Batch [26785/28711], Loss: 0.6624\n",
            "Batch [26786/28711], Loss: 0.7514\n",
            "Batch [26787/28711], Loss: 0.7151\n",
            "Batch [26788/28711], Loss: 1.0013\n",
            "Batch [26789/28711], Loss: 0.7818\n",
            "Batch [26790/28711], Loss: 0.8371\n",
            "Batch [26791/28711], Loss: 0.9220\n",
            "Batch [26792/28711], Loss: 0.8709\n",
            "Batch [26793/28711], Loss: 0.7480\n",
            "Batch [26794/28711], Loss: 1.2200\n",
            "Batch [26795/28711], Loss: 0.7785\n",
            "Batch [26796/28711], Loss: 1.0414\n",
            "Batch [26797/28711], Loss: 0.7214\n",
            "Batch [26798/28711], Loss: 0.9408\n",
            "Batch [26799/28711], Loss: 1.0757\n",
            "Batch [26800/28711], Loss: 0.5913\n",
            "Batch [26801/28711], Loss: 0.7473\n",
            "Batch [26802/28711], Loss: 0.8794\n",
            "Batch [26803/28711], Loss: 1.2230\n",
            "Batch [26804/28711], Loss: 0.7722\n",
            "Batch [26805/28711], Loss: 0.8897\n",
            "Batch [26806/28711], Loss: 0.8512\n",
            "Batch [26807/28711], Loss: 0.9684\n",
            "Batch [26808/28711], Loss: 1.2227\n",
            "Batch [26809/28711], Loss: 0.9542\n",
            "Batch [26810/28711], Loss: 0.7118\n",
            "Batch [26811/28711], Loss: 1.1830\n",
            "Batch [26812/28711], Loss: 1.3111\n",
            "Batch [26813/28711], Loss: 1.0616\n",
            "Batch [26814/28711], Loss: 0.9789\n",
            "Batch [26815/28711], Loss: 0.7646\n",
            "Batch [26816/28711], Loss: 0.9901\n",
            "Batch [26817/28711], Loss: 1.0326\n",
            "Batch [26818/28711], Loss: 0.9173\n",
            "Batch [26819/28711], Loss: 1.1426\n",
            "Batch [26820/28711], Loss: 0.7989\n",
            "Batch [26821/28711], Loss: 1.0465\n",
            "Batch [26822/28711], Loss: 1.0270\n",
            "Batch [26823/28711], Loss: 0.7946\n",
            "Batch [26824/28711], Loss: 0.8317\n",
            "Batch [26825/28711], Loss: 0.9493\n",
            "Batch [26826/28711], Loss: 0.9981\n",
            "Batch [26827/28711], Loss: 1.0417\n",
            "Batch [26828/28711], Loss: 0.8734\n",
            "Batch [26829/28711], Loss: 1.0271\n",
            "Batch [26830/28711], Loss: 1.0651\n",
            "Batch [26831/28711], Loss: 0.9884\n",
            "Batch [26832/28711], Loss: 1.0531\n",
            "Batch [26833/28711], Loss: 1.1409\n",
            "Batch [26834/28711], Loss: 0.7934\n",
            "Batch [26835/28711], Loss: 1.2435\n",
            "Batch [26836/28711], Loss: 0.8800\n",
            "Batch [26837/28711], Loss: 0.6769\n",
            "Batch [26838/28711], Loss: 0.9752\n",
            "Batch [26839/28711], Loss: 0.9955\n",
            "Batch [26840/28711], Loss: 0.8448\n",
            "Batch [26841/28711], Loss: 0.8246\n",
            "Batch [26842/28711], Loss: 1.0291\n",
            "Batch [26843/28711], Loss: 0.8275\n",
            "Batch [26844/28711], Loss: 1.0708\n",
            "Batch [26845/28711], Loss: 0.8392\n",
            "Batch [26846/28711], Loss: 0.5715\n",
            "Batch [26847/28711], Loss: 1.1731\n",
            "Batch [26848/28711], Loss: 0.9489\n",
            "Batch [26849/28711], Loss: 1.0472\n",
            "Batch [26850/28711], Loss: 1.1172\n",
            "Batch [26851/28711], Loss: 1.0680\n",
            "Batch [26852/28711], Loss: 0.9976\n",
            "Batch [26853/28711], Loss: 0.9130\n",
            "Batch [26854/28711], Loss: 0.9109\n",
            "Batch [26855/28711], Loss: 1.3049\n",
            "Batch [26856/28711], Loss: 0.9186\n",
            "Batch [26857/28711], Loss: 0.6741\n",
            "Batch [26858/28711], Loss: 0.8214\n",
            "Batch [26859/28711], Loss: 0.9863\n",
            "Batch [26860/28711], Loss: 1.0611\n",
            "Batch [26861/28711], Loss: 1.0070\n",
            "Batch [26862/28711], Loss: 0.9766\n",
            "Batch [26863/28711], Loss: 0.8853\n",
            "Batch [26864/28711], Loss: 0.8696\n",
            "Batch [26865/28711], Loss: 0.7709\n",
            "Batch [26866/28711], Loss: 0.9534\n",
            "Batch [26867/28711], Loss: 0.6002\n",
            "Batch [26868/28711], Loss: 1.1227\n",
            "Batch [26869/28711], Loss: 0.6937\n",
            "Batch [26870/28711], Loss: 0.8335\n",
            "Batch [26871/28711], Loss: 0.9185\n",
            "Batch [26872/28711], Loss: 0.7868\n",
            "Batch [26873/28711], Loss: 1.0670\n",
            "Batch [26874/28711], Loss: 0.9875\n",
            "Batch [26875/28711], Loss: 1.0047\n",
            "Batch [26876/28711], Loss: 1.0973\n",
            "Batch [26877/28711], Loss: 0.9177\n",
            "Batch [26878/28711], Loss: 0.7253\n",
            "Batch [26879/28711], Loss: 0.8428\n",
            "Batch [26880/28711], Loss: 1.0375\n",
            "Batch [26881/28711], Loss: 0.8680\n",
            "Batch [26882/28711], Loss: 0.9886\n",
            "Batch [26883/28711], Loss: 0.8227\n",
            "Batch [26884/28711], Loss: 0.8927\n",
            "Batch [26885/28711], Loss: 0.9637\n",
            "Batch [26886/28711], Loss: 1.0535\n",
            "Batch [26887/28711], Loss: 1.0708\n",
            "Batch [26888/28711], Loss: 0.6052\n",
            "Batch [26889/28711], Loss: 0.9958\n",
            "Batch [26890/28711], Loss: 0.8624\n",
            "Batch [26891/28711], Loss: 0.8336\n",
            "Batch [26892/28711], Loss: 0.7679\n",
            "Batch [26893/28711], Loss: 0.8234\n",
            "Batch [26894/28711], Loss: 0.7662\n",
            "Batch [26895/28711], Loss: 1.2182\n",
            "Batch [26896/28711], Loss: 1.0638\n",
            "Batch [26897/28711], Loss: 0.9015\n",
            "Batch [26898/28711], Loss: 0.7886\n",
            "Batch [26899/28711], Loss: 1.0160\n",
            "Batch [26900/28711], Loss: 1.0446\n",
            "Batch [26901/28711], Loss: 1.0819\n",
            "Batch [26902/28711], Loss: 1.0484\n",
            "Batch [26903/28711], Loss: 0.9319\n",
            "Batch [26904/28711], Loss: 0.9060\n",
            "Batch [26905/28711], Loss: 0.7458\n",
            "Batch [26906/28711], Loss: 0.7840\n",
            "Batch [26907/28711], Loss: 0.7426\n",
            "Batch [26908/28711], Loss: 1.1625\n",
            "Batch [26909/28711], Loss: 0.7025\n",
            "Batch [26910/28711], Loss: 1.0090\n",
            "Batch [26911/28711], Loss: 0.9451\n",
            "Batch [26912/28711], Loss: 1.0231\n",
            "Batch [26913/28711], Loss: 0.5511\n",
            "Batch [26914/28711], Loss: 0.8862\n",
            "Batch [26915/28711], Loss: 0.9645\n",
            "Batch [26916/28711], Loss: 1.1104\n",
            "Batch [26917/28711], Loss: 1.1860\n",
            "Batch [26918/28711], Loss: 0.6617\n",
            "Batch [26919/28711], Loss: 0.7820\n",
            "Batch [26920/28711], Loss: 0.7068\n",
            "Batch [26921/28711], Loss: 0.9640\n",
            "Batch [26922/28711], Loss: 0.8201\n",
            "Batch [26923/28711], Loss: 1.3049\n",
            "Batch [26924/28711], Loss: 1.0139\n",
            "Batch [26925/28711], Loss: 0.9695\n",
            "Batch [26926/28711], Loss: 1.0115\n",
            "Batch [26927/28711], Loss: 0.7980\n",
            "Batch [26928/28711], Loss: 0.5295\n",
            "Batch [26929/28711], Loss: 1.0497\n",
            "Batch [26930/28711], Loss: 0.8486\n",
            "Batch [26931/28711], Loss: 0.9592\n",
            "Batch [26932/28711], Loss: 1.1065\n",
            "Batch [26933/28711], Loss: 0.8125\n",
            "Batch [26934/28711], Loss: 0.8913\n",
            "Batch [26935/28711], Loss: 0.8430\n",
            "Batch [26936/28711], Loss: 0.7531\n",
            "Batch [26937/28711], Loss: 0.7736\n",
            "Batch [26938/28711], Loss: 0.9123\n",
            "Batch [26939/28711], Loss: 0.7645\n",
            "Batch [26940/28711], Loss: 0.9877\n",
            "Batch [26941/28711], Loss: 0.7372\n",
            "Batch [26942/28711], Loss: 0.8099\n",
            "Batch [26943/28711], Loss: 1.1254\n",
            "Batch [26944/28711], Loss: 0.7482\n",
            "Batch [26945/28711], Loss: 1.0852\n",
            "Batch [26946/28711], Loss: 0.9044\n",
            "Batch [26947/28711], Loss: 0.8088\n",
            "Batch [26948/28711], Loss: 0.9115\n",
            "Batch [26949/28711], Loss: 1.1276\n",
            "Batch [26950/28711], Loss: 0.8897\n",
            "Batch [26951/28711], Loss: 0.7826\n",
            "Batch [26952/28711], Loss: 0.7935\n",
            "Batch [26953/28711], Loss: 1.0787\n",
            "Batch [26954/28711], Loss: 0.7282\n",
            "Batch [26955/28711], Loss: 0.8956\n",
            "Batch [26956/28711], Loss: 0.5866\n",
            "Batch [26957/28711], Loss: 0.8649\n",
            "Batch [26958/28711], Loss: 0.7767\n",
            "Batch [26959/28711], Loss: 0.9225\n",
            "Batch [26960/28711], Loss: 1.0370\n",
            "Batch [26961/28711], Loss: 0.6788\n",
            "Batch [26962/28711], Loss: 0.8381\n",
            "Batch [26963/28711], Loss: 0.8202\n",
            "Batch [26964/28711], Loss: 1.0849\n",
            "Batch [26965/28711], Loss: 0.9096\n",
            "Batch [26966/28711], Loss: 0.8101\n",
            "Batch [26967/28711], Loss: 0.7299\n",
            "Batch [26968/28711], Loss: 0.9377\n",
            "Batch [26969/28711], Loss: 0.9708\n",
            "Batch [26970/28711], Loss: 0.7649\n",
            "Batch [26971/28711], Loss: 0.9550\n",
            "Batch [26972/28711], Loss: 0.9306\n",
            "Batch [26973/28711], Loss: 0.9650\n",
            "Batch [26974/28711], Loss: 0.9278\n",
            "Batch [26975/28711], Loss: 0.7120\n",
            "Batch [26976/28711], Loss: 0.9085\n",
            "Batch [26977/28711], Loss: 0.8551\n",
            "Batch [26978/28711], Loss: 0.9297\n",
            "Batch [26979/28711], Loss: 0.9502\n",
            "Batch [26980/28711], Loss: 0.9132\n",
            "Batch [26981/28711], Loss: 0.9709\n",
            "Batch [26982/28711], Loss: 0.5420\n",
            "Batch [26983/28711], Loss: 1.0235\n",
            "Batch [26984/28711], Loss: 0.9499\n",
            "Batch [26985/28711], Loss: 0.7646\n",
            "Batch [26986/28711], Loss: 0.9246\n",
            "Batch [26987/28711], Loss: 0.8024\n",
            "Batch [26988/28711], Loss: 0.9923\n",
            "Batch [26989/28711], Loss: 0.7859\n",
            "Batch [26990/28711], Loss: 0.9353\n",
            "Batch [26991/28711], Loss: 0.8014\n",
            "Batch [26992/28711], Loss: 0.6497\n",
            "Batch [26993/28711], Loss: 1.2004\n",
            "Batch [26994/28711], Loss: 0.6611\n",
            "Batch [26995/28711], Loss: 0.7916\n",
            "Batch [26996/28711], Loss: 0.6270\n",
            "Batch [26997/28711], Loss: 0.9223\n",
            "Batch [26998/28711], Loss: 0.6775\n",
            "Batch [26999/28711], Loss: 0.8067\n",
            "Batch [27000/28711], Loss: 0.7960\n",
            "Batch [27001/28711], Loss: 0.8914\n",
            "Batch [27002/28711], Loss: 0.8968\n",
            "Batch [27003/28711], Loss: 0.7929\n",
            "Batch [27004/28711], Loss: 1.0493\n",
            "Batch [27005/28711], Loss: 0.7372\n",
            "Batch [27006/28711], Loss: 0.8455\n",
            "Batch [27007/28711], Loss: 0.8863\n",
            "Batch [27008/28711], Loss: 0.6708\n",
            "Batch [27009/28711], Loss: 0.5671\n",
            "Batch [27010/28711], Loss: 1.0370\n",
            "Batch [27011/28711], Loss: 0.7224\n",
            "Batch [27012/28711], Loss: 1.1038\n",
            "Batch [27013/28711], Loss: 0.6518\n",
            "Batch [27014/28711], Loss: 1.0626\n",
            "Batch [27015/28711], Loss: 0.8686\n",
            "Batch [27016/28711], Loss: 0.8622\n",
            "Batch [27017/28711], Loss: 1.3230\n",
            "Batch [27018/28711], Loss: 1.1307\n",
            "Batch [27019/28711], Loss: 0.7167\n",
            "Batch [27020/28711], Loss: 0.8340\n",
            "Batch [27021/28711], Loss: 0.9731\n",
            "Batch [27022/28711], Loss: 0.8830\n",
            "Batch [27023/28711], Loss: 0.6362\n",
            "Batch [27024/28711], Loss: 0.8250\n",
            "Batch [27025/28711], Loss: 1.0693\n",
            "Batch [27026/28711], Loss: 0.8334\n",
            "Batch [27027/28711], Loss: 0.9692\n",
            "Batch [27028/28711], Loss: 1.1785\n",
            "Batch [27029/28711], Loss: 0.8546\n",
            "Batch [27030/28711], Loss: 0.8826\n",
            "Batch [27031/28711], Loss: 0.7322\n",
            "Batch [27032/28711], Loss: 0.6824\n",
            "Batch [27033/28711], Loss: 0.6983\n",
            "Batch [27034/28711], Loss: 0.6763\n",
            "Batch [27035/28711], Loss: 0.9991\n",
            "Batch [27036/28711], Loss: 0.9128\n",
            "Batch [27037/28711], Loss: 1.1305\n",
            "Batch [27038/28711], Loss: 0.9341\n",
            "Batch [27039/28711], Loss: 0.7843\n",
            "Batch [27040/28711], Loss: 1.1582\n",
            "Batch [27041/28711], Loss: 0.8807\n",
            "Batch [27042/28711], Loss: 0.8317\n",
            "Batch [27043/28711], Loss: 0.9464\n",
            "Batch [27044/28711], Loss: 0.8344\n",
            "Batch [27045/28711], Loss: 0.7753\n",
            "Batch [27046/28711], Loss: 0.7717\n",
            "Batch [27047/28711], Loss: 0.8314\n",
            "Batch [27048/28711], Loss: 0.8061\n",
            "Batch [27049/28711], Loss: 1.0271\n",
            "Batch [27050/28711], Loss: 0.9910\n",
            "Batch [27051/28711], Loss: 1.1027\n",
            "Batch [27052/28711], Loss: 0.7553\n",
            "Batch [27053/28711], Loss: 0.9922\n",
            "Batch [27054/28711], Loss: 0.9123\n",
            "Batch [27055/28711], Loss: 0.9029\n",
            "Batch [27056/28711], Loss: 0.9055\n",
            "Batch [27057/28711], Loss: 0.7759\n",
            "Batch [27058/28711], Loss: 1.0709\n",
            "Batch [27059/28711], Loss: 0.9790\n",
            "Batch [27060/28711], Loss: 1.1356\n",
            "Batch [27061/28711], Loss: 1.1712\n",
            "Batch [27062/28711], Loss: 1.0572\n",
            "Batch [27063/28711], Loss: 0.7120\n",
            "Batch [27064/28711], Loss: 0.6234\n",
            "Batch [27065/28711], Loss: 0.7547\n",
            "Batch [27066/28711], Loss: 1.0743\n",
            "Batch [27067/28711], Loss: 0.8959\n",
            "Batch [27068/28711], Loss: 0.7668\n",
            "Batch [27069/28711], Loss: 1.1211\n",
            "Batch [27070/28711], Loss: 0.8914\n",
            "Batch [27071/28711], Loss: 0.8278\n",
            "Batch [27072/28711], Loss: 0.7736\n",
            "Batch [27073/28711], Loss: 0.8591\n",
            "Batch [27074/28711], Loss: 0.8503\n",
            "Batch [27075/28711], Loss: 0.7026\n",
            "Batch [27076/28711], Loss: 0.6979\n",
            "Batch [27077/28711], Loss: 0.7361\n",
            "Batch [27078/28711], Loss: 0.8937\n",
            "Batch [27079/28711], Loss: 0.6554\n",
            "Batch [27080/28711], Loss: 0.9020\n",
            "Batch [27081/28711], Loss: 1.2271\n",
            "Batch [27082/28711], Loss: 0.9073\n",
            "Batch [27083/28711], Loss: 1.0292\n",
            "Batch [27084/28711], Loss: 0.8989\n",
            "Batch [27085/28711], Loss: 0.7990\n",
            "Batch [27086/28711], Loss: 0.6594\n",
            "Batch [27087/28711], Loss: 1.0233\n",
            "Batch [27088/28711], Loss: 1.0410\n",
            "Batch [27089/28711], Loss: 0.7721\n",
            "Batch [27090/28711], Loss: 0.8013\n",
            "Batch [27091/28711], Loss: 0.8646\n",
            "Batch [27092/28711], Loss: 0.8120\n",
            "Batch [27093/28711], Loss: 1.1889\n",
            "Batch [27094/28711], Loss: 0.8966\n",
            "Batch [27095/28711], Loss: 0.8481\n",
            "Batch [27096/28711], Loss: 0.7361\n",
            "Batch [27097/28711], Loss: 1.1729\n",
            "Batch [27098/28711], Loss: 0.9018\n",
            "Batch [27099/28711], Loss: 0.8248\n",
            "Batch [27100/28711], Loss: 0.9063\n",
            "Batch [27101/28711], Loss: 0.9473\n",
            "Batch [27102/28711], Loss: 0.9715\n",
            "Batch [27103/28711], Loss: 0.9234\n",
            "Batch [27104/28711], Loss: 0.8330\n",
            "Batch [27105/28711], Loss: 0.5176\n",
            "Batch [27106/28711], Loss: 0.9197\n",
            "Batch [27107/28711], Loss: 0.8174\n",
            "Batch [27108/28711], Loss: 0.7858\n",
            "Batch [27109/28711], Loss: 0.9730\n",
            "Batch [27110/28711], Loss: 0.9997\n",
            "Batch [27111/28711], Loss: 1.3809\n",
            "Batch [27112/28711], Loss: 1.1058\n",
            "Batch [27113/28711], Loss: 1.1070\n",
            "Batch [27114/28711], Loss: 0.8094\n",
            "Batch [27115/28711], Loss: 0.8391\n",
            "Batch [27116/28711], Loss: 0.6919\n",
            "Batch [27117/28711], Loss: 0.9114\n",
            "Batch [27118/28711], Loss: 0.8987\n",
            "Batch [27119/28711], Loss: 0.8893\n",
            "Batch [27120/28711], Loss: 1.1002\n",
            "Batch [27121/28711], Loss: 0.9156\n",
            "Batch [27122/28711], Loss: 0.7396\n",
            "Batch [27123/28711], Loss: 0.8197\n",
            "Batch [27124/28711], Loss: 0.9307\n",
            "Batch [27125/28711], Loss: 1.0966\n",
            "Batch [27126/28711], Loss: 1.1119\n",
            "Batch [27127/28711], Loss: 0.8982\n",
            "Batch [27128/28711], Loss: 1.1549\n",
            "Batch [27129/28711], Loss: 0.7328\n",
            "Batch [27130/28711], Loss: 0.9054\n",
            "Batch [27131/28711], Loss: 0.9456\n",
            "Batch [27132/28711], Loss: 0.8337\n",
            "Batch [27133/28711], Loss: 0.6053\n",
            "Batch [27134/28711], Loss: 0.8851\n",
            "Batch [27135/28711], Loss: 0.7835\n",
            "Batch [27136/28711], Loss: 1.0735\n",
            "Batch [27137/28711], Loss: 0.8898\n",
            "Batch [27138/28711], Loss: 0.9164\n",
            "Batch [27139/28711], Loss: 0.8220\n",
            "Batch [27140/28711], Loss: 0.9625\n",
            "Batch [27141/28711], Loss: 1.2898\n",
            "Batch [27142/28711], Loss: 1.0770\n",
            "Batch [27143/28711], Loss: 1.1904\n",
            "Batch [27144/28711], Loss: 0.9860\n",
            "Batch [27145/28711], Loss: 1.1656\n",
            "Batch [27146/28711], Loss: 1.3588\n",
            "Batch [27147/28711], Loss: 0.6093\n",
            "Batch [27148/28711], Loss: 1.0056\n",
            "Batch [27149/28711], Loss: 0.8692\n",
            "Batch [27150/28711], Loss: 1.1041\n",
            "Batch [27151/28711], Loss: 1.1513\n",
            "Batch [27152/28711], Loss: 0.7941\n",
            "Batch [27153/28711], Loss: 0.7733\n",
            "Batch [27154/28711], Loss: 0.7210\n",
            "Batch [27155/28711], Loss: 0.8466\n",
            "Batch [27156/28711], Loss: 0.8361\n",
            "Batch [27157/28711], Loss: 0.5816\n",
            "Batch [27158/28711], Loss: 0.6090\n",
            "Batch [27159/28711], Loss: 1.1018\n",
            "Batch [27160/28711], Loss: 1.0780\n",
            "Batch [27161/28711], Loss: 0.9815\n",
            "Batch [27162/28711], Loss: 1.0335\n",
            "Batch [27163/28711], Loss: 0.8277\n",
            "Batch [27164/28711], Loss: 0.8453\n",
            "Batch [27165/28711], Loss: 0.7565\n",
            "Batch [27166/28711], Loss: 0.8146\n",
            "Batch [27167/28711], Loss: 1.0171\n",
            "Batch [27168/28711], Loss: 1.0443\n",
            "Batch [27169/28711], Loss: 0.8419\n",
            "Batch [27170/28711], Loss: 1.1485\n",
            "Batch [27171/28711], Loss: 1.0815\n",
            "Batch [27172/28711], Loss: 0.9636\n",
            "Batch [27173/28711], Loss: 0.9669\n",
            "Batch [27174/28711], Loss: 0.9355\n",
            "Batch [27175/28711], Loss: 0.8173\n",
            "Batch [27176/28711], Loss: 0.7533\n",
            "Batch [27177/28711], Loss: 0.8736\n",
            "Batch [27178/28711], Loss: 0.8303\n",
            "Batch [27179/28711], Loss: 0.7652\n",
            "Batch [27180/28711], Loss: 0.8783\n",
            "Batch [27181/28711], Loss: 0.8390\n",
            "Batch [27182/28711], Loss: 0.7090\n",
            "Batch [27183/28711], Loss: 0.7920\n",
            "Batch [27184/28711], Loss: 0.7854\n",
            "Batch [27185/28711], Loss: 0.9902\n",
            "Batch [27186/28711], Loss: 0.5972\n",
            "Batch [27187/28711], Loss: 1.1299\n",
            "Batch [27188/28711], Loss: 0.7629\n",
            "Batch [27189/28711], Loss: 0.7808\n",
            "Batch [27190/28711], Loss: 0.9937\n",
            "Batch [27191/28711], Loss: 0.8110\n",
            "Batch [27192/28711], Loss: 0.9018\n",
            "Batch [27193/28711], Loss: 0.8243\n",
            "Batch [27194/28711], Loss: 0.9416\n",
            "Batch [27195/28711], Loss: 1.1298\n",
            "Batch [27196/28711], Loss: 0.8404\n",
            "Batch [27197/28711], Loss: 0.9208\n",
            "Batch [27198/28711], Loss: 1.0953\n",
            "Batch [27199/28711], Loss: 0.8390\n",
            "Batch [27200/28711], Loss: 0.8208\n",
            "Batch [27201/28711], Loss: 0.7458\n",
            "Batch [27202/28711], Loss: 0.7641\n",
            "Batch [27203/28711], Loss: 0.7541\n",
            "Batch [27204/28711], Loss: 1.1037\n",
            "Batch [27205/28711], Loss: 0.6857\n",
            "Batch [27206/28711], Loss: 0.7948\n",
            "Batch [27207/28711], Loss: 0.8451\n",
            "Batch [27208/28711], Loss: 0.6634\n",
            "Batch [27209/28711], Loss: 0.8084\n",
            "Batch [27210/28711], Loss: 0.9826\n",
            "Batch [27211/28711], Loss: 0.8950\n",
            "Batch [27212/28711], Loss: 0.9796\n",
            "Batch [27213/28711], Loss: 1.0592\n",
            "Batch [27214/28711], Loss: 1.1635\n",
            "Batch [27215/28711], Loss: 1.0042\n",
            "Batch [27216/28711], Loss: 0.9391\n",
            "Batch [27217/28711], Loss: 0.8927\n",
            "Batch [27218/28711], Loss: 0.9939\n",
            "Batch [27219/28711], Loss: 0.9286\n",
            "Batch [27220/28711], Loss: 0.9934\n",
            "Batch [27221/28711], Loss: 1.2554\n",
            "Batch [27222/28711], Loss: 0.9291\n",
            "Batch [27223/28711], Loss: 0.9257\n",
            "Batch [27224/28711], Loss: 0.8229\n",
            "Batch [27225/28711], Loss: 0.6789\n",
            "Batch [27226/28711], Loss: 0.8864\n",
            "Batch [27227/28711], Loss: 0.9827\n",
            "Batch [27228/28711], Loss: 0.8828\n",
            "Batch [27229/28711], Loss: 0.7984\n",
            "Batch [27230/28711], Loss: 1.2174\n",
            "Batch [27231/28711], Loss: 0.9434\n",
            "Batch [27232/28711], Loss: 1.0176\n",
            "Batch [27233/28711], Loss: 1.0218\n",
            "Batch [27234/28711], Loss: 0.9675\n",
            "Batch [27235/28711], Loss: 0.7972\n",
            "Batch [27236/28711], Loss: 0.8947\n",
            "Batch [27237/28711], Loss: 0.8865\n",
            "Batch [27238/28711], Loss: 0.8542\n",
            "Batch [27239/28711], Loss: 0.8757\n",
            "Batch [27240/28711], Loss: 0.8856\n",
            "Batch [27241/28711], Loss: 0.8580\n",
            "Batch [27242/28711], Loss: 0.8190\n",
            "Batch [27243/28711], Loss: 0.7383\n",
            "Batch [27244/28711], Loss: 1.0434\n",
            "Batch [27245/28711], Loss: 0.7000\n",
            "Batch [27246/28711], Loss: 0.9444\n",
            "Batch [27247/28711], Loss: 0.8878\n",
            "Batch [27248/28711], Loss: 1.1206\n",
            "Batch [27249/28711], Loss: 1.1175\n",
            "Batch [27250/28711], Loss: 0.9157\n",
            "Batch [27251/28711], Loss: 0.8575\n",
            "Batch [27252/28711], Loss: 0.9095\n",
            "Batch [27253/28711], Loss: 0.9728\n",
            "Batch [27254/28711], Loss: 0.9303\n",
            "Batch [27255/28711], Loss: 0.9582\n",
            "Batch [27256/28711], Loss: 0.9046\n",
            "Batch [27257/28711], Loss: 0.8120\n",
            "Batch [27258/28711], Loss: 0.8334\n",
            "Batch [27259/28711], Loss: 0.9659\n",
            "Batch [27260/28711], Loss: 0.6749\n",
            "Batch [27261/28711], Loss: 1.0388\n",
            "Batch [27262/28711], Loss: 0.9409\n",
            "Batch [27263/28711], Loss: 0.9665\n",
            "Batch [27264/28711], Loss: 0.8406\n",
            "Batch [27265/28711], Loss: 0.9916\n",
            "Batch [27266/28711], Loss: 0.7973\n",
            "Batch [27267/28711], Loss: 0.8410\n",
            "Batch [27268/28711], Loss: 1.0578\n",
            "Batch [27269/28711], Loss: 0.9279\n",
            "Batch [27270/28711], Loss: 0.8594\n",
            "Batch [27271/28711], Loss: 0.8613\n",
            "Batch [27272/28711], Loss: 0.7610\n",
            "Batch [27273/28711], Loss: 0.7645\n",
            "Batch [27274/28711], Loss: 0.5684\n",
            "Batch [27275/28711], Loss: 1.0505\n",
            "Batch [27276/28711], Loss: 0.8980\n",
            "Batch [27277/28711], Loss: 0.8099\n",
            "Batch [27278/28711], Loss: 0.7718\n",
            "Batch [27279/28711], Loss: 1.0440\n",
            "Batch [27280/28711], Loss: 0.7291\n",
            "Batch [27281/28711], Loss: 0.9492\n",
            "Batch [27282/28711], Loss: 0.9187\n",
            "Batch [27283/28711], Loss: 0.9918\n",
            "Batch [27284/28711], Loss: 0.8902\n",
            "Batch [27285/28711], Loss: 0.6245\n",
            "Batch [27286/28711], Loss: 0.8546\n",
            "Batch [27287/28711], Loss: 0.8809\n",
            "Batch [27288/28711], Loss: 1.1895\n",
            "Batch [27289/28711], Loss: 0.7672\n",
            "Batch [27290/28711], Loss: 0.9867\n",
            "Batch [27291/28711], Loss: 1.0426\n",
            "Batch [27292/28711], Loss: 0.8271\n",
            "Batch [27293/28711], Loss: 0.8139\n",
            "Batch [27294/28711], Loss: 0.7014\n",
            "Batch [27295/28711], Loss: 0.7875\n",
            "Batch [27296/28711], Loss: 0.9321\n",
            "Batch [27297/28711], Loss: 0.7022\n",
            "Batch [27298/28711], Loss: 0.7757\n",
            "Batch [27299/28711], Loss: 0.8404\n",
            "Batch [27300/28711], Loss: 1.0794\n",
            "Batch [27301/28711], Loss: 0.6659\n",
            "Batch [27302/28711], Loss: 0.9707\n",
            "Batch [27303/28711], Loss: 0.7204\n",
            "Batch [27304/28711], Loss: 0.8741\n",
            "Batch [27305/28711], Loss: 0.8826\n",
            "Batch [27306/28711], Loss: 0.7430\n",
            "Batch [27307/28711], Loss: 0.9099\n",
            "Batch [27308/28711], Loss: 0.8194\n",
            "Batch [27309/28711], Loss: 1.0546\n",
            "Batch [27310/28711], Loss: 0.8091\n",
            "Batch [27311/28711], Loss: 0.7847\n",
            "Batch [27312/28711], Loss: 0.8988\n",
            "Batch [27313/28711], Loss: 1.2194\n",
            "Batch [27314/28711], Loss: 1.0851\n",
            "Batch [27315/28711], Loss: 0.7887\n",
            "Batch [27316/28711], Loss: 1.0914\n",
            "Batch [27317/28711], Loss: 1.0063\n",
            "Batch [27318/28711], Loss: 0.5999\n",
            "Batch [27319/28711], Loss: 0.7208\n",
            "Batch [27320/28711], Loss: 0.8378\n",
            "Batch [27321/28711], Loss: 1.1132\n",
            "Batch [27322/28711], Loss: 0.8840\n",
            "Batch [27323/28711], Loss: 0.9038\n",
            "Batch [27324/28711], Loss: 0.9245\n",
            "Batch [27325/28711], Loss: 0.8994\n",
            "Batch [27326/28711], Loss: 0.6567\n",
            "Batch [27327/28711], Loss: 1.2171\n",
            "Batch [27328/28711], Loss: 0.9535\n",
            "Batch [27329/28711], Loss: 0.7760\n",
            "Batch [27330/28711], Loss: 0.8240\n",
            "Batch [27331/28711], Loss: 0.9280\n",
            "Batch [27332/28711], Loss: 1.0051\n",
            "Batch [27333/28711], Loss: 1.0180\n",
            "Batch [27334/28711], Loss: 0.8718\n",
            "Batch [27335/28711], Loss: 0.8907\n",
            "Batch [27336/28711], Loss: 0.8422\n",
            "Batch [27337/28711], Loss: 0.7966\n",
            "Batch [27338/28711], Loss: 0.9727\n",
            "Batch [27339/28711], Loss: 0.7751\n",
            "Batch [27340/28711], Loss: 1.1507\n",
            "Batch [27341/28711], Loss: 0.8065\n",
            "Batch [27342/28711], Loss: 0.8201\n",
            "Batch [27343/28711], Loss: 0.7550\n",
            "Batch [27344/28711], Loss: 0.9549\n",
            "Batch [27345/28711], Loss: 1.3067\n",
            "Batch [27346/28711], Loss: 0.6438\n",
            "Batch [27347/28711], Loss: 0.9523\n",
            "Batch [27348/28711], Loss: 0.8587\n",
            "Batch [27349/28711], Loss: 0.9524\n",
            "Batch [27350/28711], Loss: 1.3762\n",
            "Batch [27351/28711], Loss: 0.9554\n",
            "Batch [27352/28711], Loss: 1.1061\n",
            "Batch [27353/28711], Loss: 0.8131\n",
            "Batch [27354/28711], Loss: 0.8872\n",
            "Batch [27355/28711], Loss: 0.9812\n",
            "Batch [27356/28711], Loss: 1.0194\n",
            "Batch [27357/28711], Loss: 0.7260\n",
            "Batch [27358/28711], Loss: 1.2210\n",
            "Batch [27359/28711], Loss: 0.8561\n",
            "Batch [27360/28711], Loss: 0.7834\n",
            "Batch [27361/28711], Loss: 0.9502\n",
            "Batch [27362/28711], Loss: 1.0673\n",
            "Batch [27363/28711], Loss: 0.9330\n",
            "Batch [27364/28711], Loss: 1.0478\n",
            "Batch [27365/28711], Loss: 0.8105\n",
            "Batch [27366/28711], Loss: 0.9356\n",
            "Batch [27367/28711], Loss: 0.7759\n",
            "Batch [27368/28711], Loss: 1.2639\n",
            "Batch [27369/28711], Loss: 1.0632\n",
            "Batch [27370/28711], Loss: 0.7698\n",
            "Batch [27371/28711], Loss: 1.0627\n",
            "Batch [27372/28711], Loss: 1.0323\n",
            "Batch [27373/28711], Loss: 0.8655\n",
            "Batch [27374/28711], Loss: 0.9585\n",
            "Batch [27375/28711], Loss: 0.7847\n",
            "Batch [27376/28711], Loss: 1.2121\n",
            "Batch [27377/28711], Loss: 0.7291\n",
            "Batch [27378/28711], Loss: 1.0042\n",
            "Batch [27379/28711], Loss: 0.9086\n",
            "Batch [27380/28711], Loss: 0.8730\n",
            "Batch [27381/28711], Loss: 0.8997\n",
            "Batch [27382/28711], Loss: 0.9627\n",
            "Batch [27383/28711], Loss: 1.0960\n",
            "Batch [27384/28711], Loss: 1.1205\n",
            "Batch [27385/28711], Loss: 0.9440\n",
            "Batch [27386/28711], Loss: 0.8055\n",
            "Batch [27387/28711], Loss: 0.7640\n",
            "Batch [27388/28711], Loss: 0.7426\n",
            "Batch [27389/28711], Loss: 0.8771\n",
            "Batch [27390/28711], Loss: 0.8566\n",
            "Batch [27391/28711], Loss: 0.8592\n",
            "Batch [27392/28711], Loss: 0.9367\n",
            "Batch [27393/28711], Loss: 0.6879\n",
            "Batch [27394/28711], Loss: 1.1333\n",
            "Batch [27395/28711], Loss: 1.0111\n",
            "Batch [27396/28711], Loss: 0.9824\n",
            "Batch [27397/28711], Loss: 1.2112\n",
            "Batch [27398/28711], Loss: 0.7710\n",
            "Batch [27399/28711], Loss: 0.9199\n",
            "Batch [27400/28711], Loss: 0.6808\n",
            "Batch [27401/28711], Loss: 1.0838\n",
            "Batch [27402/28711], Loss: 0.8483\n",
            "Batch [27403/28711], Loss: 0.7343\n",
            "Batch [27404/28711], Loss: 0.7701\n",
            "Batch [27405/28711], Loss: 1.0312\n",
            "Batch [27406/28711], Loss: 0.7895\n",
            "Batch [27407/28711], Loss: 1.0157\n",
            "Batch [27408/28711], Loss: 0.8851\n",
            "Batch [27409/28711], Loss: 0.8762\n",
            "Batch [27410/28711], Loss: 0.7443\n",
            "Batch [27411/28711], Loss: 1.1152\n",
            "Batch [27412/28711], Loss: 0.9605\n",
            "Batch [27413/28711], Loss: 1.0874\n",
            "Batch [27414/28711], Loss: 0.9831\n",
            "Batch [27415/28711], Loss: 1.1447\n",
            "Batch [27416/28711], Loss: 0.9105\n",
            "Batch [27417/28711], Loss: 0.9929\n",
            "Batch [27418/28711], Loss: 0.9773\n",
            "Batch [27419/28711], Loss: 0.9851\n",
            "Batch [27420/28711], Loss: 0.8945\n",
            "Batch [27421/28711], Loss: 1.1374\n",
            "Batch [27422/28711], Loss: 0.7917\n",
            "Batch [27423/28711], Loss: 0.9760\n",
            "Batch [27424/28711], Loss: 1.1406\n",
            "Batch [27425/28711], Loss: 0.7988\n",
            "Batch [27426/28711], Loss: 0.7643\n",
            "Batch [27427/28711], Loss: 0.9936\n",
            "Batch [27428/28711], Loss: 0.6789\n",
            "Batch [27429/28711], Loss: 0.9818\n",
            "Batch [27430/28711], Loss: 0.6871\n",
            "Batch [27431/28711], Loss: 0.9498\n",
            "Batch [27432/28711], Loss: 0.8333\n",
            "Batch [27433/28711], Loss: 1.0559\n",
            "Batch [27434/28711], Loss: 0.8122\n",
            "Batch [27435/28711], Loss: 0.9399\n",
            "Batch [27436/28711], Loss: 0.7928\n",
            "Batch [27437/28711], Loss: 0.8717\n",
            "Batch [27438/28711], Loss: 0.8781\n",
            "Batch [27439/28711], Loss: 0.9005\n",
            "Batch [27440/28711], Loss: 0.9177\n",
            "Batch [27441/28711], Loss: 0.9122\n",
            "Batch [27442/28711], Loss: 0.8798\n",
            "Batch [27443/28711], Loss: 1.0132\n",
            "Batch [27444/28711], Loss: 0.9302\n",
            "Batch [27445/28711], Loss: 0.8765\n",
            "Batch [27446/28711], Loss: 0.8680\n",
            "Batch [27447/28711], Loss: 0.9680\n",
            "Batch [27448/28711], Loss: 0.9996\n",
            "Batch [27449/28711], Loss: 0.6771\n",
            "Batch [27450/28711], Loss: 0.7975\n",
            "Batch [27451/28711], Loss: 0.9601\n",
            "Batch [27452/28711], Loss: 1.1738\n",
            "Batch [27453/28711], Loss: 0.9921\n",
            "Batch [27454/28711], Loss: 0.8483\n",
            "Batch [27455/28711], Loss: 0.7740\n",
            "Batch [27456/28711], Loss: 0.7293\n",
            "Batch [27457/28711], Loss: 1.1076\n",
            "Batch [27458/28711], Loss: 0.6351\n",
            "Batch [27459/28711], Loss: 0.9501\n",
            "Batch [27460/28711], Loss: 0.9319\n",
            "Batch [27461/28711], Loss: 1.0672\n",
            "Batch [27462/28711], Loss: 0.9331\n",
            "Batch [27463/28711], Loss: 1.0467\n",
            "Batch [27464/28711], Loss: 0.8330\n",
            "Batch [27465/28711], Loss: 1.0172\n",
            "Batch [27466/28711], Loss: 0.8225\n",
            "Batch [27467/28711], Loss: 0.7990\n",
            "Batch [27468/28711], Loss: 0.9090\n",
            "Batch [27469/28711], Loss: 0.7979\n",
            "Batch [27470/28711], Loss: 0.9982\n",
            "Batch [27471/28711], Loss: 1.0929\n",
            "Batch [27472/28711], Loss: 0.9222\n",
            "Batch [27473/28711], Loss: 1.1946\n",
            "Batch [27474/28711], Loss: 0.8529\n",
            "Batch [27475/28711], Loss: 0.9094\n",
            "Batch [27476/28711], Loss: 0.8828\n",
            "Batch [27477/28711], Loss: 0.9678\n",
            "Batch [27478/28711], Loss: 1.1826\n",
            "Batch [27479/28711], Loss: 1.0541\n",
            "Batch [27480/28711], Loss: 0.7891\n",
            "Batch [27481/28711], Loss: 0.9670\n",
            "Batch [27482/28711], Loss: 1.1954\n",
            "Batch [27483/28711], Loss: 0.7098\n",
            "Batch [27484/28711], Loss: 0.7797\n",
            "Batch [27485/28711], Loss: 0.7442\n",
            "Batch [27486/28711], Loss: 0.8337\n",
            "Batch [27487/28711], Loss: 1.0339\n",
            "Batch [27488/28711], Loss: 0.7319\n",
            "Batch [27489/28711], Loss: 0.8450\n",
            "Batch [27490/28711], Loss: 0.9041\n",
            "Batch [27491/28711], Loss: 1.0620\n",
            "Batch [27492/28711], Loss: 0.8418\n",
            "Batch [27493/28711], Loss: 0.8463\n",
            "Batch [27494/28711], Loss: 0.9433\n",
            "Batch [27495/28711], Loss: 0.7996\n",
            "Batch [27496/28711], Loss: 1.0712\n",
            "Batch [27497/28711], Loss: 0.9999\n",
            "Batch [27498/28711], Loss: 0.9064\n",
            "Batch [27499/28711], Loss: 1.2792\n",
            "Batch [27500/28711], Loss: 1.1348\n",
            "Batch [27501/28711], Loss: 0.9518\n",
            "Batch [27502/28711], Loss: 0.6260\n",
            "Batch [27503/28711], Loss: 0.7915\n",
            "Batch [27504/28711], Loss: 0.7942\n",
            "Batch [27505/28711], Loss: 0.7507\n",
            "Batch [27506/28711], Loss: 1.0162\n",
            "Batch [27507/28711], Loss: 1.0058\n",
            "Batch [27508/28711], Loss: 1.0782\n",
            "Batch [27509/28711], Loss: 0.7283\n",
            "Batch [27510/28711], Loss: 0.8022\n",
            "Batch [27511/28711], Loss: 0.7382\n",
            "Batch [27512/28711], Loss: 1.1101\n",
            "Batch [27513/28711], Loss: 0.7863\n",
            "Batch [27514/28711], Loss: 0.9151\n",
            "Batch [27515/28711], Loss: 0.6612\n",
            "Batch [27516/28711], Loss: 0.8789\n",
            "Batch [27517/28711], Loss: 0.7218\n",
            "Batch [27518/28711], Loss: 0.7571\n",
            "Batch [27519/28711], Loss: 0.8038\n",
            "Batch [27520/28711], Loss: 0.8583\n",
            "Batch [27521/28711], Loss: 0.9741\n",
            "Batch [27522/28711], Loss: 0.9261\n",
            "Batch [27523/28711], Loss: 0.9681\n",
            "Batch [27524/28711], Loss: 0.7972\n",
            "Batch [27525/28711], Loss: 0.6727\n",
            "Batch [27526/28711], Loss: 0.8657\n",
            "Batch [27527/28711], Loss: 1.0549\n",
            "Batch [27528/28711], Loss: 0.7921\n",
            "Batch [27529/28711], Loss: 1.2564\n",
            "Batch [27530/28711], Loss: 0.8728\n",
            "Batch [27531/28711], Loss: 1.0117\n",
            "Batch [27532/28711], Loss: 0.5850\n",
            "Batch [27533/28711], Loss: 0.9636\n",
            "Batch [27534/28711], Loss: 0.9086\n",
            "Batch [27535/28711], Loss: 0.9303\n",
            "Batch [27536/28711], Loss: 0.9119\n",
            "Batch [27537/28711], Loss: 0.8399\n",
            "Batch [27538/28711], Loss: 0.8281\n",
            "Batch [27539/28711], Loss: 1.0338\n",
            "Batch [27540/28711], Loss: 0.8647\n",
            "Batch [27541/28711], Loss: 0.8828\n",
            "Batch [27542/28711], Loss: 1.0445\n",
            "Batch [27543/28711], Loss: 0.7383\n",
            "Batch [27544/28711], Loss: 1.1811\n",
            "Batch [27545/28711], Loss: 0.8871\n",
            "Batch [27546/28711], Loss: 0.8978\n",
            "Batch [27547/28711], Loss: 0.7391\n",
            "Batch [27548/28711], Loss: 0.9206\n",
            "Batch [27549/28711], Loss: 1.1147\n",
            "Batch [27550/28711], Loss: 1.0769\n",
            "Batch [27551/28711], Loss: 0.7374\n",
            "Batch [27552/28711], Loss: 1.0100\n",
            "Batch [27553/28711], Loss: 1.1938\n",
            "Batch [27554/28711], Loss: 0.8675\n",
            "Batch [27555/28711], Loss: 0.9832\n",
            "Batch [27556/28711], Loss: 1.0582\n",
            "Batch [27557/28711], Loss: 0.9391\n",
            "Batch [27558/28711], Loss: 0.8362\n",
            "Batch [27559/28711], Loss: 1.2909\n",
            "Batch [27560/28711], Loss: 1.0449\n",
            "Batch [27561/28711], Loss: 0.9053\n",
            "Batch [27562/28711], Loss: 0.8703\n",
            "Batch [27563/28711], Loss: 1.1130\n",
            "Batch [27564/28711], Loss: 0.7781\n",
            "Batch [27565/28711], Loss: 1.0717\n",
            "Batch [27566/28711], Loss: 0.9570\n",
            "Batch [27567/28711], Loss: 0.7762\n",
            "Batch [27568/28711], Loss: 1.0080\n",
            "Batch [27569/28711], Loss: 0.7009\n",
            "Batch [27570/28711], Loss: 0.8858\n",
            "Batch [27571/28711], Loss: 0.8452\n",
            "Batch [27572/28711], Loss: 0.9863\n",
            "Batch [27573/28711], Loss: 0.7763\n",
            "Batch [27574/28711], Loss: 1.0780\n",
            "Batch [27575/28711], Loss: 0.7977\n",
            "Batch [27576/28711], Loss: 1.0448\n",
            "Batch [27577/28711], Loss: 0.7266\n",
            "Batch [27578/28711], Loss: 0.9384\n",
            "Batch [27579/28711], Loss: 0.9379\n",
            "Batch [27580/28711], Loss: 0.9751\n",
            "Batch [27581/28711], Loss: 1.2005\n",
            "Batch [27582/28711], Loss: 1.0686\n",
            "Batch [27583/28711], Loss: 0.9037\n",
            "Batch [27584/28711], Loss: 0.6294\n",
            "Batch [27585/28711], Loss: 0.8901\n",
            "Batch [27586/28711], Loss: 1.0502\n",
            "Batch [27587/28711], Loss: 0.8854\n",
            "Batch [27588/28711], Loss: 1.0791\n",
            "Batch [27589/28711], Loss: 0.9148\n",
            "Batch [27590/28711], Loss: 0.7519\n",
            "Batch [27591/28711], Loss: 0.9746\n",
            "Batch [27592/28711], Loss: 0.9250\n",
            "Batch [27593/28711], Loss: 0.8706\n",
            "Batch [27594/28711], Loss: 0.9094\n",
            "Batch [27595/28711], Loss: 0.9487\n",
            "Batch [27596/28711], Loss: 1.0270\n",
            "Batch [27597/28711], Loss: 1.1817\n",
            "Batch [27598/28711], Loss: 0.8500\n",
            "Batch [27599/28711], Loss: 0.7507\n",
            "Batch [27600/28711], Loss: 0.9248\n",
            "Batch [27601/28711], Loss: 0.8142\n",
            "Batch [27602/28711], Loss: 0.8768\n",
            "Batch [27603/28711], Loss: 0.6642\n",
            "Batch [27604/28711], Loss: 0.7687\n",
            "Batch [27605/28711], Loss: 0.9389\n",
            "Batch [27606/28711], Loss: 0.8598\n",
            "Batch [27607/28711], Loss: 0.8924\n",
            "Batch [27608/28711], Loss: 0.8559\n",
            "Batch [27609/28711], Loss: 1.3738\n",
            "Batch [27610/28711], Loss: 0.9199\n",
            "Batch [27611/28711], Loss: 0.8806\n",
            "Batch [27612/28711], Loss: 0.9277\n",
            "Batch [27613/28711], Loss: 0.8582\n",
            "Batch [27614/28711], Loss: 0.8358\n",
            "Batch [27615/28711], Loss: 0.8678\n",
            "Batch [27616/28711], Loss: 0.9769\n",
            "Batch [27617/28711], Loss: 0.6606\n",
            "Batch [27618/28711], Loss: 0.4458\n",
            "Batch [27619/28711], Loss: 0.9608\n",
            "Batch [27620/28711], Loss: 1.2536\n",
            "Batch [27621/28711], Loss: 1.0084\n",
            "Batch [27622/28711], Loss: 0.6949\n",
            "Batch [27623/28711], Loss: 1.0508\n",
            "Batch [27624/28711], Loss: 0.8754\n",
            "Batch [27625/28711], Loss: 0.7467\n",
            "Batch [27626/28711], Loss: 0.6499\n",
            "Batch [27627/28711], Loss: 0.8812\n",
            "Batch [27628/28711], Loss: 0.8607\n",
            "Batch [27629/28711], Loss: 0.9932\n",
            "Batch [27630/28711], Loss: 0.8870\n",
            "Batch [27631/28711], Loss: 1.0612\n",
            "Batch [27632/28711], Loss: 1.0519\n",
            "Batch [27633/28711], Loss: 0.6032\n",
            "Batch [27634/28711], Loss: 0.7903\n",
            "Batch [27635/28711], Loss: 0.9774\n",
            "Batch [27636/28711], Loss: 0.5254\n",
            "Batch [27637/28711], Loss: 0.9254\n",
            "Batch [27638/28711], Loss: 0.9644\n",
            "Batch [27639/28711], Loss: 0.8734\n",
            "Batch [27640/28711], Loss: 0.8988\n",
            "Batch [27641/28711], Loss: 1.0560\n",
            "Batch [27642/28711], Loss: 0.9190\n",
            "Batch [27643/28711], Loss: 0.6186\n",
            "Batch [27644/28711], Loss: 0.7143\n",
            "Batch [27645/28711], Loss: 1.1291\n",
            "Batch [27646/28711], Loss: 0.9593\n",
            "Batch [27647/28711], Loss: 1.0450\n",
            "Batch [27648/28711], Loss: 0.7115\n",
            "Batch [27649/28711], Loss: 0.9315\n",
            "Batch [27650/28711], Loss: 0.8231\n",
            "Batch [27651/28711], Loss: 1.2457\n",
            "Batch [27652/28711], Loss: 1.2010\n",
            "Batch [27653/28711], Loss: 0.7937\n",
            "Batch [27654/28711], Loss: 0.9529\n",
            "Batch [27655/28711], Loss: 0.9680\n",
            "Batch [27656/28711], Loss: 0.6839\n",
            "Batch [27657/28711], Loss: 1.1499\n",
            "Batch [27658/28711], Loss: 0.7095\n",
            "Batch [27659/28711], Loss: 0.8052\n",
            "Batch [27660/28711], Loss: 0.7315\n",
            "Batch [27661/28711], Loss: 0.7490\n",
            "Batch [27662/28711], Loss: 0.9138\n",
            "Batch [27663/28711], Loss: 0.9559\n",
            "Batch [27664/28711], Loss: 1.0666\n",
            "Batch [27665/28711], Loss: 1.2926\n",
            "Batch [27666/28711], Loss: 1.0018\n",
            "Batch [27667/28711], Loss: 0.8363\n",
            "Batch [27668/28711], Loss: 1.0211\n",
            "Batch [27669/28711], Loss: 1.0750\n",
            "Batch [27670/28711], Loss: 0.8112\n",
            "Batch [27671/28711], Loss: 1.2025\n",
            "Batch [27672/28711], Loss: 1.0550\n",
            "Batch [27673/28711], Loss: 0.7401\n",
            "Batch [27674/28711], Loss: 0.9610\n",
            "Batch [27675/28711], Loss: 0.9233\n",
            "Batch [27676/28711], Loss: 1.0081\n",
            "Batch [27677/28711], Loss: 1.1047\n",
            "Batch [27678/28711], Loss: 0.9488\n",
            "Batch [27679/28711], Loss: 0.6556\n",
            "Batch [27680/28711], Loss: 0.8760\n",
            "Batch [27681/28711], Loss: 1.1540\n",
            "Batch [27682/28711], Loss: 0.8316\n",
            "Batch [27683/28711], Loss: 0.7287\n",
            "Batch [27684/28711], Loss: 0.7236\n",
            "Batch [27685/28711], Loss: 1.0233\n",
            "Batch [27686/28711], Loss: 0.8924\n",
            "Batch [27687/28711], Loss: 0.9912\n",
            "Batch [27688/28711], Loss: 0.9759\n",
            "Batch [27689/28711], Loss: 0.9177\n",
            "Batch [27690/28711], Loss: 1.1635\n",
            "Batch [27691/28711], Loss: 1.1186\n",
            "Batch [27692/28711], Loss: 0.7801\n",
            "Batch [27693/28711], Loss: 0.9029\n",
            "Batch [27694/28711], Loss: 1.1473\n",
            "Batch [27695/28711], Loss: 0.6049\n",
            "Batch [27696/28711], Loss: 1.1703\n",
            "Batch [27697/28711], Loss: 1.0853\n",
            "Batch [27698/28711], Loss: 0.7984\n",
            "Batch [27699/28711], Loss: 0.8316\n",
            "Batch [27700/28711], Loss: 0.8815\n",
            "Batch [27701/28711], Loss: 0.9851\n",
            "Batch [27702/28711], Loss: 0.9713\n",
            "Batch [27703/28711], Loss: 0.8643\n",
            "Batch [27704/28711], Loss: 0.8161\n",
            "Batch [27705/28711], Loss: 0.9494\n",
            "Batch [27706/28711], Loss: 1.0628\n",
            "Batch [27707/28711], Loss: 1.0474\n",
            "Batch [27708/28711], Loss: 0.7927\n",
            "Batch [27709/28711], Loss: 0.5567\n",
            "Batch [27710/28711], Loss: 0.7750\n",
            "Batch [27711/28711], Loss: 1.1759\n",
            "Batch [27712/28711], Loss: 0.6991\n",
            "Batch [27713/28711], Loss: 0.7946\n",
            "Batch [27714/28711], Loss: 1.1019\n",
            "Batch [27715/28711], Loss: 1.1076\n",
            "Batch [27716/28711], Loss: 0.7304\n",
            "Batch [27717/28711], Loss: 0.8719\n",
            "Batch [27718/28711], Loss: 0.5944\n",
            "Batch [27719/28711], Loss: 0.6268\n",
            "Batch [27720/28711], Loss: 0.7593\n",
            "Batch [27721/28711], Loss: 0.9234\n",
            "Batch [27722/28711], Loss: 0.8916\n",
            "Batch [27723/28711], Loss: 0.9604\n",
            "Batch [27724/28711], Loss: 0.8046\n",
            "Batch [27725/28711], Loss: 0.9084\n",
            "Batch [27726/28711], Loss: 0.8025\n",
            "Batch [27727/28711], Loss: 0.9655\n",
            "Batch [27728/28711], Loss: 1.2082\n",
            "Batch [27729/28711], Loss: 0.9242\n",
            "Batch [27730/28711], Loss: 0.9800\n",
            "Batch [27731/28711], Loss: 0.8015\n",
            "Batch [27732/28711], Loss: 1.0706\n",
            "Batch [27733/28711], Loss: 1.0208\n",
            "Batch [27734/28711], Loss: 0.9137\n",
            "Batch [27735/28711], Loss: 0.9303\n",
            "Batch [27736/28711], Loss: 0.8799\n",
            "Batch [27737/28711], Loss: 0.7865\n",
            "Batch [27738/28711], Loss: 0.7385\n",
            "Batch [27739/28711], Loss: 0.9909\n",
            "Batch [27740/28711], Loss: 0.8951\n",
            "Batch [27741/28711], Loss: 0.9773\n",
            "Batch [27742/28711], Loss: 0.9164\n",
            "Batch [27743/28711], Loss: 0.9873\n",
            "Batch [27744/28711], Loss: 1.0179\n",
            "Batch [27745/28711], Loss: 0.8495\n",
            "Batch [27746/28711], Loss: 0.8633\n",
            "Batch [27747/28711], Loss: 1.1859\n",
            "Batch [27748/28711], Loss: 0.7397\n",
            "Batch [27749/28711], Loss: 1.0275\n",
            "Batch [27750/28711], Loss: 0.8558\n",
            "Batch [27751/28711], Loss: 0.8262\n",
            "Batch [27752/28711], Loss: 0.9059\n",
            "Batch [27753/28711], Loss: 0.7039\n",
            "Batch [27754/28711], Loss: 0.8893\n",
            "Batch [27755/28711], Loss: 0.9493\n",
            "Batch [27756/28711], Loss: 1.0662\n",
            "Batch [27757/28711], Loss: 1.0843\n",
            "Batch [27758/28711], Loss: 1.3468\n",
            "Batch [27759/28711], Loss: 0.9184\n",
            "Batch [27760/28711], Loss: 0.8447\n",
            "Batch [27761/28711], Loss: 0.7081\n",
            "Batch [27762/28711], Loss: 0.8671\n",
            "Batch [27763/28711], Loss: 1.0011\n",
            "Batch [27764/28711], Loss: 1.0762\n",
            "Batch [27765/28711], Loss: 0.6763\n",
            "Batch [27766/28711], Loss: 0.7521\n",
            "Batch [27767/28711], Loss: 0.9076\n",
            "Batch [27768/28711], Loss: 0.8568\n",
            "Batch [27769/28711], Loss: 0.7982\n",
            "Batch [27770/28711], Loss: 0.9841\n",
            "Batch [27771/28711], Loss: 0.7399\n",
            "Batch [27772/28711], Loss: 1.2834\n",
            "Batch [27773/28711], Loss: 0.7009\n",
            "Batch [27774/28711], Loss: 0.9691\n",
            "Batch [27775/28711], Loss: 0.9202\n",
            "Batch [27776/28711], Loss: 0.7153\n",
            "Batch [27777/28711], Loss: 0.8588\n",
            "Batch [27778/28711], Loss: 1.1748\n",
            "Batch [27779/28711], Loss: 0.5774\n",
            "Batch [27780/28711], Loss: 0.9207\n",
            "Batch [27781/28711], Loss: 0.9772\n",
            "Batch [27782/28711], Loss: 1.0170\n",
            "Batch [27783/28711], Loss: 0.6474\n",
            "Batch [27784/28711], Loss: 0.7868\n",
            "Batch [27785/28711], Loss: 1.1427\n",
            "Batch [27786/28711], Loss: 0.9171\n",
            "Batch [27787/28711], Loss: 0.8962\n",
            "Batch [27788/28711], Loss: 1.1455\n",
            "Batch [27789/28711], Loss: 0.8288\n",
            "Batch [27790/28711], Loss: 0.9056\n",
            "Batch [27791/28711], Loss: 1.0765\n",
            "Batch [27792/28711], Loss: 0.8162\n",
            "Batch [27793/28711], Loss: 0.9620\n",
            "Batch [27794/28711], Loss: 1.0711\n",
            "Batch [27795/28711], Loss: 1.0783\n",
            "Batch [27796/28711], Loss: 0.8652\n",
            "Batch [27797/28711], Loss: 1.0178\n",
            "Batch [27798/28711], Loss: 0.8497\n",
            "Batch [27799/28711], Loss: 1.2487\n",
            "Batch [27800/28711], Loss: 0.7515\n",
            "Batch [27801/28711], Loss: 1.1055\n",
            "Batch [27802/28711], Loss: 1.1312\n",
            "Batch [27803/28711], Loss: 0.7314\n",
            "Batch [27804/28711], Loss: 0.8881\n",
            "Batch [27805/28711], Loss: 0.9119\n",
            "Batch [27806/28711], Loss: 0.7916\n",
            "Batch [27807/28711], Loss: 1.0990\n",
            "Batch [27808/28711], Loss: 0.8977\n",
            "Batch [27809/28711], Loss: 0.7570\n",
            "Batch [27810/28711], Loss: 0.9642\n",
            "Batch [27811/28711], Loss: 1.1213\n",
            "Batch [27812/28711], Loss: 0.8391\n",
            "Batch [27813/28711], Loss: 0.8045\n",
            "Batch [27814/28711], Loss: 0.8558\n",
            "Batch [27815/28711], Loss: 0.9352\n",
            "Batch [27816/28711], Loss: 1.0839\n",
            "Batch [27817/28711], Loss: 0.7723\n",
            "Batch [27818/28711], Loss: 0.7325\n",
            "Batch [27819/28711], Loss: 0.9896\n",
            "Batch [27820/28711], Loss: 0.8433\n",
            "Batch [27821/28711], Loss: 0.8777\n",
            "Batch [27822/28711], Loss: 0.9656\n",
            "Batch [27823/28711], Loss: 1.1209\n",
            "Batch [27824/28711], Loss: 0.5681\n",
            "Batch [27825/28711], Loss: 0.6802\n",
            "Batch [27826/28711], Loss: 1.2371\n",
            "Batch [27827/28711], Loss: 0.8463\n",
            "Batch [27828/28711], Loss: 0.6463\n",
            "Batch [27829/28711], Loss: 1.0423\n",
            "Batch [27830/28711], Loss: 0.7488\n",
            "Batch [27831/28711], Loss: 0.7860\n",
            "Batch [27832/28711], Loss: 0.9514\n",
            "Batch [27833/28711], Loss: 0.9624\n",
            "Batch [27834/28711], Loss: 1.0627\n",
            "Batch [27835/28711], Loss: 0.9822\n",
            "Batch [27836/28711], Loss: 0.7164\n",
            "Batch [27837/28711], Loss: 0.9693\n",
            "Batch [27838/28711], Loss: 1.0543\n",
            "Batch [27839/28711], Loss: 1.1477\n",
            "Batch [27840/28711], Loss: 0.8664\n",
            "Batch [27841/28711], Loss: 1.0980\n",
            "Batch [27842/28711], Loss: 0.9575\n",
            "Batch [27843/28711], Loss: 0.9955\n",
            "Batch [27844/28711], Loss: 0.8801\n",
            "Batch [27845/28711], Loss: 0.5999\n",
            "Batch [27846/28711], Loss: 0.7664\n",
            "Batch [27847/28711], Loss: 0.9602\n",
            "Batch [27848/28711], Loss: 0.8513\n",
            "Batch [27849/28711], Loss: 1.1760\n",
            "Batch [27850/28711], Loss: 0.7773\n",
            "Batch [27851/28711], Loss: 0.7595\n",
            "Batch [27852/28711], Loss: 1.0517\n",
            "Batch [27853/28711], Loss: 0.9941\n",
            "Batch [27854/28711], Loss: 0.9995\n",
            "Batch [27855/28711], Loss: 1.1455\n",
            "Batch [27856/28711], Loss: 0.7805\n",
            "Batch [27857/28711], Loss: 0.8274\n",
            "Batch [27858/28711], Loss: 0.8727\n",
            "Batch [27859/28711], Loss: 0.9856\n",
            "Batch [27860/28711], Loss: 0.9829\n",
            "Batch [27861/28711], Loss: 0.9006\n",
            "Batch [27862/28711], Loss: 1.0738\n",
            "Batch [27863/28711], Loss: 0.9173\n",
            "Batch [27864/28711], Loss: 0.7126\n",
            "Batch [27865/28711], Loss: 0.7991\n",
            "Batch [27866/28711], Loss: 1.1823\n",
            "Batch [27867/28711], Loss: 0.8180\n",
            "Batch [27868/28711], Loss: 0.8359\n",
            "Batch [27869/28711], Loss: 0.9336\n",
            "Batch [27870/28711], Loss: 0.9605\n",
            "Batch [27871/28711], Loss: 0.7168\n",
            "Batch [27872/28711], Loss: 0.7855\n",
            "Batch [27873/28711], Loss: 0.6960\n",
            "Batch [27874/28711], Loss: 0.6282\n",
            "Batch [27875/28711], Loss: 0.8363\n",
            "Batch [27876/28711], Loss: 0.9013\n",
            "Batch [27877/28711], Loss: 0.9233\n",
            "Batch [27878/28711], Loss: 0.8977\n",
            "Batch [27879/28711], Loss: 0.8888\n",
            "Batch [27880/28711], Loss: 0.8682\n",
            "Batch [27881/28711], Loss: 0.9734\n",
            "Batch [27882/28711], Loss: 1.2245\n",
            "Batch [27883/28711], Loss: 1.0488\n",
            "Batch [27884/28711], Loss: 1.0527\n",
            "Batch [27885/28711], Loss: 0.9697\n",
            "Batch [27886/28711], Loss: 0.9311\n",
            "Batch [27887/28711], Loss: 0.9265\n",
            "Batch [27888/28711], Loss: 1.0931\n",
            "Batch [27889/28711], Loss: 1.1683\n",
            "Batch [27890/28711], Loss: 1.0057\n",
            "Batch [27891/28711], Loss: 0.6388\n",
            "Batch [27892/28711], Loss: 0.7568\n",
            "Batch [27893/28711], Loss: 0.7541\n",
            "Batch [27894/28711], Loss: 1.1756\n",
            "Batch [27895/28711], Loss: 0.8563\n",
            "Batch [27896/28711], Loss: 0.7936\n",
            "Batch [27897/28711], Loss: 0.9784\n",
            "Batch [27898/28711], Loss: 0.6775\n",
            "Batch [27899/28711], Loss: 0.7592\n",
            "Batch [27900/28711], Loss: 1.0105\n",
            "Batch [27901/28711], Loss: 0.8821\n",
            "Batch [27902/28711], Loss: 0.8049\n",
            "Batch [27903/28711], Loss: 0.8429\n",
            "Batch [27904/28711], Loss: 0.8904\n",
            "Batch [27905/28711], Loss: 0.9144\n",
            "Batch [27906/28711], Loss: 0.7739\n",
            "Batch [27907/28711], Loss: 1.0461\n",
            "Batch [27908/28711], Loss: 0.8401\n",
            "Batch [27909/28711], Loss: 1.1017\n",
            "Batch [27910/28711], Loss: 0.6682\n",
            "Batch [27911/28711], Loss: 0.8889\n",
            "Batch [27912/28711], Loss: 0.6101\n",
            "Batch [27913/28711], Loss: 0.9916\n",
            "Batch [27914/28711], Loss: 0.8905\n",
            "Batch [27915/28711], Loss: 1.1466\n",
            "Batch [27916/28711], Loss: 0.6446\n",
            "Batch [27917/28711], Loss: 0.7533\n",
            "Batch [27918/28711], Loss: 0.9737\n",
            "Batch [27919/28711], Loss: 1.1433\n",
            "Batch [27920/28711], Loss: 0.9386\n",
            "Batch [27921/28711], Loss: 1.0986\n",
            "Batch [27922/28711], Loss: 0.7464\n",
            "Batch [27923/28711], Loss: 0.8987\n",
            "Batch [27924/28711], Loss: 0.7028\n",
            "Batch [27925/28711], Loss: 0.8256\n",
            "Batch [27926/28711], Loss: 0.8681\n",
            "Batch [27927/28711], Loss: 0.6708\n",
            "Batch [27928/28711], Loss: 1.2124\n",
            "Batch [27929/28711], Loss: 0.6894\n",
            "Batch [27930/28711], Loss: 0.6848\n",
            "Batch [27931/28711], Loss: 1.1458\n",
            "Batch [27932/28711], Loss: 0.5727\n",
            "Batch [27933/28711], Loss: 1.0630\n",
            "Batch [27934/28711], Loss: 0.9253\n",
            "Batch [27935/28711], Loss: 0.9815\n",
            "Batch [27936/28711], Loss: 1.0714\n",
            "Batch [27937/28711], Loss: 1.0899\n",
            "Batch [27938/28711], Loss: 0.9358\n",
            "Batch [27939/28711], Loss: 0.8717\n",
            "Batch [27940/28711], Loss: 0.9652\n",
            "Batch [27941/28711], Loss: 0.6390\n",
            "Batch [27942/28711], Loss: 1.0514\n",
            "Batch [27943/28711], Loss: 0.8477\n",
            "Batch [27944/28711], Loss: 1.1047\n",
            "Batch [27945/28711], Loss: 1.0142\n",
            "Batch [27946/28711], Loss: 0.7975\n",
            "Batch [27947/28711], Loss: 1.0471\n",
            "Batch [27948/28711], Loss: 0.8859\n",
            "Batch [27949/28711], Loss: 0.9273\n",
            "Batch [27950/28711], Loss: 0.8284\n",
            "Batch [27951/28711], Loss: 1.0481\n",
            "Batch [27952/28711], Loss: 1.2694\n",
            "Batch [27953/28711], Loss: 0.8427\n",
            "Batch [27954/28711], Loss: 0.9204\n",
            "Batch [27955/28711], Loss: 0.8727\n",
            "Batch [27956/28711], Loss: 1.0764\n",
            "Batch [27957/28711], Loss: 0.8911\n",
            "Batch [27958/28711], Loss: 0.7873\n",
            "Batch [27959/28711], Loss: 0.7349\n",
            "Batch [27960/28711], Loss: 1.1375\n",
            "Batch [27961/28711], Loss: 1.1176\n",
            "Batch [27962/28711], Loss: 1.0159\n",
            "Batch [27963/28711], Loss: 0.7653\n",
            "Batch [27964/28711], Loss: 0.8292\n",
            "Batch [27965/28711], Loss: 0.7928\n",
            "Batch [27966/28711], Loss: 0.9482\n",
            "Batch [27967/28711], Loss: 0.9369\n",
            "Batch [27968/28711], Loss: 0.7997\n",
            "Batch [27969/28711], Loss: 0.9493\n",
            "Batch [27970/28711], Loss: 0.8713\n",
            "Batch [27971/28711], Loss: 0.8365\n",
            "Batch [27972/28711], Loss: 1.0561\n",
            "Batch [27973/28711], Loss: 0.9994\n",
            "Batch [27974/28711], Loss: 0.9893\n",
            "Batch [27975/28711], Loss: 1.0890\n",
            "Batch [27976/28711], Loss: 1.0282\n",
            "Batch [27977/28711], Loss: 1.0110\n",
            "Batch [27978/28711], Loss: 0.8840\n",
            "Batch [27979/28711], Loss: 0.9762\n",
            "Batch [27980/28711], Loss: 0.9823\n",
            "Batch [27981/28711], Loss: 0.8198\n",
            "Batch [27982/28711], Loss: 1.0754\n",
            "Batch [27983/28711], Loss: 0.8238\n",
            "Batch [27984/28711], Loss: 0.6887\n",
            "Batch [27985/28711], Loss: 1.0465\n",
            "Batch [27986/28711], Loss: 0.9502\n",
            "Batch [27987/28711], Loss: 0.7596\n",
            "Batch [27988/28711], Loss: 0.7537\n",
            "Batch [27989/28711], Loss: 0.8923\n",
            "Batch [27990/28711], Loss: 0.9137\n",
            "Batch [27991/28711], Loss: 0.7326\n",
            "Batch [27992/28711], Loss: 0.8765\n",
            "Batch [27993/28711], Loss: 0.9235\n",
            "Batch [27994/28711], Loss: 0.9743\n",
            "Batch [27995/28711], Loss: 1.0386\n",
            "Batch [27996/28711], Loss: 1.2051\n",
            "Batch [27997/28711], Loss: 0.8859\n",
            "Batch [27998/28711], Loss: 0.9729\n",
            "Batch [27999/28711], Loss: 0.5788\n",
            "Batch [28000/28711], Loss: 0.9127\n",
            "Batch [28001/28711], Loss: 0.7782\n",
            "Batch [28002/28711], Loss: 1.3157\n",
            "Batch [28003/28711], Loss: 1.0590\n",
            "Batch [28004/28711], Loss: 0.9615\n",
            "Batch [28005/28711], Loss: 0.8105\n",
            "Batch [28006/28711], Loss: 0.8898\n",
            "Batch [28007/28711], Loss: 0.9324\n",
            "Batch [28008/28711], Loss: 0.9522\n",
            "Batch [28009/28711], Loss: 0.9428\n",
            "Batch [28010/28711], Loss: 1.0260\n",
            "Batch [28011/28711], Loss: 0.8282\n",
            "Batch [28012/28711], Loss: 1.1155\n",
            "Batch [28013/28711], Loss: 0.6842\n",
            "Batch [28014/28711], Loss: 0.8764\n",
            "Batch [28015/28711], Loss: 1.0894\n",
            "Batch [28016/28711], Loss: 0.5812\n",
            "Batch [28017/28711], Loss: 0.6812\n",
            "Batch [28018/28711], Loss: 1.2206\n",
            "Batch [28019/28711], Loss: 0.8092\n",
            "Batch [28020/28711], Loss: 0.6745\n",
            "Batch [28021/28711], Loss: 0.9127\n",
            "Batch [28022/28711], Loss: 0.8012\n",
            "Batch [28023/28711], Loss: 0.8266\n",
            "Batch [28024/28711], Loss: 1.1620\n",
            "Batch [28025/28711], Loss: 1.1432\n",
            "Batch [28026/28711], Loss: 0.8176\n",
            "Batch [28027/28711], Loss: 1.0187\n",
            "Batch [28028/28711], Loss: 0.9956\n",
            "Batch [28029/28711], Loss: 0.8123\n",
            "Batch [28030/28711], Loss: 1.0156\n",
            "Batch [28031/28711], Loss: 0.8365\n",
            "Batch [28032/28711], Loss: 1.2591\n",
            "Batch [28033/28711], Loss: 0.5448\n",
            "Batch [28034/28711], Loss: 0.9914\n",
            "Batch [28035/28711], Loss: 0.7383\n",
            "Batch [28036/28711], Loss: 1.0913\n",
            "Batch [28037/28711], Loss: 1.3727\n",
            "Batch [28038/28711], Loss: 1.0335\n",
            "Batch [28039/28711], Loss: 0.9284\n",
            "Batch [28040/28711], Loss: 0.8361\n",
            "Batch [28041/28711], Loss: 0.8468\n",
            "Batch [28042/28711], Loss: 0.7379\n",
            "Batch [28043/28711], Loss: 0.9602\n",
            "Batch [28044/28711], Loss: 1.1536\n",
            "Batch [28045/28711], Loss: 1.0146\n",
            "Batch [28046/28711], Loss: 0.9609\n",
            "Batch [28047/28711], Loss: 1.0292\n",
            "Batch [28048/28711], Loss: 1.1089\n",
            "Batch [28049/28711], Loss: 1.1169\n",
            "Batch [28050/28711], Loss: 1.0536\n",
            "Batch [28051/28711], Loss: 0.9790\n",
            "Batch [28052/28711], Loss: 0.7820\n",
            "Batch [28053/28711], Loss: 0.8648\n",
            "Batch [28054/28711], Loss: 0.9345\n",
            "Batch [28055/28711], Loss: 1.0186\n",
            "Batch [28056/28711], Loss: 1.0944\n",
            "Batch [28057/28711], Loss: 0.8477\n",
            "Batch [28058/28711], Loss: 0.7673\n",
            "Batch [28059/28711], Loss: 1.2565\n",
            "Batch [28060/28711], Loss: 0.9904\n",
            "Batch [28061/28711], Loss: 0.7115\n",
            "Batch [28062/28711], Loss: 0.9243\n",
            "Batch [28063/28711], Loss: 0.7939\n",
            "Batch [28064/28711], Loss: 1.1659\n",
            "Batch [28065/28711], Loss: 0.9878\n",
            "Batch [28066/28711], Loss: 0.8330\n",
            "Batch [28067/28711], Loss: 0.8652\n",
            "Batch [28068/28711], Loss: 0.9267\n",
            "Batch [28069/28711], Loss: 1.1024\n",
            "Batch [28070/28711], Loss: 0.9095\n",
            "Batch [28071/28711], Loss: 0.7456\n",
            "Batch [28072/28711], Loss: 0.7063\n",
            "Batch [28073/28711], Loss: 1.0266\n",
            "Batch [28074/28711], Loss: 1.1248\n",
            "Batch [28075/28711], Loss: 1.2668\n",
            "Batch [28076/28711], Loss: 1.0231\n",
            "Batch [28077/28711], Loss: 1.0612\n",
            "Batch [28078/28711], Loss: 0.7906\n",
            "Batch [28079/28711], Loss: 1.0493\n",
            "Batch [28080/28711], Loss: 1.2025\n",
            "Batch [28081/28711], Loss: 0.7170\n",
            "Batch [28082/28711], Loss: 0.7656\n",
            "Batch [28083/28711], Loss: 0.7627\n",
            "Batch [28084/28711], Loss: 0.8295\n",
            "Batch [28085/28711], Loss: 0.7287\n",
            "Batch [28086/28711], Loss: 0.8636\n",
            "Batch [28087/28711], Loss: 1.0275\n",
            "Batch [28088/28711], Loss: 0.9334\n",
            "Batch [28089/28711], Loss: 0.8733\n",
            "Batch [28090/28711], Loss: 0.8754\n",
            "Batch [28091/28711], Loss: 0.8207\n",
            "Batch [28092/28711], Loss: 1.0344\n",
            "Batch [28093/28711], Loss: 0.8464\n",
            "Batch [28094/28711], Loss: 0.7411\n",
            "Batch [28095/28711], Loss: 1.0560\n",
            "Batch [28096/28711], Loss: 0.7284\n",
            "Batch [28097/28711], Loss: 0.9887\n",
            "Batch [28098/28711], Loss: 0.9860\n",
            "Batch [28099/28711], Loss: 1.0716\n",
            "Batch [28100/28711], Loss: 0.9671\n",
            "Batch [28101/28711], Loss: 1.2163\n",
            "Batch [28102/28711], Loss: 0.9144\n",
            "Batch [28103/28711], Loss: 0.7800\n",
            "Batch [28104/28711], Loss: 0.8237\n",
            "Batch [28105/28711], Loss: 0.8027\n",
            "Batch [28106/28711], Loss: 1.0063\n",
            "Batch [28107/28711], Loss: 0.7109\n",
            "Batch [28108/28711], Loss: 0.7494\n",
            "Batch [28109/28711], Loss: 0.8672\n",
            "Batch [28110/28711], Loss: 0.9971\n",
            "Batch [28111/28711], Loss: 1.1130\n",
            "Batch [28112/28711], Loss: 1.2047\n",
            "Batch [28113/28711], Loss: 0.6850\n",
            "Batch [28114/28711], Loss: 1.1872\n",
            "Batch [28115/28711], Loss: 0.6926\n",
            "Batch [28116/28711], Loss: 0.9542\n",
            "Batch [28117/28711], Loss: 0.9744\n",
            "Batch [28118/28711], Loss: 0.9384\n",
            "Batch [28119/28711], Loss: 0.7695\n",
            "Batch [28120/28711], Loss: 0.8643\n",
            "Batch [28121/28711], Loss: 1.2180\n",
            "Batch [28122/28711], Loss: 0.8648\n",
            "Batch [28123/28711], Loss: 0.8757\n",
            "Batch [28124/28711], Loss: 0.8525\n",
            "Batch [28125/28711], Loss: 0.8084\n",
            "Batch [28126/28711], Loss: 0.9295\n",
            "Batch [28127/28711], Loss: 0.6908\n",
            "Batch [28128/28711], Loss: 1.1040\n",
            "Batch [28129/28711], Loss: 0.9732\n",
            "Batch [28130/28711], Loss: 0.7900\n",
            "Batch [28131/28711], Loss: 0.9583\n",
            "Batch [28132/28711], Loss: 0.9762\n",
            "Batch [28133/28711], Loss: 0.8977\n",
            "Batch [28134/28711], Loss: 1.0075\n",
            "Batch [28135/28711], Loss: 0.9087\n",
            "Batch [28136/28711], Loss: 0.7573\n",
            "Batch [28137/28711], Loss: 0.8059\n",
            "Batch [28138/28711], Loss: 0.9636\n",
            "Batch [28139/28711], Loss: 1.0163\n",
            "Batch [28140/28711], Loss: 0.8355\n",
            "Batch [28141/28711], Loss: 0.9358\n",
            "Batch [28142/28711], Loss: 0.8964\n",
            "Batch [28143/28711], Loss: 0.7830\n",
            "Batch [28144/28711], Loss: 1.3684\n",
            "Batch [28145/28711], Loss: 0.9020\n",
            "Batch [28146/28711], Loss: 0.9024\n",
            "Batch [28147/28711], Loss: 0.8328\n",
            "Batch [28148/28711], Loss: 0.8658\n",
            "Batch [28149/28711], Loss: 0.7448\n",
            "Batch [28150/28711], Loss: 1.0404\n",
            "Batch [28151/28711], Loss: 0.7949\n",
            "Batch [28152/28711], Loss: 1.0868\n",
            "Batch [28153/28711], Loss: 0.9793\n",
            "Batch [28154/28711], Loss: 0.8674\n",
            "Batch [28155/28711], Loss: 0.9200\n",
            "Batch [28156/28711], Loss: 0.9967\n",
            "Batch [28157/28711], Loss: 1.0040\n",
            "Batch [28158/28711], Loss: 0.7450\n",
            "Batch [28159/28711], Loss: 0.8825\n",
            "Batch [28160/28711], Loss: 0.6904\n",
            "Batch [28161/28711], Loss: 0.9265\n",
            "Batch [28162/28711], Loss: 0.8539\n",
            "Batch [28163/28711], Loss: 0.9502\n",
            "Batch [28164/28711], Loss: 1.2461\n",
            "Batch [28165/28711], Loss: 0.7297\n",
            "Batch [28166/28711], Loss: 1.0549\n",
            "Batch [28167/28711], Loss: 0.8538\n",
            "Batch [28168/28711], Loss: 0.6616\n",
            "Batch [28169/28711], Loss: 0.8407\n",
            "Batch [28170/28711], Loss: 0.8454\n",
            "Batch [28171/28711], Loss: 0.9856\n",
            "Batch [28172/28711], Loss: 0.9143\n",
            "Batch [28173/28711], Loss: 0.7263\n",
            "Batch [28174/28711], Loss: 1.0126\n",
            "Batch [28175/28711], Loss: 0.7797\n",
            "Batch [28176/28711], Loss: 1.0532\n",
            "Batch [28177/28711], Loss: 1.1205\n",
            "Batch [28178/28711], Loss: 1.1945\n",
            "Batch [28179/28711], Loss: 0.7825\n",
            "Batch [28180/28711], Loss: 0.6269\n",
            "Batch [28181/28711], Loss: 0.8878\n",
            "Batch [28182/28711], Loss: 0.7474\n",
            "Batch [28183/28711], Loss: 1.1443\n",
            "Batch [28184/28711], Loss: 0.7636\n",
            "Batch [28185/28711], Loss: 0.7718\n",
            "Batch [28186/28711], Loss: 0.5397\n",
            "Batch [28187/28711], Loss: 0.7999\n",
            "Batch [28188/28711], Loss: 1.2745\n",
            "Batch [28189/28711], Loss: 0.9382\n",
            "Batch [28190/28711], Loss: 1.0478\n",
            "Batch [28191/28711], Loss: 1.1309\n",
            "Batch [28192/28711], Loss: 0.8935\n",
            "Batch [28193/28711], Loss: 1.0520\n",
            "Batch [28194/28711], Loss: 0.7459\n",
            "Batch [28195/28711], Loss: 0.8555\n",
            "Batch [28196/28711], Loss: 0.9586\n",
            "Batch [28197/28711], Loss: 0.9515\n",
            "Batch [28198/28711], Loss: 0.8810\n",
            "Batch [28199/28711], Loss: 1.0905\n",
            "Batch [28200/28711], Loss: 0.9523\n",
            "Batch [28201/28711], Loss: 0.9975\n",
            "Batch [28202/28711], Loss: 0.6295\n",
            "Batch [28203/28711], Loss: 0.9745\n",
            "Batch [28204/28711], Loss: 1.0044\n",
            "Batch [28205/28711], Loss: 0.9560\n",
            "Batch [28206/28711], Loss: 0.7387\n",
            "Batch [28207/28711], Loss: 0.7906\n",
            "Batch [28208/28711], Loss: 1.1203\n",
            "Batch [28209/28711], Loss: 0.9106\n",
            "Batch [28210/28711], Loss: 0.7981\n",
            "Batch [28211/28711], Loss: 1.0918\n",
            "Batch [28212/28711], Loss: 1.0026\n",
            "Batch [28213/28711], Loss: 0.8105\n",
            "Batch [28214/28711], Loss: 0.8815\n",
            "Batch [28215/28711], Loss: 1.0170\n",
            "Batch [28216/28711], Loss: 0.6403\n",
            "Batch [28217/28711], Loss: 0.9199\n",
            "Batch [28218/28711], Loss: 1.1113\n",
            "Batch [28219/28711], Loss: 1.0238\n",
            "Batch [28220/28711], Loss: 0.9318\n",
            "Batch [28221/28711], Loss: 0.8432\n",
            "Batch [28222/28711], Loss: 0.9014\n",
            "Batch [28223/28711], Loss: 1.0391\n",
            "Batch [28224/28711], Loss: 1.0121\n",
            "Batch [28225/28711], Loss: 0.8893\n",
            "Batch [28226/28711], Loss: 0.7294\n",
            "Batch [28227/28711], Loss: 1.1759\n",
            "Batch [28228/28711], Loss: 0.8053\n",
            "Batch [28229/28711], Loss: 0.9960\n",
            "Batch [28230/28711], Loss: 0.6880\n",
            "Batch [28231/28711], Loss: 0.9808\n",
            "Batch [28232/28711], Loss: 0.5171\n",
            "Batch [28233/28711], Loss: 0.8560\n",
            "Batch [28234/28711], Loss: 1.1314\n",
            "Batch [28235/28711], Loss: 1.1586\n",
            "Batch [28236/28711], Loss: 1.2107\n",
            "Batch [28237/28711], Loss: 0.8245\n",
            "Batch [28238/28711], Loss: 1.3484\n",
            "Batch [28239/28711], Loss: 1.0484\n",
            "Batch [28240/28711], Loss: 0.9685\n",
            "Batch [28241/28711], Loss: 0.7239\n",
            "Batch [28242/28711], Loss: 1.0144\n",
            "Batch [28243/28711], Loss: 0.7499\n",
            "Batch [28244/28711], Loss: 0.8814\n",
            "Batch [28245/28711], Loss: 0.8815\n",
            "Batch [28246/28711], Loss: 0.7216\n",
            "Batch [28247/28711], Loss: 1.1427\n",
            "Batch [28248/28711], Loss: 0.9448\n",
            "Batch [28249/28711], Loss: 1.1096\n",
            "Batch [28250/28711], Loss: 0.7657\n",
            "Batch [28251/28711], Loss: 1.0187\n",
            "Batch [28252/28711], Loss: 0.8377\n",
            "Batch [28253/28711], Loss: 0.8901\n",
            "Batch [28254/28711], Loss: 0.9291\n",
            "Batch [28255/28711], Loss: 0.7145\n",
            "Batch [28256/28711], Loss: 0.7114\n",
            "Batch [28257/28711], Loss: 0.8578\n",
            "Batch [28258/28711], Loss: 0.9088\n",
            "Batch [28259/28711], Loss: 0.9658\n",
            "Batch [28260/28711], Loss: 0.7539\n",
            "Batch [28261/28711], Loss: 1.0149\n",
            "Batch [28262/28711], Loss: 1.0033\n",
            "Batch [28263/28711], Loss: 1.0932\n",
            "Batch [28264/28711], Loss: 1.1583\n",
            "Batch [28265/28711], Loss: 1.1260\n",
            "Batch [28266/28711], Loss: 0.8080\n",
            "Batch [28267/28711], Loss: 1.0470\n",
            "Batch [28268/28711], Loss: 0.8810\n",
            "Batch [28269/28711], Loss: 0.7310\n",
            "Batch [28270/28711], Loss: 1.1111\n",
            "Batch [28271/28711], Loss: 0.6413\n",
            "Batch [28272/28711], Loss: 1.1475\n",
            "Batch [28273/28711], Loss: 0.8990\n",
            "Batch [28274/28711], Loss: 0.9822\n",
            "Batch [28275/28711], Loss: 0.8570\n",
            "Batch [28276/28711], Loss: 0.7534\n",
            "Batch [28277/28711], Loss: 0.7512\n",
            "Batch [28278/28711], Loss: 0.9582\n",
            "Batch [28279/28711], Loss: 1.0563\n",
            "Batch [28280/28711], Loss: 0.6732\n",
            "Batch [28281/28711], Loss: 0.8373\n",
            "Batch [28282/28711], Loss: 0.8271\n",
            "Batch [28283/28711], Loss: 0.9054\n",
            "Batch [28284/28711], Loss: 0.9891\n",
            "Batch [28285/28711], Loss: 0.9668\n",
            "Batch [28286/28711], Loss: 1.0498\n",
            "Batch [28287/28711], Loss: 0.9684\n",
            "Batch [28288/28711], Loss: 0.7652\n",
            "Batch [28289/28711], Loss: 1.1643\n",
            "Batch [28290/28711], Loss: 0.9930\n",
            "Batch [28291/28711], Loss: 0.8547\n",
            "Batch [28292/28711], Loss: 1.0096\n",
            "Batch [28293/28711], Loss: 1.1447\n",
            "Batch [28294/28711], Loss: 0.9639\n",
            "Batch [28295/28711], Loss: 1.0293\n",
            "Batch [28296/28711], Loss: 0.7209\n",
            "Batch [28297/28711], Loss: 0.9244\n",
            "Batch [28298/28711], Loss: 0.9104\n",
            "Batch [28299/28711], Loss: 0.8412\n",
            "Batch [28300/28711], Loss: 0.9188\n",
            "Batch [28301/28711], Loss: 1.1726\n",
            "Batch [28302/28711], Loss: 0.6398\n",
            "Batch [28303/28711], Loss: 0.9366\n",
            "Batch [28304/28711], Loss: 0.8245\n",
            "Batch [28305/28711], Loss: 0.9387\n",
            "Batch [28306/28711], Loss: 0.9534\n",
            "Batch [28307/28711], Loss: 0.9091\n",
            "Batch [28308/28711], Loss: 0.7345\n",
            "Batch [28309/28711], Loss: 0.8749\n",
            "Batch [28310/28711], Loss: 0.9719\n",
            "Batch [28311/28711], Loss: 0.6577\n",
            "Batch [28312/28711], Loss: 0.8232\n",
            "Batch [28313/28711], Loss: 0.9162\n",
            "Batch [28314/28711], Loss: 0.7739\n",
            "Batch [28315/28711], Loss: 0.8232\n",
            "Batch [28316/28711], Loss: 0.9688\n",
            "Batch [28317/28711], Loss: 1.1996\n",
            "Batch [28318/28711], Loss: 0.7672\n",
            "Batch [28319/28711], Loss: 0.9547\n",
            "Batch [28320/28711], Loss: 0.6749\n",
            "Batch [28321/28711], Loss: 1.1616\n",
            "Batch [28322/28711], Loss: 0.7558\n",
            "Batch [28323/28711], Loss: 0.8562\n",
            "Batch [28324/28711], Loss: 0.6716\n",
            "Batch [28325/28711], Loss: 1.2162\n",
            "Batch [28326/28711], Loss: 0.7771\n",
            "Batch [28327/28711], Loss: 1.2358\n",
            "Batch [28328/28711], Loss: 0.7425\n",
            "Batch [28329/28711], Loss: 0.9085\n",
            "Batch [28330/28711], Loss: 0.9717\n",
            "Batch [28331/28711], Loss: 0.9803\n",
            "Batch [28332/28711], Loss: 0.8536\n",
            "Batch [28333/28711], Loss: 0.8649\n",
            "Batch [28334/28711], Loss: 0.7593\n",
            "Batch [28335/28711], Loss: 0.6270\n",
            "Batch [28336/28711], Loss: 0.6672\n",
            "Batch [28337/28711], Loss: 1.1021\n",
            "Batch [28338/28711], Loss: 0.9185\n",
            "Batch [28339/28711], Loss: 0.7028\n",
            "Batch [28340/28711], Loss: 0.9856\n",
            "Batch [28341/28711], Loss: 0.8254\n",
            "Batch [28342/28711], Loss: 0.8847\n",
            "Batch [28343/28711], Loss: 0.9276\n",
            "Batch [28344/28711], Loss: 0.7130\n",
            "Batch [28345/28711], Loss: 0.8195\n",
            "Batch [28346/28711], Loss: 0.9570\n",
            "Batch [28347/28711], Loss: 0.9526\n",
            "Batch [28348/28711], Loss: 0.9424\n",
            "Batch [28349/28711], Loss: 0.8245\n",
            "Batch [28350/28711], Loss: 1.0093\n",
            "Batch [28351/28711], Loss: 0.8542\n",
            "Batch [28352/28711], Loss: 0.8372\n",
            "Batch [28353/28711], Loss: 0.5778\n",
            "Batch [28354/28711], Loss: 0.9649\n",
            "Batch [28355/28711], Loss: 1.0537\n",
            "Batch [28356/28711], Loss: 0.8591\n",
            "Batch [28357/28711], Loss: 1.0432\n",
            "Batch [28358/28711], Loss: 0.9618\n",
            "Batch [28359/28711], Loss: 0.8396\n",
            "Batch [28360/28711], Loss: 0.8534\n",
            "Batch [28361/28711], Loss: 0.9530\n",
            "Batch [28362/28711], Loss: 0.6933\n",
            "Batch [28363/28711], Loss: 0.9607\n",
            "Batch [28364/28711], Loss: 0.8394\n",
            "Batch [28365/28711], Loss: 0.9255\n",
            "Batch [28366/28711], Loss: 0.9196\n",
            "Batch [28367/28711], Loss: 0.7908\n",
            "Batch [28368/28711], Loss: 1.0528\n",
            "Batch [28369/28711], Loss: 0.8570\n",
            "Batch [28370/28711], Loss: 1.0242\n",
            "Batch [28371/28711], Loss: 0.8552\n",
            "Batch [28372/28711], Loss: 0.9805\n",
            "Batch [28373/28711], Loss: 0.8642\n",
            "Batch [28374/28711], Loss: 1.0538\n",
            "Batch [28375/28711], Loss: 0.5826\n",
            "Batch [28376/28711], Loss: 1.0322\n",
            "Batch [28377/28711], Loss: 0.8679\n",
            "Batch [28378/28711], Loss: 1.0456\n",
            "Batch [28379/28711], Loss: 0.6006\n",
            "Batch [28380/28711], Loss: 1.2750\n",
            "Batch [28381/28711], Loss: 0.6366\n",
            "Batch [28382/28711], Loss: 0.7381\n",
            "Batch [28383/28711], Loss: 0.7252\n",
            "Batch [28384/28711], Loss: 0.8138\n",
            "Batch [28385/28711], Loss: 1.3289\n",
            "Batch [28386/28711], Loss: 1.0179\n",
            "Batch [28387/28711], Loss: 1.0132\n",
            "Batch [28388/28711], Loss: 0.7568\n",
            "Batch [28389/28711], Loss: 0.9535\n",
            "Batch [28390/28711], Loss: 0.9040\n",
            "Batch [28391/28711], Loss: 0.7308\n",
            "Batch [28392/28711], Loss: 0.9752\n",
            "Batch [28393/28711], Loss: 1.0132\n",
            "Batch [28394/28711], Loss: 0.7981\n",
            "Batch [28395/28711], Loss: 0.5942\n",
            "Batch [28396/28711], Loss: 0.9538\n",
            "Batch [28397/28711], Loss: 0.8648\n",
            "Batch [28398/28711], Loss: 0.8669\n",
            "Batch [28399/28711], Loss: 1.1750\n",
            "Batch [28400/28711], Loss: 1.0618\n",
            "Batch [28401/28711], Loss: 0.8042\n",
            "Batch [28402/28711], Loss: 1.0573\n",
            "Batch [28403/28711], Loss: 0.8638\n",
            "Batch [28404/28711], Loss: 0.7222\n",
            "Batch [28405/28711], Loss: 0.7030\n",
            "Batch [28406/28711], Loss: 0.9718\n",
            "Batch [28407/28711], Loss: 0.8232\n",
            "Batch [28408/28711], Loss: 1.2267\n",
            "Batch [28409/28711], Loss: 0.9419\n",
            "Batch [28410/28711], Loss: 1.1056\n",
            "Batch [28411/28711], Loss: 0.9034\n",
            "Batch [28412/28711], Loss: 0.6903\n",
            "Batch [28413/28711], Loss: 0.8778\n",
            "Batch [28414/28711], Loss: 1.0292\n",
            "Batch [28415/28711], Loss: 0.8413\n",
            "Batch [28416/28711], Loss: 1.0495\n",
            "Batch [28417/28711], Loss: 0.9884\n",
            "Batch [28418/28711], Loss: 0.8886\n",
            "Batch [28419/28711], Loss: 1.0394\n",
            "Batch [28420/28711], Loss: 0.7993\n",
            "Batch [28421/28711], Loss: 0.8647\n",
            "Batch [28422/28711], Loss: 0.9408\n",
            "Batch [28423/28711], Loss: 0.9080\n",
            "Batch [28424/28711], Loss: 0.8788\n",
            "Batch [28425/28711], Loss: 0.8263\n",
            "Batch [28426/28711], Loss: 0.8352\n",
            "Batch [28427/28711], Loss: 0.6875\n",
            "Batch [28428/28711], Loss: 0.8681\n",
            "Batch [28429/28711], Loss: 0.9457\n",
            "Batch [28430/28711], Loss: 1.0634\n",
            "Batch [28431/28711], Loss: 1.1253\n",
            "Batch [28432/28711], Loss: 0.8916\n",
            "Batch [28433/28711], Loss: 1.0397\n",
            "Batch [28434/28711], Loss: 0.6752\n",
            "Batch [28435/28711], Loss: 0.7243\n",
            "Batch [28436/28711], Loss: 0.8515\n",
            "Batch [28437/28711], Loss: 1.0052\n",
            "Batch [28438/28711], Loss: 0.9422\n",
            "Batch [28439/28711], Loss: 0.7873\n",
            "Batch [28440/28711], Loss: 0.7617\n",
            "Batch [28441/28711], Loss: 0.9355\n",
            "Batch [28442/28711], Loss: 0.9764\n",
            "Batch [28443/28711], Loss: 1.0074\n",
            "Batch [28444/28711], Loss: 0.6937\n",
            "Batch [28445/28711], Loss: 0.6851\n",
            "Batch [28446/28711], Loss: 1.1259\n",
            "Batch [28447/28711], Loss: 0.7027\n",
            "Batch [28448/28711], Loss: 1.2384\n",
            "Batch [28449/28711], Loss: 0.8023\n",
            "Batch [28450/28711], Loss: 0.8176\n",
            "Batch [28451/28711], Loss: 0.9576\n",
            "Batch [28452/28711], Loss: 0.7419\n",
            "Batch [28453/28711], Loss: 1.2306\n",
            "Batch [28454/28711], Loss: 0.9916\n",
            "Batch [28455/28711], Loss: 0.8688\n",
            "Batch [28456/28711], Loss: 0.7394\n",
            "Batch [28457/28711], Loss: 0.9477\n",
            "Batch [28458/28711], Loss: 1.1574\n",
            "Batch [28459/28711], Loss: 1.1440\n",
            "Batch [28460/28711], Loss: 1.0519\n",
            "Batch [28461/28711], Loss: 0.6840\n",
            "Batch [28462/28711], Loss: 0.6648\n",
            "Batch [28463/28711], Loss: 0.8917\n",
            "Batch [28464/28711], Loss: 0.8066\n",
            "Batch [28465/28711], Loss: 0.6877\n",
            "Batch [28466/28711], Loss: 0.9950\n",
            "Batch [28467/28711], Loss: 0.8041\n",
            "Batch [28468/28711], Loss: 0.8807\n",
            "Batch [28469/28711], Loss: 0.7292\n",
            "Batch [28470/28711], Loss: 0.8711\n",
            "Batch [28471/28711], Loss: 0.7649\n",
            "Batch [28472/28711], Loss: 0.9320\n",
            "Batch [28473/28711], Loss: 0.6103\n",
            "Batch [28474/28711], Loss: 0.7484\n",
            "Batch [28475/28711], Loss: 1.1053\n",
            "Batch [28476/28711], Loss: 0.6529\n",
            "Batch [28477/28711], Loss: 0.9986\n",
            "Batch [28478/28711], Loss: 0.5215\n",
            "Batch [28479/28711], Loss: 0.9462\n",
            "Batch [28480/28711], Loss: 1.1382\n",
            "Batch [28481/28711], Loss: 0.8319\n",
            "Batch [28482/28711], Loss: 1.1241\n",
            "Batch [28483/28711], Loss: 0.8964\n",
            "Batch [28484/28711], Loss: 0.9578\n",
            "Batch [28485/28711], Loss: 1.0622\n",
            "Batch [28486/28711], Loss: 1.0793\n",
            "Batch [28487/28711], Loss: 0.8952\n",
            "Batch [28488/28711], Loss: 0.9657\n",
            "Batch [28489/28711], Loss: 0.9582\n",
            "Batch [28490/28711], Loss: 0.9459\n",
            "Batch [28491/28711], Loss: 0.7876\n",
            "Batch [28492/28711], Loss: 0.7161\n",
            "Batch [28493/28711], Loss: 0.9986\n",
            "Batch [28494/28711], Loss: 0.9089\n",
            "Batch [28495/28711], Loss: 0.7549\n",
            "Batch [28496/28711], Loss: 0.9044\n",
            "Batch [28497/28711], Loss: 0.7533\n",
            "Batch [28498/28711], Loss: 0.8126\n",
            "Batch [28499/28711], Loss: 0.7463\n",
            "Batch [28500/28711], Loss: 0.9194\n",
            "Batch [28501/28711], Loss: 1.1695\n",
            "Batch [28502/28711], Loss: 0.9362\n",
            "Batch [28503/28711], Loss: 0.8192\n",
            "Batch [28504/28711], Loss: 1.1585\n",
            "Batch [28505/28711], Loss: 0.7423\n",
            "Batch [28506/28711], Loss: 1.1573\n",
            "Batch [28507/28711], Loss: 0.8620\n",
            "Batch [28508/28711], Loss: 0.8012\n",
            "Batch [28509/28711], Loss: 1.1024\n",
            "Batch [28510/28711], Loss: 0.6199\n",
            "Batch [28511/28711], Loss: 0.7753\n",
            "Batch [28512/28711], Loss: 0.9363\n",
            "Batch [28513/28711], Loss: 1.0157\n",
            "Batch [28514/28711], Loss: 1.0962\n",
            "Batch [28515/28711], Loss: 0.8555\n",
            "Batch [28516/28711], Loss: 0.8232\n",
            "Batch [28517/28711], Loss: 1.0783\n",
            "Batch [28518/28711], Loss: 1.0347\n",
            "Batch [28519/28711], Loss: 0.8618\n",
            "Batch [28520/28711], Loss: 0.7465\n",
            "Batch [28521/28711], Loss: 1.2225\n",
            "Batch [28522/28711], Loss: 1.1037\n",
            "Batch [28523/28711], Loss: 1.0908\n",
            "Batch [28524/28711], Loss: 0.8822\n",
            "Batch [28525/28711], Loss: 1.0074\n",
            "Batch [28526/28711], Loss: 0.9349\n",
            "Batch [28527/28711], Loss: 0.8597\n",
            "Batch [28528/28711], Loss: 0.6615\n",
            "Batch [28529/28711], Loss: 1.0081\n",
            "Batch [28530/28711], Loss: 0.6475\n",
            "Batch [28531/28711], Loss: 0.7448\n",
            "Batch [28532/28711], Loss: 1.2278\n",
            "Batch [28533/28711], Loss: 0.7187\n",
            "Batch [28534/28711], Loss: 0.7643\n",
            "Batch [28535/28711], Loss: 0.8695\n",
            "Batch [28536/28711], Loss: 0.8524\n",
            "Batch [28537/28711], Loss: 1.1450\n",
            "Batch [28538/28711], Loss: 0.7067\n",
            "Batch [28539/28711], Loss: 1.0077\n",
            "Batch [28540/28711], Loss: 1.0997\n",
            "Batch [28541/28711], Loss: 0.9805\n",
            "Batch [28542/28711], Loss: 1.0008\n",
            "Batch [28543/28711], Loss: 0.8446\n",
            "Batch [28544/28711], Loss: 0.7607\n",
            "Batch [28545/28711], Loss: 0.9868\n",
            "Batch [28546/28711], Loss: 0.7384\n",
            "Batch [28547/28711], Loss: 0.6303\n",
            "Batch [28548/28711], Loss: 0.9343\n",
            "Batch [28549/28711], Loss: 0.8947\n",
            "Batch [28550/28711], Loss: 0.9464\n",
            "Batch [28551/28711], Loss: 0.8641\n",
            "Batch [28552/28711], Loss: 1.0171\n",
            "Batch [28553/28711], Loss: 1.0987\n",
            "Batch [28554/28711], Loss: 0.9573\n",
            "Batch [28555/28711], Loss: 0.9515\n",
            "Batch [28556/28711], Loss: 0.8977\n",
            "Batch [28557/28711], Loss: 0.8149\n",
            "Batch [28558/28711], Loss: 0.9514\n",
            "Batch [28559/28711], Loss: 0.6350\n",
            "Batch [28560/28711], Loss: 0.8322\n",
            "Batch [28561/28711], Loss: 0.8032\n",
            "Batch [28562/28711], Loss: 0.7916\n",
            "Batch [28563/28711], Loss: 0.9708\n",
            "Batch [28564/28711], Loss: 1.0279\n",
            "Batch [28565/28711], Loss: 1.1452\n",
            "Batch [28566/28711], Loss: 0.9647\n",
            "Batch [28567/28711], Loss: 1.0458\n",
            "Batch [28568/28711], Loss: 0.9273\n",
            "Batch [28569/28711], Loss: 0.7767\n",
            "Batch [28570/28711], Loss: 1.2012\n",
            "Batch [28571/28711], Loss: 1.1038\n",
            "Batch [28572/28711], Loss: 0.9737\n",
            "Batch [28573/28711], Loss: 0.7668\n",
            "Batch [28574/28711], Loss: 0.9521\n",
            "Batch [28575/28711], Loss: 0.9920\n",
            "Batch [28576/28711], Loss: 1.0879\n",
            "Batch [28577/28711], Loss: 0.7832\n",
            "Batch [28578/28711], Loss: 0.9165\n",
            "Batch [28579/28711], Loss: 1.1045\n",
            "Batch [28580/28711], Loss: 0.8175\n",
            "Batch [28581/28711], Loss: 1.1765\n",
            "Batch [28582/28711], Loss: 0.9606\n",
            "Batch [28583/28711], Loss: 0.7592\n",
            "Batch [28584/28711], Loss: 0.8971\n",
            "Batch [28585/28711], Loss: 0.9373\n",
            "Batch [28586/28711], Loss: 1.0663\n",
            "Batch [28587/28711], Loss: 1.2629\n",
            "Batch [28588/28711], Loss: 1.0463\n",
            "Batch [28589/28711], Loss: 0.9717\n",
            "Batch [28590/28711], Loss: 0.6874\n",
            "Batch [28591/28711], Loss: 0.7382\n",
            "Batch [28592/28711], Loss: 1.0950\n",
            "Batch [28593/28711], Loss: 0.6455\n",
            "Batch [28594/28711], Loss: 0.6685\n",
            "Batch [28595/28711], Loss: 0.8566\n",
            "Batch [28596/28711], Loss: 1.0796\n",
            "Batch [28597/28711], Loss: 1.0516\n",
            "Batch [28598/28711], Loss: 0.8939\n",
            "Batch [28599/28711], Loss: 0.7213\n",
            "Batch [28600/28711], Loss: 0.7777\n",
            "Batch [28601/28711], Loss: 1.0797\n",
            "Batch [28602/28711], Loss: 0.6633\n",
            "Batch [28603/28711], Loss: 1.0579\n",
            "Batch [28604/28711], Loss: 0.7048\n",
            "Batch [28605/28711], Loss: 1.0458\n",
            "Batch [28606/28711], Loss: 0.9577\n",
            "Batch [28607/28711], Loss: 0.8744\n",
            "Batch [28608/28711], Loss: 0.9167\n",
            "Batch [28609/28711], Loss: 1.2928\n",
            "Batch [28610/28711], Loss: 0.8339\n",
            "Batch [28611/28711], Loss: 0.9695\n",
            "Batch [28612/28711], Loss: 0.7857\n",
            "Batch [28613/28711], Loss: 0.9561\n",
            "Batch [28614/28711], Loss: 0.8622\n",
            "Batch [28615/28711], Loss: 1.1988\n",
            "Batch [28616/28711], Loss: 0.7600\n",
            "Batch [28617/28711], Loss: 1.0015\n",
            "Batch [28618/28711], Loss: 0.7429\n",
            "Batch [28619/28711], Loss: 0.7812\n",
            "Batch [28620/28711], Loss: 0.8786\n",
            "Batch [28621/28711], Loss: 1.2462\n",
            "Batch [28622/28711], Loss: 1.0296\n",
            "Batch [28623/28711], Loss: 0.8184\n",
            "Batch [28624/28711], Loss: 0.8322\n",
            "Batch [28625/28711], Loss: 1.0811\n",
            "Batch [28626/28711], Loss: 0.8843\n",
            "Batch [28627/28711], Loss: 1.2427\n",
            "Batch [28628/28711], Loss: 0.7223\n",
            "Batch [28629/28711], Loss: 1.1017\n",
            "Batch [28630/28711], Loss: 0.8508\n",
            "Batch [28631/28711], Loss: 0.9547\n",
            "Batch [28632/28711], Loss: 0.7886\n",
            "Batch [28633/28711], Loss: 1.0342\n",
            "Batch [28634/28711], Loss: 0.8929\n",
            "Batch [28635/28711], Loss: 1.0121\n",
            "Batch [28636/28711], Loss: 0.9372\n",
            "Batch [28637/28711], Loss: 0.8499\n",
            "Batch [28638/28711], Loss: 0.9694\n",
            "Batch [28639/28711], Loss: 0.9935\n",
            "Batch [28640/28711], Loss: 0.8467\n",
            "Batch [28641/28711], Loss: 1.2199\n",
            "Batch [28642/28711], Loss: 0.8521\n",
            "Batch [28643/28711], Loss: 0.8069\n",
            "Batch [28644/28711], Loss: 0.9711\n",
            "Batch [28645/28711], Loss: 0.9211\n",
            "Batch [28646/28711], Loss: 1.0173\n",
            "Batch [28647/28711], Loss: 0.6433\n",
            "Batch [28648/28711], Loss: 0.6898\n",
            "Batch [28649/28711], Loss: 0.6733\n",
            "Batch [28650/28711], Loss: 1.0534\n",
            "Batch [28651/28711], Loss: 0.6918\n",
            "Batch [28652/28711], Loss: 0.8284\n",
            "Batch [28653/28711], Loss: 0.8170\n",
            "Batch [28654/28711], Loss: 0.7895\n",
            "Batch [28655/28711], Loss: 0.7072\n",
            "Batch [28656/28711], Loss: 1.0578\n",
            "Batch [28657/28711], Loss: 1.0029\n",
            "Batch [28658/28711], Loss: 1.0751\n",
            "Batch [28659/28711], Loss: 0.8976\n",
            "Batch [28660/28711], Loss: 0.8139\n",
            "Batch [28661/28711], Loss: 1.0442\n",
            "Batch [28662/28711], Loss: 0.8475\n",
            "Batch [28663/28711], Loss: 0.9207\n",
            "Batch [28664/28711], Loss: 1.0922\n",
            "Batch [28665/28711], Loss: 1.1828\n",
            "Batch [28666/28711], Loss: 0.7950\n",
            "Batch [28667/28711], Loss: 0.9608\n",
            "Batch [28668/28711], Loss: 0.8739\n",
            "Batch [28669/28711], Loss: 0.7893\n",
            "Batch [28670/28711], Loss: 1.1248\n",
            "Batch [28671/28711], Loss: 1.1115\n",
            "Batch [28672/28711], Loss: 0.9634\n",
            "Batch [28673/28711], Loss: 1.0110\n",
            "Batch [28674/28711], Loss: 0.7753\n",
            "Batch [28675/28711], Loss: 0.7806\n",
            "Batch [28676/28711], Loss: 0.7685\n",
            "Batch [28677/28711], Loss: 0.9174\n",
            "Batch [28678/28711], Loss: 0.6560\n",
            "Batch [28679/28711], Loss: 1.1706\n",
            "Batch [28680/28711], Loss: 1.0617\n",
            "Batch [28681/28711], Loss: 0.8564\n",
            "Batch [28682/28711], Loss: 1.0946\n",
            "Batch [28683/28711], Loss: 0.9392\n",
            "Batch [28684/28711], Loss: 1.2017\n",
            "Batch [28685/28711], Loss: 0.7228\n",
            "Batch [28686/28711], Loss: 0.9917\n",
            "Batch [28687/28711], Loss: 0.8617\n",
            "Batch [28688/28711], Loss: 0.9790\n",
            "Batch [28689/28711], Loss: 1.2286\n",
            "Batch [28690/28711], Loss: 1.2238\n",
            "Batch [28691/28711], Loss: 1.0906\n",
            "Batch [28692/28711], Loss: 0.9905\n",
            "Batch [28693/28711], Loss: 0.9403\n",
            "Batch [28694/28711], Loss: 0.8002\n",
            "Batch [28695/28711], Loss: 0.9524\n",
            "Batch [28696/28711], Loss: 1.0414\n",
            "Batch [28697/28711], Loss: 1.1309\n",
            "Batch [28698/28711], Loss: 0.8309\n",
            "Batch [28699/28711], Loss: 0.8324\n",
            "Batch [28700/28711], Loss: 1.0684\n",
            "Batch [28701/28711], Loss: 0.6813\n",
            "Batch [28702/28711], Loss: 0.6053\n",
            "Batch [28703/28711], Loss: 1.1122\n",
            "Batch [28704/28711], Loss: 0.8579\n",
            "Batch [28705/28711], Loss: 0.8593\n",
            "Batch [28706/28711], Loss: 1.0241\n",
            "Batch [28707/28711], Loss: 1.0407\n",
            "Batch [28708/28711], Loss: 0.8534\n",
            "Batch [28709/28711], Loss: 0.9938\n",
            "Batch [28710/28711], Loss: 0.9236\n",
            "Batch [28711/28711], Loss: 0.8779\n",
            "Batch [28712/28711], Loss: 1.0896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Model Evaluation:"
      ],
      "metadata": {
        "id": "riEPd8UKwfwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0sfxpvoCVIA",
        "outputId": "33b5fc0a-b602-4b45-9900-19deb064e558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss_values = []\n",
        "val_dataset = load_dataset('cnn_dailymail', '3.0.0', split='validation')\n",
        "batches_val_dataset=int(len(val_dataset)/batch_size)\n",
        "\n",
        "# Extract input texts and target texts from the validation dataset\n",
        "val_input_texts = val_dataset['article']\n",
        "val_target_texts = val_dataset['highlights']\n",
        "\n",
        "# Create a custom dataset for validation\n",
        "val_dataset = CustomDataset(val_input_texts, val_target_texts, tokenizer)\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "val_loss = 0.0\n",
        "for val_batch_ind,val_batch in enumerate(val_loader):\n",
        "    val_batch_input_ids, val_batch_target_ids = val_batch\n",
        "    val_batch_input_ids = val_batch_input_ids.to(device)\n",
        "    val_batch_target_ids = val_batch_target_ids.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(input_ids=val_batch_input_ids, labels=val_batch_target_ids)\n",
        "        val_loss += val_outputs.loss.item() * len(val_batch_input_ids)\n",
        "    print(f'Batch [{val_batch_ind+1}/{batches_val_dataset}], Val Loss: {loss.item():.4f}')\n",
        "\n",
        "    val_loss /= batch_size\n",
        "    val_loss_values.append(val_loss)\n",
        "\n",
        "model.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "z6MENmySwmwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9720f172-fb9c-4998-f2e6-74bf363d252d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch [1/1336], Val Loss: 1.0896\n",
            "Batch [2/1336], Val Loss: 1.0896\n",
            "Batch [3/1336], Val Loss: 1.0896\n",
            "Batch [4/1336], Val Loss: 1.0896\n",
            "Batch [5/1336], Val Loss: 1.0896\n",
            "Batch [6/1336], Val Loss: 1.0896\n",
            "Batch [7/1336], Val Loss: 1.0896\n",
            "Batch [8/1336], Val Loss: 1.0896\n",
            "Batch [9/1336], Val Loss: 1.0896\n",
            "Batch [10/1336], Val Loss: 1.0896\n",
            "Batch [11/1336], Val Loss: 1.0896\n",
            "Batch [12/1336], Val Loss: 1.0896\n",
            "Batch [13/1336], Val Loss: 1.0896\n",
            "Batch [14/1336], Val Loss: 1.0896\n",
            "Batch [15/1336], Val Loss: 1.0896\n",
            "Batch [16/1336], Val Loss: 1.0896\n",
            "Batch [17/1336], Val Loss: 1.0896\n",
            "Batch [18/1336], Val Loss: 1.0896\n",
            "Batch [19/1336], Val Loss: 1.0896\n",
            "Batch [20/1336], Val Loss: 1.0896\n",
            "Batch [21/1336], Val Loss: 1.0896\n",
            "Batch [22/1336], Val Loss: 1.0896\n",
            "Batch [23/1336], Val Loss: 1.0896\n",
            "Batch [24/1336], Val Loss: 1.0896\n",
            "Batch [25/1336], Val Loss: 1.0896\n",
            "Batch [26/1336], Val Loss: 1.0896\n",
            "Batch [27/1336], Val Loss: 1.0896\n",
            "Batch [28/1336], Val Loss: 1.0896\n",
            "Batch [29/1336], Val Loss: 1.0896\n",
            "Batch [30/1336], Val Loss: 1.0896\n",
            "Batch [31/1336], Val Loss: 1.0896\n",
            "Batch [32/1336], Val Loss: 1.0896\n",
            "Batch [33/1336], Val Loss: 1.0896\n",
            "Batch [34/1336], Val Loss: 1.0896\n",
            "Batch [35/1336], Val Loss: 1.0896\n",
            "Batch [36/1336], Val Loss: 1.0896\n",
            "Batch [37/1336], Val Loss: 1.0896\n",
            "Batch [38/1336], Val Loss: 1.0896\n",
            "Batch [39/1336], Val Loss: 1.0896\n",
            "Batch [40/1336], Val Loss: 1.0896\n",
            "Batch [41/1336], Val Loss: 1.0896\n",
            "Batch [42/1336], Val Loss: 1.0896\n",
            "Batch [43/1336], Val Loss: 1.0896\n",
            "Batch [44/1336], Val Loss: 1.0896\n",
            "Batch [45/1336], Val Loss: 1.0896\n",
            "Batch [46/1336], Val Loss: 1.0896\n",
            "Batch [47/1336], Val Loss: 1.0896\n",
            "Batch [48/1336], Val Loss: 1.0896\n",
            "Batch [49/1336], Val Loss: 1.0896\n",
            "Batch [50/1336], Val Loss: 1.0896\n",
            "Batch [51/1336], Val Loss: 1.0896\n",
            "Batch [52/1336], Val Loss: 1.0896\n",
            "Batch [53/1336], Val Loss: 1.0896\n",
            "Batch [54/1336], Val Loss: 1.0896\n",
            "Batch [55/1336], Val Loss: 1.0896\n",
            "Batch [56/1336], Val Loss: 1.0896\n",
            "Batch [57/1336], Val Loss: 1.0896\n",
            "Batch [58/1336], Val Loss: 1.0896\n",
            "Batch [59/1336], Val Loss: 1.0896\n",
            "Batch [60/1336], Val Loss: 1.0896\n",
            "Batch [61/1336], Val Loss: 1.0896\n",
            "Batch [62/1336], Val Loss: 1.0896\n",
            "Batch [63/1336], Val Loss: 1.0896\n",
            "Batch [64/1336], Val Loss: 1.0896\n",
            "Batch [65/1336], Val Loss: 1.0896\n",
            "Batch [66/1336], Val Loss: 1.0896\n",
            "Batch [67/1336], Val Loss: 1.0896\n",
            "Batch [68/1336], Val Loss: 1.0896\n",
            "Batch [69/1336], Val Loss: 1.0896\n",
            "Batch [70/1336], Val Loss: 1.0896\n",
            "Batch [71/1336], Val Loss: 1.0896\n",
            "Batch [72/1336], Val Loss: 1.0896\n",
            "Batch [73/1336], Val Loss: 1.0896\n",
            "Batch [74/1336], Val Loss: 1.0896\n",
            "Batch [75/1336], Val Loss: 1.0896\n",
            "Batch [76/1336], Val Loss: 1.0896\n",
            "Batch [77/1336], Val Loss: 1.0896\n",
            "Batch [78/1336], Val Loss: 1.0896\n",
            "Batch [79/1336], Val Loss: 1.0896\n",
            "Batch [80/1336], Val Loss: 1.0896\n",
            "Batch [81/1336], Val Loss: 1.0896\n",
            "Batch [82/1336], Val Loss: 1.0896\n",
            "Batch [83/1336], Val Loss: 1.0896\n",
            "Batch [84/1336], Val Loss: 1.0896\n",
            "Batch [85/1336], Val Loss: 1.0896\n",
            "Batch [86/1336], Val Loss: 1.0896\n",
            "Batch [87/1336], Val Loss: 1.0896\n",
            "Batch [88/1336], Val Loss: 1.0896\n",
            "Batch [89/1336], Val Loss: 1.0896\n",
            "Batch [90/1336], Val Loss: 1.0896\n",
            "Batch [91/1336], Val Loss: 1.0896\n",
            "Batch [92/1336], Val Loss: 1.0896\n",
            "Batch [93/1336], Val Loss: 1.0896\n",
            "Batch [94/1336], Val Loss: 1.0896\n",
            "Batch [95/1336], Val Loss: 1.0896\n",
            "Batch [96/1336], Val Loss: 1.0896\n",
            "Batch [97/1336], Val Loss: 1.0896\n",
            "Batch [98/1336], Val Loss: 1.0896\n",
            "Batch [99/1336], Val Loss: 1.0896\n",
            "Batch [100/1336], Val Loss: 1.0896\n",
            "Batch [101/1336], Val Loss: 1.0896\n",
            "Batch [102/1336], Val Loss: 1.0896\n",
            "Batch [103/1336], Val Loss: 1.0896\n",
            "Batch [104/1336], Val Loss: 1.0896\n",
            "Batch [105/1336], Val Loss: 1.0896\n",
            "Batch [106/1336], Val Loss: 1.0896\n",
            "Batch [107/1336], Val Loss: 1.0896\n",
            "Batch [108/1336], Val Loss: 1.0896\n",
            "Batch [109/1336], Val Loss: 1.0896\n",
            "Batch [110/1336], Val Loss: 1.0896\n",
            "Batch [111/1336], Val Loss: 1.0896\n",
            "Batch [112/1336], Val Loss: 1.0896\n",
            "Batch [113/1336], Val Loss: 1.0896\n",
            "Batch [114/1336], Val Loss: 1.0896\n",
            "Batch [115/1336], Val Loss: 1.0896\n",
            "Batch [116/1336], Val Loss: 1.0896\n",
            "Batch [117/1336], Val Loss: 1.0896\n",
            "Batch [118/1336], Val Loss: 1.0896\n",
            "Batch [119/1336], Val Loss: 1.0896\n",
            "Batch [120/1336], Val Loss: 1.0896\n",
            "Batch [121/1336], Val Loss: 1.0896\n",
            "Batch [122/1336], Val Loss: 1.0896\n",
            "Batch [123/1336], Val Loss: 1.0896\n",
            "Batch [124/1336], Val Loss: 1.0896\n",
            "Batch [125/1336], Val Loss: 1.0896\n",
            "Batch [126/1336], Val Loss: 1.0896\n",
            "Batch [127/1336], Val Loss: 1.0896\n",
            "Batch [128/1336], Val Loss: 1.0896\n",
            "Batch [129/1336], Val Loss: 1.0896\n",
            "Batch [130/1336], Val Loss: 1.0896\n",
            "Batch [131/1336], Val Loss: 1.0896\n",
            "Batch [132/1336], Val Loss: 1.0896\n",
            "Batch [133/1336], Val Loss: 1.0896\n",
            "Batch [134/1336], Val Loss: 1.0896\n",
            "Batch [135/1336], Val Loss: 1.0896\n",
            "Batch [136/1336], Val Loss: 1.0896\n",
            "Batch [137/1336], Val Loss: 1.0896\n",
            "Batch [138/1336], Val Loss: 1.0896\n",
            "Batch [139/1336], Val Loss: 1.0896\n",
            "Batch [140/1336], Val Loss: 1.0896\n",
            "Batch [141/1336], Val Loss: 1.0896\n",
            "Batch [142/1336], Val Loss: 1.0896\n",
            "Batch [143/1336], Val Loss: 1.0896\n",
            "Batch [144/1336], Val Loss: 1.0896\n",
            "Batch [145/1336], Val Loss: 1.0896\n",
            "Batch [146/1336], Val Loss: 1.0896\n",
            "Batch [147/1336], Val Loss: 1.0896\n",
            "Batch [148/1336], Val Loss: 1.0896\n",
            "Batch [149/1336], Val Loss: 1.0896\n",
            "Batch [150/1336], Val Loss: 1.0896\n",
            "Batch [151/1336], Val Loss: 1.0896\n",
            "Batch [152/1336], Val Loss: 1.0896\n",
            "Batch [153/1336], Val Loss: 1.0896\n",
            "Batch [154/1336], Val Loss: 1.0896\n",
            "Batch [155/1336], Val Loss: 1.0896\n",
            "Batch [156/1336], Val Loss: 1.0896\n",
            "Batch [157/1336], Val Loss: 1.0896\n",
            "Batch [158/1336], Val Loss: 1.0896\n",
            "Batch [159/1336], Val Loss: 1.0896\n",
            "Batch [160/1336], Val Loss: 1.0896\n",
            "Batch [161/1336], Val Loss: 1.0896\n",
            "Batch [162/1336], Val Loss: 1.0896\n",
            "Batch [163/1336], Val Loss: 1.0896\n",
            "Batch [164/1336], Val Loss: 1.0896\n",
            "Batch [165/1336], Val Loss: 1.0896\n",
            "Batch [166/1336], Val Loss: 1.0896\n",
            "Batch [167/1336], Val Loss: 1.0896\n",
            "Batch [168/1336], Val Loss: 1.0896\n",
            "Batch [169/1336], Val Loss: 1.0896\n",
            "Batch [170/1336], Val Loss: 1.0896\n",
            "Batch [171/1336], Val Loss: 1.0896\n",
            "Batch [172/1336], Val Loss: 1.0896\n",
            "Batch [173/1336], Val Loss: 1.0896\n",
            "Batch [174/1336], Val Loss: 1.0896\n",
            "Batch [175/1336], Val Loss: 1.0896\n",
            "Batch [176/1336], Val Loss: 1.0896\n",
            "Batch [177/1336], Val Loss: 1.0896\n",
            "Batch [178/1336], Val Loss: 1.0896\n",
            "Batch [179/1336], Val Loss: 1.0896\n",
            "Batch [180/1336], Val Loss: 1.0896\n",
            "Batch [181/1336], Val Loss: 1.0896\n",
            "Batch [182/1336], Val Loss: 1.0896\n",
            "Batch [183/1336], Val Loss: 1.0896\n",
            "Batch [184/1336], Val Loss: 1.0896\n",
            "Batch [185/1336], Val Loss: 1.0896\n",
            "Batch [186/1336], Val Loss: 1.0896\n",
            "Batch [187/1336], Val Loss: 1.0896\n",
            "Batch [188/1336], Val Loss: 1.0896\n",
            "Batch [189/1336], Val Loss: 1.0896\n",
            "Batch [190/1336], Val Loss: 1.0896\n",
            "Batch [191/1336], Val Loss: 1.0896\n",
            "Batch [192/1336], Val Loss: 1.0896\n",
            "Batch [193/1336], Val Loss: 1.0896\n",
            "Batch [194/1336], Val Loss: 1.0896\n",
            "Batch [195/1336], Val Loss: 1.0896\n",
            "Batch [196/1336], Val Loss: 1.0896\n",
            "Batch [197/1336], Val Loss: 1.0896\n",
            "Batch [198/1336], Val Loss: 1.0896\n",
            "Batch [199/1336], Val Loss: 1.0896\n",
            "Batch [200/1336], Val Loss: 1.0896\n",
            "Batch [201/1336], Val Loss: 1.0896\n",
            "Batch [202/1336], Val Loss: 1.0896\n",
            "Batch [203/1336], Val Loss: 1.0896\n",
            "Batch [204/1336], Val Loss: 1.0896\n",
            "Batch [205/1336], Val Loss: 1.0896\n",
            "Batch [206/1336], Val Loss: 1.0896\n",
            "Batch [207/1336], Val Loss: 1.0896\n",
            "Batch [208/1336], Val Loss: 1.0896\n",
            "Batch [209/1336], Val Loss: 1.0896\n",
            "Batch [210/1336], Val Loss: 1.0896\n",
            "Batch [211/1336], Val Loss: 1.0896\n",
            "Batch [212/1336], Val Loss: 1.0896\n",
            "Batch [213/1336], Val Loss: 1.0896\n",
            "Batch [214/1336], Val Loss: 1.0896\n",
            "Batch [215/1336], Val Loss: 1.0896\n",
            "Batch [216/1336], Val Loss: 1.0896\n",
            "Batch [217/1336], Val Loss: 1.0896\n",
            "Batch [218/1336], Val Loss: 1.0896\n",
            "Batch [219/1336], Val Loss: 1.0896\n",
            "Batch [220/1336], Val Loss: 1.0896\n",
            "Batch [221/1336], Val Loss: 1.0896\n",
            "Batch [222/1336], Val Loss: 1.0896\n",
            "Batch [223/1336], Val Loss: 1.0896\n",
            "Batch [224/1336], Val Loss: 1.0896\n",
            "Batch [225/1336], Val Loss: 1.0896\n",
            "Batch [226/1336], Val Loss: 1.0896\n",
            "Batch [227/1336], Val Loss: 1.0896\n",
            "Batch [228/1336], Val Loss: 1.0896\n",
            "Batch [229/1336], Val Loss: 1.0896\n",
            "Batch [230/1336], Val Loss: 1.0896\n",
            "Batch [231/1336], Val Loss: 1.0896\n",
            "Batch [232/1336], Val Loss: 1.0896\n",
            "Batch [233/1336], Val Loss: 1.0896\n",
            "Batch [234/1336], Val Loss: 1.0896\n",
            "Batch [235/1336], Val Loss: 1.0896\n",
            "Batch [236/1336], Val Loss: 1.0896\n",
            "Batch [237/1336], Val Loss: 1.0896\n",
            "Batch [238/1336], Val Loss: 1.0896\n",
            "Batch [239/1336], Val Loss: 1.0896\n",
            "Batch [240/1336], Val Loss: 1.0896\n",
            "Batch [241/1336], Val Loss: 1.0896\n",
            "Batch [242/1336], Val Loss: 1.0896\n",
            "Batch [243/1336], Val Loss: 1.0896\n",
            "Batch [244/1336], Val Loss: 1.0896\n",
            "Batch [245/1336], Val Loss: 1.0896\n",
            "Batch [246/1336], Val Loss: 1.0896\n",
            "Batch [247/1336], Val Loss: 1.0896\n",
            "Batch [248/1336], Val Loss: 1.0896\n",
            "Batch [249/1336], Val Loss: 1.0896\n",
            "Batch [250/1336], Val Loss: 1.0896\n",
            "Batch [251/1336], Val Loss: 1.0896\n",
            "Batch [252/1336], Val Loss: 1.0896\n",
            "Batch [253/1336], Val Loss: 1.0896\n",
            "Batch [254/1336], Val Loss: 1.0896\n",
            "Batch [255/1336], Val Loss: 1.0896\n",
            "Batch [256/1336], Val Loss: 1.0896\n",
            "Batch [257/1336], Val Loss: 1.0896\n",
            "Batch [258/1336], Val Loss: 1.0896\n",
            "Batch [259/1336], Val Loss: 1.0896\n",
            "Batch [260/1336], Val Loss: 1.0896\n",
            "Batch [261/1336], Val Loss: 1.0896\n",
            "Batch [262/1336], Val Loss: 1.0896\n",
            "Batch [263/1336], Val Loss: 1.0896\n",
            "Batch [264/1336], Val Loss: 1.0896\n",
            "Batch [265/1336], Val Loss: 1.0896\n",
            "Batch [266/1336], Val Loss: 1.0896\n",
            "Batch [267/1336], Val Loss: 1.0896\n",
            "Batch [268/1336], Val Loss: 1.0896\n",
            "Batch [269/1336], Val Loss: 1.0896\n",
            "Batch [270/1336], Val Loss: 1.0896\n",
            "Batch [271/1336], Val Loss: 1.0896\n",
            "Batch [272/1336], Val Loss: 1.0896\n",
            "Batch [273/1336], Val Loss: 1.0896\n",
            "Batch [274/1336], Val Loss: 1.0896\n",
            "Batch [275/1336], Val Loss: 1.0896\n",
            "Batch [276/1336], Val Loss: 1.0896\n",
            "Batch [277/1336], Val Loss: 1.0896\n",
            "Batch [278/1336], Val Loss: 1.0896\n",
            "Batch [279/1336], Val Loss: 1.0896\n",
            "Batch [280/1336], Val Loss: 1.0896\n",
            "Batch [281/1336], Val Loss: 1.0896\n",
            "Batch [282/1336], Val Loss: 1.0896\n",
            "Batch [283/1336], Val Loss: 1.0896\n",
            "Batch [284/1336], Val Loss: 1.0896\n",
            "Batch [285/1336], Val Loss: 1.0896\n",
            "Batch [286/1336], Val Loss: 1.0896\n",
            "Batch [287/1336], Val Loss: 1.0896\n",
            "Batch [288/1336], Val Loss: 1.0896\n",
            "Batch [289/1336], Val Loss: 1.0896\n",
            "Batch [290/1336], Val Loss: 1.0896\n",
            "Batch [291/1336], Val Loss: 1.0896\n",
            "Batch [292/1336], Val Loss: 1.0896\n",
            "Batch [293/1336], Val Loss: 1.0896\n",
            "Batch [294/1336], Val Loss: 1.0896\n",
            "Batch [295/1336], Val Loss: 1.0896\n",
            "Batch [296/1336], Val Loss: 1.0896\n",
            "Batch [297/1336], Val Loss: 1.0896\n",
            "Batch [298/1336], Val Loss: 1.0896\n",
            "Batch [299/1336], Val Loss: 1.0896\n",
            "Batch [300/1336], Val Loss: 1.0896\n",
            "Batch [301/1336], Val Loss: 1.0896\n",
            "Batch [302/1336], Val Loss: 1.0896\n",
            "Batch [303/1336], Val Loss: 1.0896\n",
            "Batch [304/1336], Val Loss: 1.0896\n",
            "Batch [305/1336], Val Loss: 1.0896\n",
            "Batch [306/1336], Val Loss: 1.0896\n",
            "Batch [307/1336], Val Loss: 1.0896\n",
            "Batch [308/1336], Val Loss: 1.0896\n",
            "Batch [309/1336], Val Loss: 1.0896\n",
            "Batch [310/1336], Val Loss: 1.0896\n",
            "Batch [311/1336], Val Loss: 1.0896\n",
            "Batch [312/1336], Val Loss: 1.0896\n",
            "Batch [313/1336], Val Loss: 1.0896\n",
            "Batch [314/1336], Val Loss: 1.0896\n",
            "Batch [315/1336], Val Loss: 1.0896\n",
            "Batch [316/1336], Val Loss: 1.0896\n",
            "Batch [317/1336], Val Loss: 1.0896\n",
            "Batch [318/1336], Val Loss: 1.0896\n",
            "Batch [319/1336], Val Loss: 1.0896\n",
            "Batch [320/1336], Val Loss: 1.0896\n",
            "Batch [321/1336], Val Loss: 1.0896\n",
            "Batch [322/1336], Val Loss: 1.0896\n",
            "Batch [323/1336], Val Loss: 1.0896\n",
            "Batch [324/1336], Val Loss: 1.0896\n",
            "Batch [325/1336], Val Loss: 1.0896\n",
            "Batch [326/1336], Val Loss: 1.0896\n",
            "Batch [327/1336], Val Loss: 1.0896\n",
            "Batch [328/1336], Val Loss: 1.0896\n",
            "Batch [329/1336], Val Loss: 1.0896\n",
            "Batch [330/1336], Val Loss: 1.0896\n",
            "Batch [331/1336], Val Loss: 1.0896\n",
            "Batch [332/1336], Val Loss: 1.0896\n",
            "Batch [333/1336], Val Loss: 1.0896\n",
            "Batch [334/1336], Val Loss: 1.0896\n",
            "Batch [335/1336], Val Loss: 1.0896\n",
            "Batch [336/1336], Val Loss: 1.0896\n",
            "Batch [337/1336], Val Loss: 1.0896\n",
            "Batch [338/1336], Val Loss: 1.0896\n",
            "Batch [339/1336], Val Loss: 1.0896\n",
            "Batch [340/1336], Val Loss: 1.0896\n",
            "Batch [341/1336], Val Loss: 1.0896\n",
            "Batch [342/1336], Val Loss: 1.0896\n",
            "Batch [343/1336], Val Loss: 1.0896\n",
            "Batch [344/1336], Val Loss: 1.0896\n",
            "Batch [345/1336], Val Loss: 1.0896\n",
            "Batch [346/1336], Val Loss: 1.0896\n",
            "Batch [347/1336], Val Loss: 1.0896\n",
            "Batch [348/1336], Val Loss: 1.0896\n",
            "Batch [349/1336], Val Loss: 1.0896\n",
            "Batch [350/1336], Val Loss: 1.0896\n",
            "Batch [351/1336], Val Loss: 1.0896\n",
            "Batch [352/1336], Val Loss: 1.0896\n",
            "Batch [353/1336], Val Loss: 1.0896\n",
            "Batch [354/1336], Val Loss: 1.0896\n",
            "Batch [355/1336], Val Loss: 1.0896\n",
            "Batch [356/1336], Val Loss: 1.0896\n",
            "Batch [357/1336], Val Loss: 1.0896\n",
            "Batch [358/1336], Val Loss: 1.0896\n",
            "Batch [359/1336], Val Loss: 1.0896\n",
            "Batch [360/1336], Val Loss: 1.0896\n",
            "Batch [361/1336], Val Loss: 1.0896\n",
            "Batch [362/1336], Val Loss: 1.0896\n",
            "Batch [363/1336], Val Loss: 1.0896\n",
            "Batch [364/1336], Val Loss: 1.0896\n",
            "Batch [365/1336], Val Loss: 1.0896\n",
            "Batch [366/1336], Val Loss: 1.0896\n",
            "Batch [367/1336], Val Loss: 1.0896\n",
            "Batch [368/1336], Val Loss: 1.0896\n",
            "Batch [369/1336], Val Loss: 1.0896\n",
            "Batch [370/1336], Val Loss: 1.0896\n",
            "Batch [371/1336], Val Loss: 1.0896\n",
            "Batch [372/1336], Val Loss: 1.0896\n",
            "Batch [373/1336], Val Loss: 1.0896\n",
            "Batch [374/1336], Val Loss: 1.0896\n",
            "Batch [375/1336], Val Loss: 1.0896\n",
            "Batch [376/1336], Val Loss: 1.0896\n",
            "Batch [377/1336], Val Loss: 1.0896\n",
            "Batch [378/1336], Val Loss: 1.0896\n",
            "Batch [379/1336], Val Loss: 1.0896\n",
            "Batch [380/1336], Val Loss: 1.0896\n",
            "Batch [381/1336], Val Loss: 1.0896\n",
            "Batch [382/1336], Val Loss: 1.0896\n",
            "Batch [383/1336], Val Loss: 1.0896\n",
            "Batch [384/1336], Val Loss: 1.0896\n",
            "Batch [385/1336], Val Loss: 1.0896\n",
            "Batch [386/1336], Val Loss: 1.0896\n",
            "Batch [387/1336], Val Loss: 1.0896\n",
            "Batch [388/1336], Val Loss: 1.0896\n",
            "Batch [389/1336], Val Loss: 1.0896\n",
            "Batch [390/1336], Val Loss: 1.0896\n",
            "Batch [391/1336], Val Loss: 1.0896\n",
            "Batch [392/1336], Val Loss: 1.0896\n",
            "Batch [393/1336], Val Loss: 1.0896\n",
            "Batch [394/1336], Val Loss: 1.0896\n",
            "Batch [395/1336], Val Loss: 1.0896\n",
            "Batch [396/1336], Val Loss: 1.0896\n",
            "Batch [397/1336], Val Loss: 1.0896\n",
            "Batch [398/1336], Val Loss: 1.0896\n",
            "Batch [399/1336], Val Loss: 1.0896\n",
            "Batch [400/1336], Val Loss: 1.0896\n",
            "Batch [401/1336], Val Loss: 1.0896\n",
            "Batch [402/1336], Val Loss: 1.0896\n",
            "Batch [403/1336], Val Loss: 1.0896\n",
            "Batch [404/1336], Val Loss: 1.0896\n",
            "Batch [405/1336], Val Loss: 1.0896\n",
            "Batch [406/1336], Val Loss: 1.0896\n",
            "Batch [407/1336], Val Loss: 1.0896\n",
            "Batch [408/1336], Val Loss: 1.0896\n",
            "Batch [409/1336], Val Loss: 1.0896\n",
            "Batch [410/1336], Val Loss: 1.0896\n",
            "Batch [411/1336], Val Loss: 1.0896\n",
            "Batch [412/1336], Val Loss: 1.0896\n",
            "Batch [413/1336], Val Loss: 1.0896\n",
            "Batch [414/1336], Val Loss: 1.0896\n",
            "Batch [415/1336], Val Loss: 1.0896\n",
            "Batch [416/1336], Val Loss: 1.0896\n",
            "Batch [417/1336], Val Loss: 1.0896\n",
            "Batch [418/1336], Val Loss: 1.0896\n",
            "Batch [419/1336], Val Loss: 1.0896\n",
            "Batch [420/1336], Val Loss: 1.0896\n",
            "Batch [421/1336], Val Loss: 1.0896\n",
            "Batch [422/1336], Val Loss: 1.0896\n",
            "Batch [423/1336], Val Loss: 1.0896\n",
            "Batch [424/1336], Val Loss: 1.0896\n",
            "Batch [425/1336], Val Loss: 1.0896\n",
            "Batch [426/1336], Val Loss: 1.0896\n",
            "Batch [427/1336], Val Loss: 1.0896\n",
            "Batch [428/1336], Val Loss: 1.0896\n",
            "Batch [429/1336], Val Loss: 1.0896\n",
            "Batch [430/1336], Val Loss: 1.0896\n",
            "Batch [431/1336], Val Loss: 1.0896\n",
            "Batch [432/1336], Val Loss: 1.0896\n",
            "Batch [433/1336], Val Loss: 1.0896\n",
            "Batch [434/1336], Val Loss: 1.0896\n",
            "Batch [435/1336], Val Loss: 1.0896\n",
            "Batch [436/1336], Val Loss: 1.0896\n",
            "Batch [437/1336], Val Loss: 1.0896\n",
            "Batch [438/1336], Val Loss: 1.0896\n",
            "Batch [439/1336], Val Loss: 1.0896\n",
            "Batch [440/1336], Val Loss: 1.0896\n",
            "Batch [441/1336], Val Loss: 1.0896\n",
            "Batch [442/1336], Val Loss: 1.0896\n",
            "Batch [443/1336], Val Loss: 1.0896\n",
            "Batch [444/1336], Val Loss: 1.0896\n",
            "Batch [445/1336], Val Loss: 1.0896\n",
            "Batch [446/1336], Val Loss: 1.0896\n",
            "Batch [447/1336], Val Loss: 1.0896\n",
            "Batch [448/1336], Val Loss: 1.0896\n",
            "Batch [449/1336], Val Loss: 1.0896\n",
            "Batch [450/1336], Val Loss: 1.0896\n",
            "Batch [451/1336], Val Loss: 1.0896\n",
            "Batch [452/1336], Val Loss: 1.0896\n",
            "Batch [453/1336], Val Loss: 1.0896\n",
            "Batch [454/1336], Val Loss: 1.0896\n",
            "Batch [455/1336], Val Loss: 1.0896\n",
            "Batch [456/1336], Val Loss: 1.0896\n",
            "Batch [457/1336], Val Loss: 1.0896\n",
            "Batch [458/1336], Val Loss: 1.0896\n",
            "Batch [459/1336], Val Loss: 1.0896\n",
            "Batch [460/1336], Val Loss: 1.0896\n",
            "Batch [461/1336], Val Loss: 1.0896\n",
            "Batch [462/1336], Val Loss: 1.0896\n",
            "Batch [463/1336], Val Loss: 1.0896\n",
            "Batch [464/1336], Val Loss: 1.0896\n",
            "Batch [465/1336], Val Loss: 1.0896\n",
            "Batch [466/1336], Val Loss: 1.0896\n",
            "Batch [467/1336], Val Loss: 1.0896\n",
            "Batch [468/1336], Val Loss: 1.0896\n",
            "Batch [469/1336], Val Loss: 1.0896\n",
            "Batch [470/1336], Val Loss: 1.0896\n",
            "Batch [471/1336], Val Loss: 1.0896\n",
            "Batch [472/1336], Val Loss: 1.0896\n",
            "Batch [473/1336], Val Loss: 1.0896\n",
            "Batch [474/1336], Val Loss: 1.0896\n",
            "Batch [475/1336], Val Loss: 1.0896\n",
            "Batch [476/1336], Val Loss: 1.0896\n",
            "Batch [477/1336], Val Loss: 1.0896\n",
            "Batch [478/1336], Val Loss: 1.0896\n",
            "Batch [479/1336], Val Loss: 1.0896\n",
            "Batch [480/1336], Val Loss: 1.0896\n",
            "Batch [481/1336], Val Loss: 1.0896\n",
            "Batch [482/1336], Val Loss: 1.0896\n",
            "Batch [483/1336], Val Loss: 1.0896\n",
            "Batch [484/1336], Val Loss: 1.0896\n",
            "Batch [485/1336], Val Loss: 1.0896\n",
            "Batch [486/1336], Val Loss: 1.0896\n",
            "Batch [487/1336], Val Loss: 1.0896\n",
            "Batch [488/1336], Val Loss: 1.0896\n",
            "Batch [489/1336], Val Loss: 1.0896\n",
            "Batch [490/1336], Val Loss: 1.0896\n",
            "Batch [491/1336], Val Loss: 1.0896\n",
            "Batch [492/1336], Val Loss: 1.0896\n",
            "Batch [493/1336], Val Loss: 1.0896\n",
            "Batch [494/1336], Val Loss: 1.0896\n",
            "Batch [495/1336], Val Loss: 1.0896\n",
            "Batch [496/1336], Val Loss: 1.0896\n",
            "Batch [497/1336], Val Loss: 1.0896\n",
            "Batch [498/1336], Val Loss: 1.0896\n",
            "Batch [499/1336], Val Loss: 1.0896\n",
            "Batch [500/1336], Val Loss: 1.0896\n",
            "Batch [501/1336], Val Loss: 1.0896\n",
            "Batch [502/1336], Val Loss: 1.0896\n",
            "Batch [503/1336], Val Loss: 1.0896\n",
            "Batch [504/1336], Val Loss: 1.0896\n",
            "Batch [505/1336], Val Loss: 1.0896\n",
            "Batch [506/1336], Val Loss: 1.0896\n",
            "Batch [507/1336], Val Loss: 1.0896\n",
            "Batch [508/1336], Val Loss: 1.0896\n",
            "Batch [509/1336], Val Loss: 1.0896\n",
            "Batch [510/1336], Val Loss: 1.0896\n",
            "Batch [511/1336], Val Loss: 1.0896\n",
            "Batch [512/1336], Val Loss: 1.0896\n",
            "Batch [513/1336], Val Loss: 1.0896\n",
            "Batch [514/1336], Val Loss: 1.0896\n",
            "Batch [515/1336], Val Loss: 1.0896\n",
            "Batch [516/1336], Val Loss: 1.0896\n",
            "Batch [517/1336], Val Loss: 1.0896\n",
            "Batch [518/1336], Val Loss: 1.0896\n",
            "Batch [519/1336], Val Loss: 1.0896\n",
            "Batch [520/1336], Val Loss: 1.0896\n",
            "Batch [521/1336], Val Loss: 1.0896\n",
            "Batch [522/1336], Val Loss: 1.0896\n",
            "Batch [523/1336], Val Loss: 1.0896\n",
            "Batch [524/1336], Val Loss: 1.0896\n",
            "Batch [525/1336], Val Loss: 1.0896\n",
            "Batch [526/1336], Val Loss: 1.0896\n",
            "Batch [527/1336], Val Loss: 1.0896\n",
            "Batch [528/1336], Val Loss: 1.0896\n",
            "Batch [529/1336], Val Loss: 1.0896\n",
            "Batch [530/1336], Val Loss: 1.0896\n",
            "Batch [531/1336], Val Loss: 1.0896\n",
            "Batch [532/1336], Val Loss: 1.0896\n",
            "Batch [533/1336], Val Loss: 1.0896\n",
            "Batch [534/1336], Val Loss: 1.0896\n",
            "Batch [535/1336], Val Loss: 1.0896\n",
            "Batch [536/1336], Val Loss: 1.0896\n",
            "Batch [537/1336], Val Loss: 1.0896\n",
            "Batch [538/1336], Val Loss: 1.0896\n",
            "Batch [539/1336], Val Loss: 1.0896\n",
            "Batch [540/1336], Val Loss: 1.0896\n",
            "Batch [541/1336], Val Loss: 1.0896\n",
            "Batch [542/1336], Val Loss: 1.0896\n",
            "Batch [543/1336], Val Loss: 1.0896\n",
            "Batch [544/1336], Val Loss: 1.0896\n",
            "Batch [545/1336], Val Loss: 1.0896\n",
            "Batch [546/1336], Val Loss: 1.0896\n",
            "Batch [547/1336], Val Loss: 1.0896\n",
            "Batch [548/1336], Val Loss: 1.0896\n",
            "Batch [549/1336], Val Loss: 1.0896\n",
            "Batch [550/1336], Val Loss: 1.0896\n",
            "Batch [551/1336], Val Loss: 1.0896\n",
            "Batch [552/1336], Val Loss: 1.0896\n",
            "Batch [553/1336], Val Loss: 1.0896\n",
            "Batch [554/1336], Val Loss: 1.0896\n",
            "Batch [555/1336], Val Loss: 1.0896\n",
            "Batch [556/1336], Val Loss: 1.0896\n",
            "Batch [557/1336], Val Loss: 1.0896\n",
            "Batch [558/1336], Val Loss: 1.0896\n",
            "Batch [559/1336], Val Loss: 1.0896\n",
            "Batch [560/1336], Val Loss: 1.0896\n",
            "Batch [561/1336], Val Loss: 1.0896\n",
            "Batch [562/1336], Val Loss: 1.0896\n",
            "Batch [563/1336], Val Loss: 1.0896\n",
            "Batch [564/1336], Val Loss: 1.0896\n",
            "Batch [565/1336], Val Loss: 1.0896\n",
            "Batch [566/1336], Val Loss: 1.0896\n",
            "Batch [567/1336], Val Loss: 1.0896\n",
            "Batch [568/1336], Val Loss: 1.0896\n",
            "Batch [569/1336], Val Loss: 1.0896\n",
            "Batch [570/1336], Val Loss: 1.0896\n",
            "Batch [571/1336], Val Loss: 1.0896\n",
            "Batch [572/1336], Val Loss: 1.0896\n",
            "Batch [573/1336], Val Loss: 1.0896\n",
            "Batch [574/1336], Val Loss: 1.0896\n",
            "Batch [575/1336], Val Loss: 1.0896\n",
            "Batch [576/1336], Val Loss: 1.0896\n",
            "Batch [577/1336], Val Loss: 1.0896\n",
            "Batch [578/1336], Val Loss: 1.0896\n",
            "Batch [579/1336], Val Loss: 1.0896\n",
            "Batch [580/1336], Val Loss: 1.0896\n",
            "Batch [581/1336], Val Loss: 1.0896\n",
            "Batch [582/1336], Val Loss: 1.0896\n",
            "Batch [583/1336], Val Loss: 1.0896\n",
            "Batch [584/1336], Val Loss: 1.0896\n",
            "Batch [585/1336], Val Loss: 1.0896\n",
            "Batch [586/1336], Val Loss: 1.0896\n",
            "Batch [587/1336], Val Loss: 1.0896\n",
            "Batch [588/1336], Val Loss: 1.0896\n",
            "Batch [589/1336], Val Loss: 1.0896\n",
            "Batch [590/1336], Val Loss: 1.0896\n",
            "Batch [591/1336], Val Loss: 1.0896\n",
            "Batch [592/1336], Val Loss: 1.0896\n",
            "Batch [593/1336], Val Loss: 1.0896\n",
            "Batch [594/1336], Val Loss: 1.0896\n",
            "Batch [595/1336], Val Loss: 1.0896\n",
            "Batch [596/1336], Val Loss: 1.0896\n",
            "Batch [597/1336], Val Loss: 1.0896\n",
            "Batch [598/1336], Val Loss: 1.0896\n",
            "Batch [599/1336], Val Loss: 1.0896\n",
            "Batch [600/1336], Val Loss: 1.0896\n",
            "Batch [601/1336], Val Loss: 1.0896\n",
            "Batch [602/1336], Val Loss: 1.0896\n",
            "Batch [603/1336], Val Loss: 1.0896\n",
            "Batch [604/1336], Val Loss: 1.0896\n",
            "Batch [605/1336], Val Loss: 1.0896\n",
            "Batch [606/1336], Val Loss: 1.0896\n",
            "Batch [607/1336], Val Loss: 1.0896\n",
            "Batch [608/1336], Val Loss: 1.0896\n",
            "Batch [609/1336], Val Loss: 1.0896\n",
            "Batch [610/1336], Val Loss: 1.0896\n",
            "Batch [611/1336], Val Loss: 1.0896\n",
            "Batch [612/1336], Val Loss: 1.0896\n",
            "Batch [613/1336], Val Loss: 1.0896\n",
            "Batch [614/1336], Val Loss: 1.0896\n",
            "Batch [615/1336], Val Loss: 1.0896\n",
            "Batch [616/1336], Val Loss: 1.0896\n",
            "Batch [617/1336], Val Loss: 1.0896\n",
            "Batch [618/1336], Val Loss: 1.0896\n",
            "Batch [619/1336], Val Loss: 1.0896\n",
            "Batch [620/1336], Val Loss: 1.0896\n",
            "Batch [621/1336], Val Loss: 1.0896\n",
            "Batch [622/1336], Val Loss: 1.0896\n",
            "Batch [623/1336], Val Loss: 1.0896\n",
            "Batch [624/1336], Val Loss: 1.0896\n",
            "Batch [625/1336], Val Loss: 1.0896\n",
            "Batch [626/1336], Val Loss: 1.0896\n",
            "Batch [627/1336], Val Loss: 1.0896\n",
            "Batch [628/1336], Val Loss: 1.0896\n",
            "Batch [629/1336], Val Loss: 1.0896\n",
            "Batch [630/1336], Val Loss: 1.0896\n",
            "Batch [631/1336], Val Loss: 1.0896\n",
            "Batch [632/1336], Val Loss: 1.0896\n",
            "Batch [633/1336], Val Loss: 1.0896\n",
            "Batch [634/1336], Val Loss: 1.0896\n",
            "Batch [635/1336], Val Loss: 1.0896\n",
            "Batch [636/1336], Val Loss: 1.0896\n",
            "Batch [637/1336], Val Loss: 1.0896\n",
            "Batch [638/1336], Val Loss: 1.0896\n",
            "Batch [639/1336], Val Loss: 1.0896\n",
            "Batch [640/1336], Val Loss: 1.0896\n",
            "Batch [641/1336], Val Loss: 1.0896\n",
            "Batch [642/1336], Val Loss: 1.0896\n",
            "Batch [643/1336], Val Loss: 1.0896\n",
            "Batch [644/1336], Val Loss: 1.0896\n",
            "Batch [645/1336], Val Loss: 1.0896\n",
            "Batch [646/1336], Val Loss: 1.0896\n",
            "Batch [647/1336], Val Loss: 1.0896\n",
            "Batch [648/1336], Val Loss: 1.0896\n",
            "Batch [649/1336], Val Loss: 1.0896\n",
            "Batch [650/1336], Val Loss: 1.0896\n",
            "Batch [651/1336], Val Loss: 1.0896\n",
            "Batch [652/1336], Val Loss: 1.0896\n",
            "Batch [653/1336], Val Loss: 1.0896\n",
            "Batch [654/1336], Val Loss: 1.0896\n",
            "Batch [655/1336], Val Loss: 1.0896\n",
            "Batch [656/1336], Val Loss: 1.0896\n",
            "Batch [657/1336], Val Loss: 1.0896\n",
            "Batch [658/1336], Val Loss: 1.0896\n",
            "Batch [659/1336], Val Loss: 1.0896\n",
            "Batch [660/1336], Val Loss: 1.0896\n",
            "Batch [661/1336], Val Loss: 1.0896\n",
            "Batch [662/1336], Val Loss: 1.0896\n",
            "Batch [663/1336], Val Loss: 1.0896\n",
            "Batch [664/1336], Val Loss: 1.0896\n",
            "Batch [665/1336], Val Loss: 1.0896\n",
            "Batch [666/1336], Val Loss: 1.0896\n",
            "Batch [667/1336], Val Loss: 1.0896\n",
            "Batch [668/1336], Val Loss: 1.0896\n",
            "Batch [669/1336], Val Loss: 1.0896\n",
            "Batch [670/1336], Val Loss: 1.0896\n",
            "Batch [671/1336], Val Loss: 1.0896\n",
            "Batch [672/1336], Val Loss: 1.0896\n",
            "Batch [673/1336], Val Loss: 1.0896\n",
            "Batch [674/1336], Val Loss: 1.0896\n",
            "Batch [675/1336], Val Loss: 1.0896\n",
            "Batch [676/1336], Val Loss: 1.0896\n",
            "Batch [677/1336], Val Loss: 1.0896\n",
            "Batch [678/1336], Val Loss: 1.0896\n",
            "Batch [679/1336], Val Loss: 1.0896\n",
            "Batch [680/1336], Val Loss: 1.0896\n",
            "Batch [681/1336], Val Loss: 1.0896\n",
            "Batch [682/1336], Val Loss: 1.0896\n",
            "Batch [683/1336], Val Loss: 1.0896\n",
            "Batch [684/1336], Val Loss: 1.0896\n",
            "Batch [685/1336], Val Loss: 1.0896\n",
            "Batch [686/1336], Val Loss: 1.0896\n",
            "Batch [687/1336], Val Loss: 1.0896\n",
            "Batch [688/1336], Val Loss: 1.0896\n",
            "Batch [689/1336], Val Loss: 1.0896\n",
            "Batch [690/1336], Val Loss: 1.0896\n",
            "Batch [691/1336], Val Loss: 1.0896\n",
            "Batch [692/1336], Val Loss: 1.0896\n",
            "Batch [693/1336], Val Loss: 1.0896\n",
            "Batch [694/1336], Val Loss: 1.0896\n",
            "Batch [695/1336], Val Loss: 1.0896\n",
            "Batch [696/1336], Val Loss: 1.0896\n",
            "Batch [697/1336], Val Loss: 1.0896\n",
            "Batch [698/1336], Val Loss: 1.0896\n",
            "Batch [699/1336], Val Loss: 1.0896\n",
            "Batch [700/1336], Val Loss: 1.0896\n",
            "Batch [701/1336], Val Loss: 1.0896\n",
            "Batch [702/1336], Val Loss: 1.0896\n",
            "Batch [703/1336], Val Loss: 1.0896\n",
            "Batch [704/1336], Val Loss: 1.0896\n",
            "Batch [705/1336], Val Loss: 1.0896\n",
            "Batch [706/1336], Val Loss: 1.0896\n",
            "Batch [707/1336], Val Loss: 1.0896\n",
            "Batch [708/1336], Val Loss: 1.0896\n",
            "Batch [709/1336], Val Loss: 1.0896\n",
            "Batch [710/1336], Val Loss: 1.0896\n",
            "Batch [711/1336], Val Loss: 1.0896\n",
            "Batch [712/1336], Val Loss: 1.0896\n",
            "Batch [713/1336], Val Loss: 1.0896\n",
            "Batch [714/1336], Val Loss: 1.0896\n",
            "Batch [715/1336], Val Loss: 1.0896\n",
            "Batch [716/1336], Val Loss: 1.0896\n",
            "Batch [717/1336], Val Loss: 1.0896\n",
            "Batch [718/1336], Val Loss: 1.0896\n",
            "Batch [719/1336], Val Loss: 1.0896\n",
            "Batch [720/1336], Val Loss: 1.0896\n",
            "Batch [721/1336], Val Loss: 1.0896\n",
            "Batch [722/1336], Val Loss: 1.0896\n",
            "Batch [723/1336], Val Loss: 1.0896\n",
            "Batch [724/1336], Val Loss: 1.0896\n",
            "Batch [725/1336], Val Loss: 1.0896\n",
            "Batch [726/1336], Val Loss: 1.0896\n",
            "Batch [727/1336], Val Loss: 1.0896\n",
            "Batch [728/1336], Val Loss: 1.0896\n",
            "Batch [729/1336], Val Loss: 1.0896\n",
            "Batch [730/1336], Val Loss: 1.0896\n",
            "Batch [731/1336], Val Loss: 1.0896\n",
            "Batch [732/1336], Val Loss: 1.0896\n",
            "Batch [733/1336], Val Loss: 1.0896\n",
            "Batch [734/1336], Val Loss: 1.0896\n",
            "Batch [735/1336], Val Loss: 1.0896\n",
            "Batch [736/1336], Val Loss: 1.0896\n",
            "Batch [737/1336], Val Loss: 1.0896\n",
            "Batch [738/1336], Val Loss: 1.0896\n",
            "Batch [739/1336], Val Loss: 1.0896\n",
            "Batch [740/1336], Val Loss: 1.0896\n",
            "Batch [741/1336], Val Loss: 1.0896\n",
            "Batch [742/1336], Val Loss: 1.0896\n",
            "Batch [743/1336], Val Loss: 1.0896\n",
            "Batch [744/1336], Val Loss: 1.0896\n",
            "Batch [745/1336], Val Loss: 1.0896\n",
            "Batch [746/1336], Val Loss: 1.0896\n",
            "Batch [747/1336], Val Loss: 1.0896\n",
            "Batch [748/1336], Val Loss: 1.0896\n",
            "Batch [749/1336], Val Loss: 1.0896\n",
            "Batch [750/1336], Val Loss: 1.0896\n",
            "Batch [751/1336], Val Loss: 1.0896\n",
            "Batch [752/1336], Val Loss: 1.0896\n",
            "Batch [753/1336], Val Loss: 1.0896\n",
            "Batch [754/1336], Val Loss: 1.0896\n",
            "Batch [755/1336], Val Loss: 1.0896\n",
            "Batch [756/1336], Val Loss: 1.0896\n",
            "Batch [757/1336], Val Loss: 1.0896\n",
            "Batch [758/1336], Val Loss: 1.0896\n",
            "Batch [759/1336], Val Loss: 1.0896\n",
            "Batch [760/1336], Val Loss: 1.0896\n",
            "Batch [761/1336], Val Loss: 1.0896\n",
            "Batch [762/1336], Val Loss: 1.0896\n",
            "Batch [763/1336], Val Loss: 1.0896\n",
            "Batch [764/1336], Val Loss: 1.0896\n",
            "Batch [765/1336], Val Loss: 1.0896\n",
            "Batch [766/1336], Val Loss: 1.0896\n",
            "Batch [767/1336], Val Loss: 1.0896\n",
            "Batch [768/1336], Val Loss: 1.0896\n",
            "Batch [769/1336], Val Loss: 1.0896\n",
            "Batch [770/1336], Val Loss: 1.0896\n",
            "Batch [771/1336], Val Loss: 1.0896\n",
            "Batch [772/1336], Val Loss: 1.0896\n",
            "Batch [773/1336], Val Loss: 1.0896\n",
            "Batch [774/1336], Val Loss: 1.0896\n",
            "Batch [775/1336], Val Loss: 1.0896\n",
            "Batch [776/1336], Val Loss: 1.0896\n",
            "Batch [777/1336], Val Loss: 1.0896\n",
            "Batch [778/1336], Val Loss: 1.0896\n",
            "Batch [779/1336], Val Loss: 1.0896\n",
            "Batch [780/1336], Val Loss: 1.0896\n",
            "Batch [781/1336], Val Loss: 1.0896\n",
            "Batch [782/1336], Val Loss: 1.0896\n",
            "Batch [783/1336], Val Loss: 1.0896\n",
            "Batch [784/1336], Val Loss: 1.0896\n",
            "Batch [785/1336], Val Loss: 1.0896\n",
            "Batch [786/1336], Val Loss: 1.0896\n",
            "Batch [787/1336], Val Loss: 1.0896\n",
            "Batch [788/1336], Val Loss: 1.0896\n",
            "Batch [789/1336], Val Loss: 1.0896\n",
            "Batch [790/1336], Val Loss: 1.0896\n",
            "Batch [791/1336], Val Loss: 1.0896\n",
            "Batch [792/1336], Val Loss: 1.0896\n",
            "Batch [793/1336], Val Loss: 1.0896\n",
            "Batch [794/1336], Val Loss: 1.0896\n",
            "Batch [795/1336], Val Loss: 1.0896\n",
            "Batch [796/1336], Val Loss: 1.0896\n",
            "Batch [797/1336], Val Loss: 1.0896\n",
            "Batch [798/1336], Val Loss: 1.0896\n",
            "Batch [799/1336], Val Loss: 1.0896\n",
            "Batch [800/1336], Val Loss: 1.0896\n",
            "Batch [801/1336], Val Loss: 1.0896\n",
            "Batch [802/1336], Val Loss: 1.0896\n",
            "Batch [803/1336], Val Loss: 1.0896\n",
            "Batch [804/1336], Val Loss: 1.0896\n",
            "Batch [805/1336], Val Loss: 1.0896\n",
            "Batch [806/1336], Val Loss: 1.0896\n",
            "Batch [807/1336], Val Loss: 1.0896\n",
            "Batch [808/1336], Val Loss: 1.0896\n",
            "Batch [809/1336], Val Loss: 1.0896\n",
            "Batch [810/1336], Val Loss: 1.0896\n",
            "Batch [811/1336], Val Loss: 1.0896\n",
            "Batch [812/1336], Val Loss: 1.0896\n",
            "Batch [813/1336], Val Loss: 1.0896\n",
            "Batch [814/1336], Val Loss: 1.0896\n",
            "Batch [815/1336], Val Loss: 1.0896\n",
            "Batch [816/1336], Val Loss: 1.0896\n",
            "Batch [817/1336], Val Loss: 1.0896\n",
            "Batch [818/1336], Val Loss: 1.0896\n",
            "Batch [819/1336], Val Loss: 1.0896\n",
            "Batch [820/1336], Val Loss: 1.0896\n",
            "Batch [821/1336], Val Loss: 1.0896\n",
            "Batch [822/1336], Val Loss: 1.0896\n",
            "Batch [823/1336], Val Loss: 1.0896\n",
            "Batch [824/1336], Val Loss: 1.0896\n",
            "Batch [825/1336], Val Loss: 1.0896\n",
            "Batch [826/1336], Val Loss: 1.0896\n",
            "Batch [827/1336], Val Loss: 1.0896\n",
            "Batch [828/1336], Val Loss: 1.0896\n",
            "Batch [829/1336], Val Loss: 1.0896\n",
            "Batch [830/1336], Val Loss: 1.0896\n",
            "Batch [831/1336], Val Loss: 1.0896\n",
            "Batch [832/1336], Val Loss: 1.0896\n",
            "Batch [833/1336], Val Loss: 1.0896\n",
            "Batch [834/1336], Val Loss: 1.0896\n",
            "Batch [835/1336], Val Loss: 1.0896\n",
            "Batch [836/1336], Val Loss: 1.0896\n",
            "Batch [837/1336], Val Loss: 1.0896\n",
            "Batch [838/1336], Val Loss: 1.0896\n",
            "Batch [839/1336], Val Loss: 1.0896\n",
            "Batch [840/1336], Val Loss: 1.0896\n",
            "Batch [841/1336], Val Loss: 1.0896\n",
            "Batch [842/1336], Val Loss: 1.0896\n",
            "Batch [843/1336], Val Loss: 1.0896\n",
            "Batch [844/1336], Val Loss: 1.0896\n",
            "Batch [845/1336], Val Loss: 1.0896\n",
            "Batch [846/1336], Val Loss: 1.0896\n",
            "Batch [847/1336], Val Loss: 1.0896\n",
            "Batch [848/1336], Val Loss: 1.0896\n",
            "Batch [849/1336], Val Loss: 1.0896\n",
            "Batch [850/1336], Val Loss: 1.0896\n",
            "Batch [851/1336], Val Loss: 1.0896\n",
            "Batch [852/1336], Val Loss: 1.0896\n",
            "Batch [853/1336], Val Loss: 1.0896\n",
            "Batch [854/1336], Val Loss: 1.0896\n",
            "Batch [855/1336], Val Loss: 1.0896\n",
            "Batch [856/1336], Val Loss: 1.0896\n",
            "Batch [857/1336], Val Loss: 1.0896\n",
            "Batch [858/1336], Val Loss: 1.0896\n",
            "Batch [859/1336], Val Loss: 1.0896\n",
            "Batch [860/1336], Val Loss: 1.0896\n",
            "Batch [861/1336], Val Loss: 1.0896\n",
            "Batch [862/1336], Val Loss: 1.0896\n",
            "Batch [863/1336], Val Loss: 1.0896\n",
            "Batch [864/1336], Val Loss: 1.0896\n",
            "Batch [865/1336], Val Loss: 1.0896\n",
            "Batch [866/1336], Val Loss: 1.0896\n",
            "Batch [867/1336], Val Loss: 1.0896\n",
            "Batch [868/1336], Val Loss: 1.0896\n",
            "Batch [869/1336], Val Loss: 1.0896\n",
            "Batch [870/1336], Val Loss: 1.0896\n",
            "Batch [871/1336], Val Loss: 1.0896\n",
            "Batch [872/1336], Val Loss: 1.0896\n",
            "Batch [873/1336], Val Loss: 1.0896\n",
            "Batch [874/1336], Val Loss: 1.0896\n",
            "Batch [875/1336], Val Loss: 1.0896\n",
            "Batch [876/1336], Val Loss: 1.0896\n",
            "Batch [877/1336], Val Loss: 1.0896\n",
            "Batch [878/1336], Val Loss: 1.0896\n",
            "Batch [879/1336], Val Loss: 1.0896\n",
            "Batch [880/1336], Val Loss: 1.0896\n",
            "Batch [881/1336], Val Loss: 1.0896\n",
            "Batch [882/1336], Val Loss: 1.0896\n",
            "Batch [883/1336], Val Loss: 1.0896\n",
            "Batch [884/1336], Val Loss: 1.0896\n",
            "Batch [885/1336], Val Loss: 1.0896\n",
            "Batch [886/1336], Val Loss: 1.0896\n",
            "Batch [887/1336], Val Loss: 1.0896\n",
            "Batch [888/1336], Val Loss: 1.0896\n",
            "Batch [889/1336], Val Loss: 1.0896\n",
            "Batch [890/1336], Val Loss: 1.0896\n",
            "Batch [891/1336], Val Loss: 1.0896\n",
            "Batch [892/1336], Val Loss: 1.0896\n",
            "Batch [893/1336], Val Loss: 1.0896\n",
            "Batch [894/1336], Val Loss: 1.0896\n",
            "Batch [895/1336], Val Loss: 1.0896\n",
            "Batch [896/1336], Val Loss: 1.0896\n",
            "Batch [897/1336], Val Loss: 1.0896\n",
            "Batch [898/1336], Val Loss: 1.0896\n",
            "Batch [899/1336], Val Loss: 1.0896\n",
            "Batch [900/1336], Val Loss: 1.0896\n",
            "Batch [901/1336], Val Loss: 1.0896\n",
            "Batch [902/1336], Val Loss: 1.0896\n",
            "Batch [903/1336], Val Loss: 1.0896\n",
            "Batch [904/1336], Val Loss: 1.0896\n",
            "Batch [905/1336], Val Loss: 1.0896\n",
            "Batch [906/1336], Val Loss: 1.0896\n",
            "Batch [907/1336], Val Loss: 1.0896\n",
            "Batch [908/1336], Val Loss: 1.0896\n",
            "Batch [909/1336], Val Loss: 1.0896\n",
            "Batch [910/1336], Val Loss: 1.0896\n",
            "Batch [911/1336], Val Loss: 1.0896\n",
            "Batch [912/1336], Val Loss: 1.0896\n",
            "Batch [913/1336], Val Loss: 1.0896\n",
            "Batch [914/1336], Val Loss: 1.0896\n",
            "Batch [915/1336], Val Loss: 1.0896\n",
            "Batch [916/1336], Val Loss: 1.0896\n",
            "Batch [917/1336], Val Loss: 1.0896\n",
            "Batch [918/1336], Val Loss: 1.0896\n",
            "Batch [919/1336], Val Loss: 1.0896\n",
            "Batch [920/1336], Val Loss: 1.0896\n",
            "Batch [921/1336], Val Loss: 1.0896\n",
            "Batch [922/1336], Val Loss: 1.0896\n",
            "Batch [923/1336], Val Loss: 1.0896\n",
            "Batch [924/1336], Val Loss: 1.0896\n",
            "Batch [925/1336], Val Loss: 1.0896\n",
            "Batch [926/1336], Val Loss: 1.0896\n",
            "Batch [927/1336], Val Loss: 1.0896\n",
            "Batch [928/1336], Val Loss: 1.0896\n",
            "Batch [929/1336], Val Loss: 1.0896\n",
            "Batch [930/1336], Val Loss: 1.0896\n",
            "Batch [931/1336], Val Loss: 1.0896\n",
            "Batch [932/1336], Val Loss: 1.0896\n",
            "Batch [933/1336], Val Loss: 1.0896\n",
            "Batch [934/1336], Val Loss: 1.0896\n",
            "Batch [935/1336], Val Loss: 1.0896\n",
            "Batch [936/1336], Val Loss: 1.0896\n",
            "Batch [937/1336], Val Loss: 1.0896\n",
            "Batch [938/1336], Val Loss: 1.0896\n",
            "Batch [939/1336], Val Loss: 1.0896\n",
            "Batch [940/1336], Val Loss: 1.0896\n",
            "Batch [941/1336], Val Loss: 1.0896\n",
            "Batch [942/1336], Val Loss: 1.0896\n",
            "Batch [943/1336], Val Loss: 1.0896\n",
            "Batch [944/1336], Val Loss: 1.0896\n",
            "Batch [945/1336], Val Loss: 1.0896\n",
            "Batch [946/1336], Val Loss: 1.0896\n",
            "Batch [947/1336], Val Loss: 1.0896\n",
            "Batch [948/1336], Val Loss: 1.0896\n",
            "Batch [949/1336], Val Loss: 1.0896\n",
            "Batch [950/1336], Val Loss: 1.0896\n",
            "Batch [951/1336], Val Loss: 1.0896\n",
            "Batch [952/1336], Val Loss: 1.0896\n",
            "Batch [953/1336], Val Loss: 1.0896\n",
            "Batch [954/1336], Val Loss: 1.0896\n",
            "Batch [955/1336], Val Loss: 1.0896\n",
            "Batch [956/1336], Val Loss: 1.0896\n",
            "Batch [957/1336], Val Loss: 1.0896\n",
            "Batch [958/1336], Val Loss: 1.0896\n",
            "Batch [959/1336], Val Loss: 1.0896\n",
            "Batch [960/1336], Val Loss: 1.0896\n",
            "Batch [961/1336], Val Loss: 1.0896\n",
            "Batch [962/1336], Val Loss: 1.0896\n",
            "Batch [963/1336], Val Loss: 1.0896\n",
            "Batch [964/1336], Val Loss: 1.0896\n",
            "Batch [965/1336], Val Loss: 1.0896\n",
            "Batch [966/1336], Val Loss: 1.0896\n",
            "Batch [967/1336], Val Loss: 1.0896\n",
            "Batch [968/1336], Val Loss: 1.0896\n",
            "Batch [969/1336], Val Loss: 1.0896\n",
            "Batch [970/1336], Val Loss: 1.0896\n",
            "Batch [971/1336], Val Loss: 1.0896\n",
            "Batch [972/1336], Val Loss: 1.0896\n",
            "Batch [973/1336], Val Loss: 1.0896\n",
            "Batch [974/1336], Val Loss: 1.0896\n",
            "Batch [975/1336], Val Loss: 1.0896\n",
            "Batch [976/1336], Val Loss: 1.0896\n",
            "Batch [977/1336], Val Loss: 1.0896\n",
            "Batch [978/1336], Val Loss: 1.0896\n",
            "Batch [979/1336], Val Loss: 1.0896\n",
            "Batch [980/1336], Val Loss: 1.0896\n",
            "Batch [981/1336], Val Loss: 1.0896\n",
            "Batch [982/1336], Val Loss: 1.0896\n",
            "Batch [983/1336], Val Loss: 1.0896\n",
            "Batch [984/1336], Val Loss: 1.0896\n",
            "Batch [985/1336], Val Loss: 1.0896\n",
            "Batch [986/1336], Val Loss: 1.0896\n",
            "Batch [987/1336], Val Loss: 1.0896\n",
            "Batch [988/1336], Val Loss: 1.0896\n",
            "Batch [989/1336], Val Loss: 1.0896\n",
            "Batch [990/1336], Val Loss: 1.0896\n",
            "Batch [991/1336], Val Loss: 1.0896\n",
            "Batch [992/1336], Val Loss: 1.0896\n",
            "Batch [993/1336], Val Loss: 1.0896\n",
            "Batch [994/1336], Val Loss: 1.0896\n",
            "Batch [995/1336], Val Loss: 1.0896\n",
            "Batch [996/1336], Val Loss: 1.0896\n",
            "Batch [997/1336], Val Loss: 1.0896\n",
            "Batch [998/1336], Val Loss: 1.0896\n",
            "Batch [999/1336], Val Loss: 1.0896\n",
            "Batch [1000/1336], Val Loss: 1.0896\n",
            "Batch [1001/1336], Val Loss: 1.0896\n",
            "Batch [1002/1336], Val Loss: 1.0896\n",
            "Batch [1003/1336], Val Loss: 1.0896\n",
            "Batch [1004/1336], Val Loss: 1.0896\n",
            "Batch [1005/1336], Val Loss: 1.0896\n",
            "Batch [1006/1336], Val Loss: 1.0896\n",
            "Batch [1007/1336], Val Loss: 1.0896\n",
            "Batch [1008/1336], Val Loss: 1.0896\n",
            "Batch [1009/1336], Val Loss: 1.0896\n",
            "Batch [1010/1336], Val Loss: 1.0896\n",
            "Batch [1011/1336], Val Loss: 1.0896\n",
            "Batch [1012/1336], Val Loss: 1.0896\n",
            "Batch [1013/1336], Val Loss: 1.0896\n",
            "Batch [1014/1336], Val Loss: 1.0896\n",
            "Batch [1015/1336], Val Loss: 1.0896\n",
            "Batch [1016/1336], Val Loss: 1.0896\n",
            "Batch [1017/1336], Val Loss: 1.0896\n",
            "Batch [1018/1336], Val Loss: 1.0896\n",
            "Batch [1019/1336], Val Loss: 1.0896\n",
            "Batch [1020/1336], Val Loss: 1.0896\n",
            "Batch [1021/1336], Val Loss: 1.0896\n",
            "Batch [1022/1336], Val Loss: 1.0896\n",
            "Batch [1023/1336], Val Loss: 1.0896\n",
            "Batch [1024/1336], Val Loss: 1.0896\n",
            "Batch [1025/1336], Val Loss: 1.0896\n",
            "Batch [1026/1336], Val Loss: 1.0896\n",
            "Batch [1027/1336], Val Loss: 1.0896\n",
            "Batch [1028/1336], Val Loss: 1.0896\n",
            "Batch [1029/1336], Val Loss: 1.0896\n",
            "Batch [1030/1336], Val Loss: 1.0896\n",
            "Batch [1031/1336], Val Loss: 1.0896\n",
            "Batch [1032/1336], Val Loss: 1.0896\n",
            "Batch [1033/1336], Val Loss: 1.0896\n",
            "Batch [1034/1336], Val Loss: 1.0896\n",
            "Batch [1035/1336], Val Loss: 1.0896\n",
            "Batch [1036/1336], Val Loss: 1.0896\n",
            "Batch [1037/1336], Val Loss: 1.0896\n",
            "Batch [1038/1336], Val Loss: 1.0896\n",
            "Batch [1039/1336], Val Loss: 1.0896\n",
            "Batch [1040/1336], Val Loss: 1.0896\n",
            "Batch [1041/1336], Val Loss: 1.0896\n",
            "Batch [1042/1336], Val Loss: 1.0896\n",
            "Batch [1043/1336], Val Loss: 1.0896\n",
            "Batch [1044/1336], Val Loss: 1.0896\n",
            "Batch [1045/1336], Val Loss: 1.0896\n",
            "Batch [1046/1336], Val Loss: 1.0896\n",
            "Batch [1047/1336], Val Loss: 1.0896\n",
            "Batch [1048/1336], Val Loss: 1.0896\n",
            "Batch [1049/1336], Val Loss: 1.0896\n",
            "Batch [1050/1336], Val Loss: 1.0896\n",
            "Batch [1051/1336], Val Loss: 1.0896\n",
            "Batch [1052/1336], Val Loss: 1.0896\n",
            "Batch [1053/1336], Val Loss: 1.0896\n",
            "Batch [1054/1336], Val Loss: 1.0896\n",
            "Batch [1055/1336], Val Loss: 1.0896\n",
            "Batch [1056/1336], Val Loss: 1.0896\n",
            "Batch [1057/1336], Val Loss: 1.0896\n",
            "Batch [1058/1336], Val Loss: 1.0896\n",
            "Batch [1059/1336], Val Loss: 1.0896\n",
            "Batch [1060/1336], Val Loss: 1.0896\n",
            "Batch [1061/1336], Val Loss: 1.0896\n",
            "Batch [1062/1336], Val Loss: 1.0896\n",
            "Batch [1063/1336], Val Loss: 1.0896\n",
            "Batch [1064/1336], Val Loss: 1.0896\n",
            "Batch [1065/1336], Val Loss: 1.0896\n",
            "Batch [1066/1336], Val Loss: 1.0896\n",
            "Batch [1067/1336], Val Loss: 1.0896\n",
            "Batch [1068/1336], Val Loss: 1.0896\n",
            "Batch [1069/1336], Val Loss: 1.0896\n",
            "Batch [1070/1336], Val Loss: 1.0896\n",
            "Batch [1071/1336], Val Loss: 1.0896\n",
            "Batch [1072/1336], Val Loss: 1.0896\n",
            "Batch [1073/1336], Val Loss: 1.0896\n",
            "Batch [1074/1336], Val Loss: 1.0896\n",
            "Batch [1075/1336], Val Loss: 1.0896\n",
            "Batch [1076/1336], Val Loss: 1.0896\n",
            "Batch [1077/1336], Val Loss: 1.0896\n",
            "Batch [1078/1336], Val Loss: 1.0896\n",
            "Batch [1079/1336], Val Loss: 1.0896\n",
            "Batch [1080/1336], Val Loss: 1.0896\n",
            "Batch [1081/1336], Val Loss: 1.0896\n",
            "Batch [1082/1336], Val Loss: 1.0896\n",
            "Batch [1083/1336], Val Loss: 1.0896\n",
            "Batch [1084/1336], Val Loss: 1.0896\n",
            "Batch [1085/1336], Val Loss: 1.0896\n",
            "Batch [1086/1336], Val Loss: 1.0896\n",
            "Batch [1087/1336], Val Loss: 1.0896\n",
            "Batch [1088/1336], Val Loss: 1.0896\n",
            "Batch [1089/1336], Val Loss: 1.0896\n",
            "Batch [1090/1336], Val Loss: 1.0896\n",
            "Batch [1091/1336], Val Loss: 1.0896\n",
            "Batch [1092/1336], Val Loss: 1.0896\n",
            "Batch [1093/1336], Val Loss: 1.0896\n",
            "Batch [1094/1336], Val Loss: 1.0896\n",
            "Batch [1095/1336], Val Loss: 1.0896\n",
            "Batch [1096/1336], Val Loss: 1.0896\n",
            "Batch [1097/1336], Val Loss: 1.0896\n",
            "Batch [1098/1336], Val Loss: 1.0896\n",
            "Batch [1099/1336], Val Loss: 1.0896\n",
            "Batch [1100/1336], Val Loss: 1.0896\n",
            "Batch [1101/1336], Val Loss: 1.0896\n",
            "Batch [1102/1336], Val Loss: 1.0896\n",
            "Batch [1103/1336], Val Loss: 1.0896\n",
            "Batch [1104/1336], Val Loss: 1.0896\n",
            "Batch [1105/1336], Val Loss: 1.0896\n",
            "Batch [1106/1336], Val Loss: 1.0896\n",
            "Batch [1107/1336], Val Loss: 1.0896\n",
            "Batch [1108/1336], Val Loss: 1.0896\n",
            "Batch [1109/1336], Val Loss: 1.0896\n",
            "Batch [1110/1336], Val Loss: 1.0896\n",
            "Batch [1111/1336], Val Loss: 1.0896\n",
            "Batch [1112/1336], Val Loss: 1.0896\n",
            "Batch [1113/1336], Val Loss: 1.0896\n",
            "Batch [1114/1336], Val Loss: 1.0896\n",
            "Batch [1115/1336], Val Loss: 1.0896\n",
            "Batch [1116/1336], Val Loss: 1.0896\n",
            "Batch [1117/1336], Val Loss: 1.0896\n",
            "Batch [1118/1336], Val Loss: 1.0896\n",
            "Batch [1119/1336], Val Loss: 1.0896\n",
            "Batch [1120/1336], Val Loss: 1.0896\n",
            "Batch [1121/1336], Val Loss: 1.0896\n",
            "Batch [1122/1336], Val Loss: 1.0896\n",
            "Batch [1123/1336], Val Loss: 1.0896\n",
            "Batch [1124/1336], Val Loss: 1.0896\n",
            "Batch [1125/1336], Val Loss: 1.0896\n",
            "Batch [1126/1336], Val Loss: 1.0896\n",
            "Batch [1127/1336], Val Loss: 1.0896\n",
            "Batch [1128/1336], Val Loss: 1.0896\n",
            "Batch [1129/1336], Val Loss: 1.0896\n",
            "Batch [1130/1336], Val Loss: 1.0896\n",
            "Batch [1131/1336], Val Loss: 1.0896\n",
            "Batch [1132/1336], Val Loss: 1.0896\n",
            "Batch [1133/1336], Val Loss: 1.0896\n",
            "Batch [1134/1336], Val Loss: 1.0896\n",
            "Batch [1135/1336], Val Loss: 1.0896\n",
            "Batch [1136/1336], Val Loss: 1.0896\n",
            "Batch [1137/1336], Val Loss: 1.0896\n",
            "Batch [1138/1336], Val Loss: 1.0896\n",
            "Batch [1139/1336], Val Loss: 1.0896\n",
            "Batch [1140/1336], Val Loss: 1.0896\n",
            "Batch [1141/1336], Val Loss: 1.0896\n",
            "Batch [1142/1336], Val Loss: 1.0896\n",
            "Batch [1143/1336], Val Loss: 1.0896\n",
            "Batch [1144/1336], Val Loss: 1.0896\n",
            "Batch [1145/1336], Val Loss: 1.0896\n",
            "Batch [1146/1336], Val Loss: 1.0896\n",
            "Batch [1147/1336], Val Loss: 1.0896\n",
            "Batch [1148/1336], Val Loss: 1.0896\n",
            "Batch [1149/1336], Val Loss: 1.0896\n",
            "Batch [1150/1336], Val Loss: 1.0896\n",
            "Batch [1151/1336], Val Loss: 1.0896\n",
            "Batch [1152/1336], Val Loss: 1.0896\n",
            "Batch [1153/1336], Val Loss: 1.0896\n",
            "Batch [1154/1336], Val Loss: 1.0896\n",
            "Batch [1155/1336], Val Loss: 1.0896\n",
            "Batch [1156/1336], Val Loss: 1.0896\n",
            "Batch [1157/1336], Val Loss: 1.0896\n",
            "Batch [1158/1336], Val Loss: 1.0896\n",
            "Batch [1159/1336], Val Loss: 1.0896\n",
            "Batch [1160/1336], Val Loss: 1.0896\n",
            "Batch [1161/1336], Val Loss: 1.0896\n",
            "Batch [1162/1336], Val Loss: 1.0896\n",
            "Batch [1163/1336], Val Loss: 1.0896\n",
            "Batch [1164/1336], Val Loss: 1.0896\n",
            "Batch [1165/1336], Val Loss: 1.0896\n",
            "Batch [1166/1336], Val Loss: 1.0896\n",
            "Batch [1167/1336], Val Loss: 1.0896\n",
            "Batch [1168/1336], Val Loss: 1.0896\n",
            "Batch [1169/1336], Val Loss: 1.0896\n",
            "Batch [1170/1336], Val Loss: 1.0896\n",
            "Batch [1171/1336], Val Loss: 1.0896\n",
            "Batch [1172/1336], Val Loss: 1.0896\n",
            "Batch [1173/1336], Val Loss: 1.0896\n",
            "Batch [1174/1336], Val Loss: 1.0896\n",
            "Batch [1175/1336], Val Loss: 1.0896\n",
            "Batch [1176/1336], Val Loss: 1.0896\n",
            "Batch [1177/1336], Val Loss: 1.0896\n",
            "Batch [1178/1336], Val Loss: 1.0896\n",
            "Batch [1179/1336], Val Loss: 1.0896\n",
            "Batch [1180/1336], Val Loss: 1.0896\n",
            "Batch [1181/1336], Val Loss: 1.0896\n",
            "Batch [1182/1336], Val Loss: 1.0896\n",
            "Batch [1183/1336], Val Loss: 1.0896\n",
            "Batch [1184/1336], Val Loss: 1.0896\n",
            "Batch [1185/1336], Val Loss: 1.0896\n",
            "Batch [1186/1336], Val Loss: 1.0896\n",
            "Batch [1187/1336], Val Loss: 1.0896\n",
            "Batch [1188/1336], Val Loss: 1.0896\n",
            "Batch [1189/1336], Val Loss: 1.0896\n",
            "Batch [1190/1336], Val Loss: 1.0896\n",
            "Batch [1191/1336], Val Loss: 1.0896\n",
            "Batch [1192/1336], Val Loss: 1.0896\n",
            "Batch [1193/1336], Val Loss: 1.0896\n",
            "Batch [1194/1336], Val Loss: 1.0896\n",
            "Batch [1195/1336], Val Loss: 1.0896\n",
            "Batch [1196/1336], Val Loss: 1.0896\n",
            "Batch [1197/1336], Val Loss: 1.0896\n",
            "Batch [1198/1336], Val Loss: 1.0896\n",
            "Batch [1199/1336], Val Loss: 1.0896\n",
            "Batch [1200/1336], Val Loss: 1.0896\n",
            "Batch [1201/1336], Val Loss: 1.0896\n",
            "Batch [1202/1336], Val Loss: 1.0896\n",
            "Batch [1203/1336], Val Loss: 1.0896\n",
            "Batch [1204/1336], Val Loss: 1.0896\n",
            "Batch [1205/1336], Val Loss: 1.0896\n",
            "Batch [1206/1336], Val Loss: 1.0896\n",
            "Batch [1207/1336], Val Loss: 1.0896\n",
            "Batch [1208/1336], Val Loss: 1.0896\n",
            "Batch [1209/1336], Val Loss: 1.0896\n",
            "Batch [1210/1336], Val Loss: 1.0896\n",
            "Batch [1211/1336], Val Loss: 1.0896\n",
            "Batch [1212/1336], Val Loss: 1.0896\n",
            "Batch [1213/1336], Val Loss: 1.0896\n",
            "Batch [1214/1336], Val Loss: 1.0896\n",
            "Batch [1215/1336], Val Loss: 1.0896\n",
            "Batch [1216/1336], Val Loss: 1.0896\n",
            "Batch [1217/1336], Val Loss: 1.0896\n",
            "Batch [1218/1336], Val Loss: 1.0896\n",
            "Batch [1219/1336], Val Loss: 1.0896\n",
            "Batch [1220/1336], Val Loss: 1.0896\n",
            "Batch [1221/1336], Val Loss: 1.0896\n",
            "Batch [1222/1336], Val Loss: 1.0896\n",
            "Batch [1223/1336], Val Loss: 1.0896\n",
            "Batch [1224/1336], Val Loss: 1.0896\n",
            "Batch [1225/1336], Val Loss: 1.0896\n",
            "Batch [1226/1336], Val Loss: 1.0896\n",
            "Batch [1227/1336], Val Loss: 1.0896\n",
            "Batch [1228/1336], Val Loss: 1.0896\n",
            "Batch [1229/1336], Val Loss: 1.0896\n",
            "Batch [1230/1336], Val Loss: 1.0896\n",
            "Batch [1231/1336], Val Loss: 1.0896\n",
            "Batch [1232/1336], Val Loss: 1.0896\n",
            "Batch [1233/1336], Val Loss: 1.0896\n",
            "Batch [1234/1336], Val Loss: 1.0896\n",
            "Batch [1235/1336], Val Loss: 1.0896\n",
            "Batch [1236/1336], Val Loss: 1.0896\n",
            "Batch [1237/1336], Val Loss: 1.0896\n",
            "Batch [1238/1336], Val Loss: 1.0896\n",
            "Batch [1239/1336], Val Loss: 1.0896\n",
            "Batch [1240/1336], Val Loss: 1.0896\n",
            "Batch [1241/1336], Val Loss: 1.0896\n",
            "Batch [1242/1336], Val Loss: 1.0896\n",
            "Batch [1243/1336], Val Loss: 1.0896\n",
            "Batch [1244/1336], Val Loss: 1.0896\n",
            "Batch [1245/1336], Val Loss: 1.0896\n",
            "Batch [1246/1336], Val Loss: 1.0896\n",
            "Batch [1247/1336], Val Loss: 1.0896\n",
            "Batch [1248/1336], Val Loss: 1.0896\n",
            "Batch [1249/1336], Val Loss: 1.0896\n",
            "Batch [1250/1336], Val Loss: 1.0896\n",
            "Batch [1251/1336], Val Loss: 1.0896\n",
            "Batch [1252/1336], Val Loss: 1.0896\n",
            "Batch [1253/1336], Val Loss: 1.0896\n",
            "Batch [1254/1336], Val Loss: 1.0896\n",
            "Batch [1255/1336], Val Loss: 1.0896\n",
            "Batch [1256/1336], Val Loss: 1.0896\n",
            "Batch [1257/1336], Val Loss: 1.0896\n",
            "Batch [1258/1336], Val Loss: 1.0896\n",
            "Batch [1259/1336], Val Loss: 1.0896\n",
            "Batch [1260/1336], Val Loss: 1.0896\n",
            "Batch [1261/1336], Val Loss: 1.0896\n",
            "Batch [1262/1336], Val Loss: 1.0896\n",
            "Batch [1263/1336], Val Loss: 1.0896\n",
            "Batch [1264/1336], Val Loss: 1.0896\n",
            "Batch [1265/1336], Val Loss: 1.0896\n",
            "Batch [1266/1336], Val Loss: 1.0896\n",
            "Batch [1267/1336], Val Loss: 1.0896\n",
            "Batch [1268/1336], Val Loss: 1.0896\n",
            "Batch [1269/1336], Val Loss: 1.0896\n",
            "Batch [1270/1336], Val Loss: 1.0896\n",
            "Batch [1271/1336], Val Loss: 1.0896\n",
            "Batch [1272/1336], Val Loss: 1.0896\n",
            "Batch [1273/1336], Val Loss: 1.0896\n",
            "Batch [1274/1336], Val Loss: 1.0896\n",
            "Batch [1275/1336], Val Loss: 1.0896\n",
            "Batch [1276/1336], Val Loss: 1.0896\n",
            "Batch [1277/1336], Val Loss: 1.0896\n",
            "Batch [1278/1336], Val Loss: 1.0896\n",
            "Batch [1279/1336], Val Loss: 1.0896\n",
            "Batch [1280/1336], Val Loss: 1.0896\n",
            "Batch [1281/1336], Val Loss: 1.0896\n",
            "Batch [1282/1336], Val Loss: 1.0896\n",
            "Batch [1283/1336], Val Loss: 1.0896\n",
            "Batch [1284/1336], Val Loss: 1.0896\n",
            "Batch [1285/1336], Val Loss: 1.0896\n",
            "Batch [1286/1336], Val Loss: 1.0896\n",
            "Batch [1287/1336], Val Loss: 1.0896\n",
            "Batch [1288/1336], Val Loss: 1.0896\n",
            "Batch [1289/1336], Val Loss: 1.0896\n",
            "Batch [1290/1336], Val Loss: 1.0896\n",
            "Batch [1291/1336], Val Loss: 1.0896\n",
            "Batch [1292/1336], Val Loss: 1.0896\n",
            "Batch [1293/1336], Val Loss: 1.0896\n",
            "Batch [1294/1336], Val Loss: 1.0896\n",
            "Batch [1295/1336], Val Loss: 1.0896\n",
            "Batch [1296/1336], Val Loss: 1.0896\n",
            "Batch [1297/1336], Val Loss: 1.0896\n",
            "Batch [1298/1336], Val Loss: 1.0896\n",
            "Batch [1299/1336], Val Loss: 1.0896\n",
            "Batch [1300/1336], Val Loss: 1.0896\n",
            "Batch [1301/1336], Val Loss: 1.0896\n",
            "Batch [1302/1336], Val Loss: 1.0896\n",
            "Batch [1303/1336], Val Loss: 1.0896\n",
            "Batch [1304/1336], Val Loss: 1.0896\n",
            "Batch [1305/1336], Val Loss: 1.0896\n",
            "Batch [1306/1336], Val Loss: 1.0896\n",
            "Batch [1307/1336], Val Loss: 1.0896\n",
            "Batch [1308/1336], Val Loss: 1.0896\n",
            "Batch [1309/1336], Val Loss: 1.0896\n",
            "Batch [1310/1336], Val Loss: 1.0896\n",
            "Batch [1311/1336], Val Loss: 1.0896\n",
            "Batch [1312/1336], Val Loss: 1.0896\n",
            "Batch [1313/1336], Val Loss: 1.0896\n",
            "Batch [1314/1336], Val Loss: 1.0896\n",
            "Batch [1315/1336], Val Loss: 1.0896\n",
            "Batch [1316/1336], Val Loss: 1.0896\n",
            "Batch [1317/1336], Val Loss: 1.0896\n",
            "Batch [1318/1336], Val Loss: 1.0896\n",
            "Batch [1319/1336], Val Loss: 1.0896\n",
            "Batch [1320/1336], Val Loss: 1.0896\n",
            "Batch [1321/1336], Val Loss: 1.0896\n",
            "Batch [1322/1336], Val Loss: 1.0896\n",
            "Batch [1323/1336], Val Loss: 1.0896\n",
            "Batch [1324/1336], Val Loss: 1.0896\n",
            "Batch [1325/1336], Val Loss: 1.0896\n",
            "Batch [1326/1336], Val Loss: 1.0896\n",
            "Batch [1327/1336], Val Loss: 1.0896\n",
            "Batch [1328/1336], Val Loss: 1.0896\n",
            "Batch [1329/1336], Val Loss: 1.0896\n",
            "Batch [1330/1336], Val Loss: 1.0896\n",
            "Batch [1331/1336], Val Loss: 1.0896\n",
            "Batch [1332/1336], Val Loss: 1.0896\n",
            "Batch [1333/1336], Val Loss: 1.0896\n",
            "Batch [1334/1336], Val Loss: 1.0896\n",
            "Batch [1335/1336], Val Loss: 1.0896\n",
            "Batch [1336/1336], Val Loss: 1.0896\n",
            "Batch [1337/1336], Val Loss: 1.0896\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-11): 11 x T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseActDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): ReLU()\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "checkpoint_path = f'model_checkpoint_epoch_{val_batch_ind+1}.pt'\n",
        "torch.save(model.state_dict(), checkpoint_path)"
      ],
      "metadata": {
        "id": "eWJSm9HrJsat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss values\n",
        "plt.plot(range(1, batches_val_dataset+2 ), val_loss_values, 'r', label='Val Loss')\n",
        "plt.title('Validation Loss')\n",
        "plt.xlabel('Batches')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HQhJ1vSE6jaE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "30327506-be83-4f93-c97c-54acc79eeabc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSSsuICxqjQq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Model Inference:\n"
      ],
      "metadata": {
        "id": "OryTIQiiwwCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save_pretrained('text_summarization_model')\n",
        "\n",
        "# Load the trained model\n",
        "model = T5ForConditionalGeneration.from_pretrained('text_summarization_model')\n",
        "\n",
        "# Generate summaries for new input text\n",
        "input_text = \"Islington Council said the move was necessary to improve air quality in the borough. The authority said pollutants in diesel exhausts had been linked to heart and lung disease. But a motoring group said drivers were confused by the penalising of one fuel over another as today's diesel cars were the cleanest ever. Mike Hawes, from the Society of Motor Manufacturers and Traders, said: Bans and parking taxes on diesel vehicles therefore make no sense from an environmental point of view. The allegations against diesel cars made in recent months threaten to misguide policy-making and undermine public confidence in diesel. It's time to put the record straight. The surcharge, which will be imposed by Islington Council from Monday, coincides with an increase in its parking permits. The cost of an Islington resident's permit depends on the emission or engine size of their vehicle with the highest priced at £444 for a year from Monday. This was found to be the highest charge for some drivers in the capital, according to a recent survey carried out by Churchill Car Insurance. Claudia Webbe, the council's executive member for transport and environment, said diesel fumes were the major cause of pollution. She added: Pollutants in diesel exhausts have been linked to heart and lung diseases, which are major causes of serious and long-term health issues and even death in Islington, and the surcharge will encourage a move away from diesel. In 2014 the council threatened to hand out £20 fines to drivers who refused to switch off their diesel engines while parked\"\n",
        "input_ids=tokenizer.encode(input_text, padding='max_length', truncation=True, max_length=512, return_tensors='pt') #.to(device)\n",
        "summary_ids = model.generate(input_ids, num_beams=4, max_length=100, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "riViQwP20oxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570c6359-4655-4515-f7c8-064c003d8a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            "Islington Council said the move was necessary to improve air quality. Pollutants in diesel exhausts have been linked to heart and lung disease. Motoring group said drivers confused by penalising one fuel over another.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qvNb7wf79Lkb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}